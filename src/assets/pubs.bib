@inproceedings{abdullah2016,
  title = {Sketching, {{Embedding}} and {{Dimensionality Reduction}} in {{Information Theoretic Spaces}}},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Abdullah, Amirali and Kumar, Ravi and McGregor, Andrew and Vassilvitskii, Sergei and Venkatasubramanian, Suresh},
  year = {2016},
  month = may,
  pages = {948--956},
  urldate = {2020-02-02},
  abstract = {In this paper we show how to embed information distances like the {$\chi\sphat$}2 and Jensen-Shannon divergences efficiently in low dimensional spaces while preserving all  pairwise distances.  We then prove a...},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Abdullah et al. - 2016 - Sketching, Embedding and Dimensionality Reduction .pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\IMQDGQ7X\\abdullah16.html}
}

@inproceedings{acharya2017,
  title = {A {{Unified Maximum Likelihood Approach}} for {{Estimating Symmetric Properties}} of {{Discrete Distributions}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Acharya, Jayadev and Das, Hirakendu and Orlitsky, Alon and Suresh, Ananda Theertha},
  year = {2017},
  month = jul,
  pages = {11--21},
  issn = {1938-7228},
  urldate = {2020-05-12},
  abstract = {Symmetric distribution properties such as support size, support coverage, entropy, and proximity to uniformity, arise in many applications. Recently, researchers applied different estimators and an...},
  chapter = {Machine Learning},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Acharya et al. - 2017 - A Unified Maximum Likelihood Approach for Estimati.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\UXRBP56U\\acharya17a.html}
}

@inproceedings{acharya2018,
  title = {Profile {{Maximum Likelihood}} Is {{Optimal}} for {{Estimating KL Divergence}}},
  booktitle = {2018 {{IEEE International Symposium}} on {{Information Theory}} ({{ISIT}})},
  author = {Acharya, Jayadev},
  year = {2018},
  month = jun,
  pages = {1400--1404},
  issn = {2157-8117},
  doi = {10.1109/ISIT.2018.8437461},
  abstract = {Estimating distribution properties and information measures is an important problem in many fields. The class of symmetric properties have received particular attention. We extend the framework of symmetric property estimation to properties of collection of distributions, and show that a simple plug-in maximum likelihood based method is competitive for estimating any symmetric property. Kullback-Leibler (KL) divergence is a symmetric property of a pair of distributions, for which we show that the profile maximum likelihood based approach is sample optimal.},
  keywords = {Complexity theory,distribution properties,Entropy,Error probability,Information theory,KL divergence,Kullback-Leibler divergence,maximum likelihood estimation,Maximum likelihood estimation,simple plug-in maximum likelihood based method,symmetric property estimation},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Acharya - 2018 - Profile Maximum Likelihood is Optimal for Estimati.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\WZ6LQ72L\\8437461.html}
}

@article{alberghini2022,
  title = {Adaptive Ensemble of Self-Adjusting Nearest Neighbor Subspaces for Multi-Label Drifting Data Streams},
  author = {Alberghini, Gavin and Barbon Junior, Sylvio and Cano, Alberto},
  year = {2022},
  month = apr,
  journal = {Neurocomputing},
  volume = {481},
  pages = {228--248},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2022.01.075},
  urldate = {2023-02-25},
  abstract = {Multi-label data streams are sequences of multi-label instances arriving over time to a multi-label classifier. The properties of the stream may continuously change due to concept drift. Therefore, algorithms must constantly adapt to the new data distributions. In this paper we propose a novel ensemble method for multi-label drifting streams named Adaptive Ensemble of Self-Adjusting Nearest Neighbor Subspaces (AESAKNNS). It leverages a self-adjusting kNN as a base classifier with the advantages of ensembles to adapt to concept drift in the multi-label environment. To promote diverse knowledge within the ensemble, each base classifier is given a unique subset of features and samples to train on. These samples are distributed to classifiers in a probabilistic manner that follows a Poisson distribution as in online bagging. Accompanying these mechanisms, a collection of ADWIN detectors monitor each classifier for the occurrence of a concept drift on the subspace. Upon detection, the algorithm automatically trains additional classifiers in the background to attempt to capture new concepts on new subspaces of features. The dynamic classifier selection chooses the most accurate classifiers from the active and background ensembles to replace the current ensemble. Our experimental study compares the proposed approach with 30 other classifiers, including problem transformation, algorithm adaptation, kNNs, and ensembles on 30 diverse multi-label datasets and 12 performance metrics. Results, validated using non-parametric statistical analysis, support the better performance of the AESAKNNS and highlight the contribution of its components in improving the performance of the ensemble.},
  langid = {english},
  keywords = {Concept drift,Data stream,Ensemble learning,Multi-label stream},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Alberghini et al. - 2022 - Adaptive ensemble of self-adjusting nearest neighb.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\5HXFCXFW\\S0925231222000984.html}
}

@article{ali1966,
  title = {A {{General Class}} of {{Coefficients}} of {{Divergence}} of {{One Distribution}} from {{Another}}},
  author = {Ali, S. M. and Silvey, S. D.},
  year = {1966},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {28},
  number = {1},
  eprint = {2984279},
  eprinttype = {jstor},
  pages = {131--142},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  urldate = {2023-03-30},
  abstract = {Let P\textsubscript{1} and P\textsubscript{2} be two probability measures on the same space and let {$\varphi$} be the generalized Radon-Nikodym derivative of P\textsubscript{2} with respect to P\textsubscript{1}. If C is a continuous convex function of a real variable such that the P\textsubscript{1}-expectation (generalized as in Section 3) of C({$\varphi$}) provides a reasonable coefficient of the P\textsubscript{1}-dispersion of {$\varphi$}, then this expectation has basic properties which it is natural to demand of a coefficient of divergence of P\textsubscript{2} from P\textsubscript{1}. A general class of coefficients of divergence is generated in this way and it is shown that various available measures of divergence, distance, discriminatory information, etc., are members of this class.}
}

@inproceedings{alippi2016,
  title = {Change Detection in Multivariate Datastreams: Likelihood and Detectability Loss},
  shorttitle = {Change Detection in Multivariate Datastreams},
  booktitle = {Proceedings of the {{Twenty-Fifth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Alippi, Cesare and Boracchi, Giacomo and Carrera, Diego and Roveri, Manuel},
  year = {2016},
  month = jul,
  series = {{{IJCAI}}'16},
  pages = {1368--1374},
  publisher = {{AAAI Press}},
  address = {{New York, New York, USA}},
  urldate = {2020-02-03},
  abstract = {We address the problem of detecting changes in multivariate datastreams, and we investigate the intrinsic difficulty that change-detection methods have to face when the data dimension scales. In particular, we consider a general approach where changes are detected by comparing the distribution of the log-likelihood of the datastream over different time windows. Despite the fact that this approach constitutes the frame of several change-detection methods, its effectiveness when data dimension scales has never been investigated, which is indeed the goal of our paper. We show that the magnitude of the change can be naturally measured by the symmetric Kullback-Leibler divergence between the pre- and post-change distributions, and that the detectability of a change of a given magnitude worsens when the data dimension increases. This problem, which we refer to as detectability loss , is due to the linear relationship between the variance of the log-likelihood and the data dimension. We analytically derive the detectability loss on Gaussian-distributed datastreams, and empirically demonstrate that this problem holds also on real-world datasets and that can be harmful even at low data-dimensions (say, 10).},
  isbn = {978-1-57735-770-4},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Alippi et al. - 2016 - Change detection in multivariate datastreams like.pdf}
}

@article{almeida2018,
  title = {Adapting Dynamic Classifier Selection for Concept Drift},
  author = {Almeida, Paulo R. L. and Oliveira, Luiz S. and Britto, Alceu S. and Sabourin, Robert},
  year = {2018},
  month = aug,
  journal = {Expert Systems with Applications},
  volume = {104},
  pages = {67--85},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2018.03.021},
  urldate = {2020-02-16},
  abstract = {One popular approach employed to tackle classification problems in a static environment consists in using a Dynamic Classifier Selection (DCS)-based method to select a custom classifier/ensemble for each test instance according to its neighborhood in a validation set, where the selection can be considered region-dependent. This idea can be extended to concept drift scenarios, where the distribution or the a posteriori probabilities may change over time. Nevertheless, in these scenarios, the classifier selection becomes not only region but also time-dependent. By adding a time dependency, in this work, we hypothesize that any DCS-based approach can be used to handle concept drift problems. Since some regions may not be affected by a concept drift, we introduce the idea of concept diversity, which shows that a pool containing classifiers trained under different concepts may be beneficial when dealing with concept drift problems through a DCS approach. The impacts of pruning mechanisms are discussed and seven well-known DCS methods are evaluated in the proposed framework, using a robust experimental protocol based on 12 common concept drift problems with different properties, and the PKLot dataset considering an experimental protocol specially designed in this work to test concept drift methods. The experimental results have shown that the DCS approach comes out ahead in terms of stability, i.e., it performs well in most cases requiring almost no parameter tuning.},
  langid = {english},
  keywords = {Concept diversity,Concept drift,Dynamic classifier selection,Dynamic ensemble selection},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Almeida et al. - 2018 - Adapting dynamic classifier selection for concept .pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\BY2UPBRS\\S0957417418301611.html}
}

@misc{alpaydine.1998,
  title = {{{UCI Machine Learning Repository}}: {{Pen-Based Recognition}} of {{Handwritten Digits Data Set}}},
  author = {{Alpaydin, E.} and {Alimoglu, Fevzi.}},
  year = {1998},
  month = jul,
  urldate = {2018-11-08},
  howpublished = {https://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\KCRB4AXL\\Pen-Based+Recognition+of+Handwritten+Digits.html}
}

@article{amari2009,
  title = {{$\alpha$} -{{Divergence Is Unique}}, {{Belonging}} to {{Both}} \$f\$-{{Divergence}} and {{Bregman Divergence Classes}}},
  author = {Amari, Shun-Ichi},
  year = {2009},
  month = nov,
  journal = {IEEE Transactions on Information Theory},
  volume = {55},
  number = {11},
  pages = {4925--4931},
  issn = {1557-9654},
  doi = {10.1109/TIT.2009.2030485},
  abstract = {A divergence measure between two probability distributions or positive arrays (positive measures) is a useful tool for solving optimization problems in optimization, signal processing, machine learning, and statistical inference. The Csiszar f-divergence is a unique class of divergences having information monotonicity, from which the dual alpha geometrical structure with the Fisher metric is derived. The Bregman divergence is another class of divergences that gives a dually flat geometrical structure different from the alpha-structure in general. Csiszar gave an axiomatic characterization of divergences related to inference problems. The Kullback-Leibler divergence is proved to belong to both classes, and this is the only such one in the space of probability distributions. This paper proves that the alpha-divergences constitute a unique class belonging to both classes when the space of positive measures or positive arrays is considered. They are the canonical divergences derived from the dually flat geometrical structure of the space of positive measures.},
  keywords = {\$f\$-divergence,Bregman divergence,canonical divergence,dually flat structure,Entropy,Fisher information,information geometry,Information geometry,information monotonicity,Matrix decomposition,Physics,Probability distribution},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Amari - 2009 - α -Divergence Is Unique, Belonging to Both $f$-Div.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\DJXWM3UM\\5290302.html}
}

@book{amari2016,
  title = {Information {{Geometry}} and {{Its Applications}}},
  author = {Amari, Shun-ichi},
  year = {2016},
  month = feb,
  publisher = {{Springer}},
  abstract = {This is the first comprehensive book on information geometry, written by the founder of the field. It begins with an elementary introduction to dualistic geometry and proceeds to a wide range of applications, covering information science, engineering, and neuroscience. It consists of four parts, which on the whole can be read independently. A manifold with a divergence function is first introduced, leading directly to dualistic structure, the heart of information geometry. This part (Part I) can be apprehended without any knowledge of differential geometry. An intuitive explanation of modern differential geometry then follows in Part II, although the book is for the most part understandable without modern differential geometry. Information geometry of statistical inference, including time series analysis and semiparametric estimation (the Neyman\textendash Scott problem), is demonstrated concisely in Part III. Applications addressed in Part IV include hot current topics in machine learning, signal processing, optimization, and neural networks. The book is interdisciplinary, connecting mathematics, information sciences, physics, and neurosciences, inviting readers to a new world of information and geometry. This book is highly recommended to graduate students and researchers who seek new mathematical methods and tools useful in their own fields.},
  googlebooks = {UkSFCwAAQBAJ},
  isbn = {978-4-431-55978-8},
  langid = {english},
  keywords = {Mathematics / Applied,Mathematics / Geometry / Differential,Mathematics / Geometry / General,Mathematics / Probability \& Statistics / General},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Amari - 2016 - Information Geometry and Its Applications.pdf}
}

@article{amir2010,
  title = {Approximation {{Algorithms}} for {{Treewidth}}},
  author = {Amir, Eyal},
  year = {2010},
  month = apr,
  journal = {Algorithmica},
  volume = {56},
  number = {4},
  pages = {448--479},
  issn = {1432-0541},
  doi = {10.1007/s00453-008-9180-4},
  urldate = {2020-01-12},
  abstract = {This paper presents algorithms whose input is an undirected graph, and whose output is a tree decomposition of width that approximates the optimal, the treewidth of that graph. The algorithms differ in their computation time and their approximation guarantees. The first algorithm works in polynomial-time and finds a factor-O(log\,OPT) approximation, where OPT is the treewidth of the graph. This is the first polynomial-time algorithm that approximates the optimal by a factor that does not depend on n, the number of nodes in the input graph. As a result, we get an algorithm for finding pathwidth within a factor of O(log\,OPT{$\cdot$}log\,n) from the optimal. We also present algorithms that approximate the treewidth of a graph by constant factors of 3.66, 4, and 4.5, respectively and take time that is exponential in the treewidth. These are more efficient than previously known algorithms by an exponential factor, and are of practical interest. Finding triangulations of minimum treewidth for graphs is central to many problems in computer science. Real-world problems in artificial intelligence, VLSI design and databases are efficiently solvable if we have an efficient approximation algorithm for them. Many of those applications rely on weighted graphs. We extend our results to weighted graphs and weighted treewidth, showing similar approximation results for this more general notion. We report on experimental results confirming the effectiveness of our algorithms for large graphs associated with real-world problems.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Amir - 2010 - Approximation Algorithms for Treewidth.pdf}
}

@article{anderson2019,
  title = {Recurring Concept Meta-Learning for Evolving Data Streams},
  author = {Anderson, Robert and Koh, Yun Sing and Dobbie, Gillian and Bifet, Albert},
  year = {2019},
  month = dec,
  journal = {Expert Systems with Applications},
  volume = {138},
  pages = {112832},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2019.112832},
  urldate = {2020-02-16},
  abstract = {When concept drift is detected during classification in a data stream, a common remedy is to retrain a framework's classifier. However, this loses useful information if the classifier has learnt the current concept well, and this concept will recur again in the future. Some frameworks retain and reuse classifiers, but it can be time-consuming to select an appropriate classifier to reuse. These frameworks rarely match the accuracy of state-of-the-art ensemble approaches. For many data stream tasks, speed is important: fast, accurate frameworks are needed for time-dependent applications. We propose the Enhanced Concept Profiling Framework (ECPF), which aims to recognise recurring concepts and reuse a classifier trained previously, enabling accurate classification immediately following a drift. The novelty of ECPF is in how it uses similarity of classifications on new data, between a new classifier and existing classifiers, to quickly identify the best classifier to reuse. It always trains both a new classifier and a reused classifier, and retains the more accurate classifier when concept drift occurs. Finally, it creates a copy of reused classifiers, so a classifier well-suited for a recurring concept will not be impacted by being trained on a different concept. In our experiments, ECPF classifies significantly more accurately than a state-of-the-art classifier reuse framework (Diversity Pool) and a state-of-the-art ensemble technique (Adaptive Random Forest) on synthetic datasets with recurring concepts. It classifies real-world datasets five times faster than Diversity Pool, and six times faster than Adaptive Random Forest and is not significantly less accurate than either.},
  langid = {english},
  keywords = {Classification,Concept drift,Data streams,Recurring concepts},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Anderson et al. - 2019 - Recurring concept meta-learning for evolving data .pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\44CZMY8N\\S0957417419305342.html}
}

@article{anevski2017,
  title = {Estimating a Probability Mass Function with Unknown Labels},
  author = {Anevski, Dragi and Gill, Richard D. and Zohren, Stefan},
  year = {2017},
  month = dec,
  journal = {Annals of Statistics},
  volume = {45},
  number = {6},
  pages = {2708--2735},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/17-AOS1542},
  urldate = {2021-02-07},
  abstract = {In the context of a species sampling problem, we discuss a nonparametric maximum likelihood estimator for the underlying probability mass function. The estimator is known in the computer science literature as the high profile estimator. We prove strong consistency and derive the rates of convergence, for an extended model version of the estimator. We also study a sieved estimator for which similar consistency results are derived. Numerical computation of the sieved estimator is of great interest for practical problems, such as forensic DNA analysis, and we present a computational algorithm based on the stochastic approximation of the expectation maximisation algorithm. As an interesting byproduct of the numerical analyses, we introduce an algorithm for bounded isotonic regression for which we also prove convergence.},
  langid = {english},
  mrnumber = {MR3737907},
  zmnumber = {06838148},
  keywords = {high profile,monotone rearrangement,nonparametric,NPMLE,ordered,probability mass function,rates,SA-EM,sieve,strong consistency},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Anevski et al. - 2017 - Estimating a probability mass function with unknow.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\HKUWHVVJ\\1513328588.html}
}

@article{ankan2015,
  title = {Pgmpy: {{Probabilistic Graphical Models}} Using {{Python}}},
  shorttitle = {Pgmpy},
  author = {Ankan, Ankur and Panda, Abinash},
  year = {2015},
  journal = {Proceedings of the 14th Python in Science Conference},
  pages = {6--11},
  doi = {10.25080/Majora-7b98e3ed-001},
  urldate = {2022-06-22},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Ankan and Panda - 2015 - pgmpy Probabilistic Graphical Models using Python2.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\J98BXCUG\\ankur_ankan.html}
}

@article{baena-garcia2006,
  title = {Early {{Drift Detection Method}}},
  author = {{Baena-Garc{\'i}a}, Manuel and {Campo-{\'A}vila}, Jos{\'e} and {Fidalgo-Merino}, Ra{\'u}l and Bifet, Albert and Gavald, Ricard and Bueno, Rafael},
  year = {2006},
  month = jan,
  abstract = {An emerging problem in Data Streams is the detection of concept drift. This problem is aggravated when the drift is gradual over time. In this work we deflne a method for detecting concept drift, even in the case of slow gradual change. It is based on the estimated distribution of the distances between classiflcation errors. The proposed method can be used with any learning algorithm in two ways: using it as a wrapper of a batch learning algorithm or implementing it inside an incremental and online algorithm. The experimentation results compare our method (EDDM) with a similar one (DDM). Latter uses the error-rate instead of distance-error-rate.},
  file = {D\:\\work\\literature\\library\\Baena-García et al. - 2006 - Early Drift Detection Method.pdf}
}

@article{baidari2021,
  title = {Bhattacharyya Distance Based Concept Drift Detection Method for Evolving Data Stream},
  author = {Baidari, Ishwar and Honnikoll, Nagaraj},
  year = {2021},
  month = nov,
  journal = {Expert Systems with Applications},
  volume = {183},
  pages = {115303},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2021.115303},
  urldate = {2022-08-02},
  abstract = {The majority of online learners assume that the data distribution to be learned is established in advance. There are many real-world problems where the distribution of the data changes as a function of time. Variations in data streams data distributions can sufficiently reduce the skill of the learning algorithm on new data, if the learning algorithm is not equipped to track such changes. Therefore, the algorithm should initiate required actions to make sure that the new information is learned properly. In this article, we propose Bhattacharyya Distance-based Concept Drift Detection Method (BDDM) which uses Bhattacharyya distance to identify gradual or abrupt variations in the distribution. Experiments executed in the MOA framework using three artificial data generators and ten real-world datasets suggest that BDDM improves the detections and accuracies in many scenarios.},
  langid = {english},
  keywords = {Bhattacharyya distance,Concept drift,Hoeffding tree classifier,Massive Online Analysis,Naive Bayes classifier},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Baidari and Honnikoll - 2021 - Bhattacharyya distance based concept drift detecti.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\VIVG8XUD\\S0957417421007326.html}
}

@article{baier2020,
  title = {Handling {{Concept Drifts}} in {{Regression Problems}} -- the {{Error Intersection Approach}}},
  author = {Baier, Lucas and Hofmann, Marcel and K{\"u}hl, Niklas and Mohr, Marisa and Satzger, Gerhard},
  year = {2020},
  month = mar,
  journal = {arXiv:2004.00438 [cs, stat]},
  eprint = {2004.00438},
  primaryclass = {cs, stat},
  pages = {210--224},
  doi = {10.30844/wi\_2020\_c1-baier},
  urldate = {2021-03-30},
  abstract = {Machine learning models are omnipresent for predictions on big data. One challenge of deployed models is the change of the data over time, a phenomenon called concept drift. If not handled correctly, a concept drift can lead to significant mispredictions. We explore a novel approach for concept drift handling, which depicts a strategy to switch between the application of simple and complex machine learning models for regression tasks. We assume that the approach plays out the individual strengths of each model, switching to the simpler model if a drift occurs and switching back to the complex model for typical situations. We instantiate the approach on a real-world data set of taxi demand in New York City, which is prone to multiple drifts, e.g. the weather phenomena of blizzards, resulting in a sudden decrease of taxi demand. We are able to show that our suggested approach outperforms all regarded baselines significantly.},
  archiveprefix = {arxiv},
  keywords = {\_tablet,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\LoongKuan\\Dropbox\\academic papers\\paper library\\Baier et al\\Baier et al. - 2020 - Handling Concept Drifts in Regression Problems -- .pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\Q7BTLVPT\\2004.html}
}

@article{balakrishnan2018,
  title = {Hypothesis Testing for High-Dimensional Multinomials: {{A}} Selective Review},
  shorttitle = {Hypothesis Testing for High-Dimensional Multinomials},
  author = {Balakrishnan, Sivaraman and Wasserman, Larry},
  year = {2018},
  month = jun,
  journal = {The Annals of Applied Statistics},
  volume = {12},
  number = {2},
  pages = {727--749},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/18-AOAS1155SF},
  urldate = {2020-03-24},
  abstract = {The statistical analysis of discrete data has been the subject of extensive statistical research dating back to the work of Pearson. In this survey we review some recently developed methods for testing hypotheses about high-dimensional multinomials. Traditional tests like the {$\chi$}2{$\chi$}2\textbackslash chi\^\{2\}-test and the likelihood ratio test can have poor power in the high-dimensional setting. Much of the research in this area has focused on finding tests with asymptotically normal limits and developing (stringent) conditions under which tests have normal limits. We argue that this perspective suffers from a significant deficiency: it can exclude many high-dimensional cases when\textemdash despite having non-normal null distributions\textemdash carefully designed tests can have high power. Finally, we illustrate that taking a minimax perspective and considering refinements of this perspective can lead naturally to powerful and practical tests.},
  langid = {english},
  mrnumber = {MR3834283},
  zmnumber = {06980473},
  keywords = {high-dimensional multinomials,Hypothesis testing},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Balakrishnan and Wasserman - 2018 - Hypothesis testing for high-dimensional multinomia.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\WLVRXMZN\\1532743474.html}
}

@inproceedings{balzanella2013,
  title = {Data {{Stream Summarization}} by {{Histograms Clustering}}},
  booktitle = {Statistical {{Models}} for {{Data Analysis}}},
  author = {Balzanella, Antonio and Rivoli, Lidia and Verde, Rosanna},
  editor = {Giudici, Paolo and Ingrassia, Salvatore and Vichi, Maurizio},
  year = {2013},
  series = {Studies in {{Classification}}, {{Data Analysis}}, and {{Knowledge Organization}}},
  pages = {27--35},
  publisher = {{Springer International Publishing}},
  address = {{Heidelberg}},
  doi = {10.1007/978-3-319-00032-9\_4},
  abstract = {In this paper we introduce a new strategy for summarizing a fast changing data stream. Evolving data streams are generated by non stationary processes which require to adapt the knowledge discovery process to the new emerging concepts. To deal with this challenge we propose a clustering algorithm where each cluster is summarized by a histogram and data are allocated to clusters through a Wasserstein derived distance. Histograms are a well known graphical tool for representing the frequency distribution of data and are widely used in data stream mining, however, unlike to existing methods, we discover a set of histograms where each one represents a main concept in the data. In order to evaluate the performance of the method, we have performed extensive tests on simulated data.},
  isbn = {978-3-319-00032-9},
  langid = {english},
  keywords = {CluStream Algorithm,Data Stream Mining Methods,Data Stream Summarization,Irpino,Knowledge Discovery Process},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Balzanella et al. - 2013 - Data Stream Summarization by Histograms Clustering.pdf}
}

@article{baptista2021,
  title = {Learning Non-{{Gaussian}} Graphical Models via {{Hessian}} Scores and Triangular Transport},
  author = {Baptista, Ricardo and Marzouk, Youssef and Morrison, Rebecca E. and Zahm, Olivier},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.03093 [cs, stat]},
  eprint = {2101.03093},
  primaryclass = {cs, stat},
  urldate = {2021-04-18},
  abstract = {Undirected probabilistic graphical models represent the conditional dependencies, or Markov properties, of a collection of random variables. Knowing the sparsity of such a graphical model is valuable for modeling multivariate distributions and for efficiently performing inference. While the problem of learning graph structure from data has been studied extensively for certain parametric families of distributions, most existing methods fail to consistently recover the graph structure for non-Gaussian data. Here we propose an algorithm for learning the Markov structure of continuous and non-Gaussian distributions. To characterize conditional independence, we introduce a score based on integrated Hessian information from the joint log-density, and we prove that this score upper bounds the conditional mutual information for a general class of distributions. To compute the score, our algorithm SING estimates the density using a deterministic coupling, induced by a triangular transport map, and iteratively exploits sparse structure in the map to reveal sparsity in the graph. For certain non-Gaussian datasets, we show that our algorithm recovers the graph structure even with a biased approximation to the density. Among other examples, we apply sing to learn the dependencies between the states of a chaotic dynamical system with local interactions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Baptista et al. - 2021 - Learning non-Gaussian graphical models via Hessian.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\HXSDN7UR\\2101.html}
}

@article{barabasi1999,
  title = {Emergence of {{Scaling}} in {{Random Networks}}},
  author = {Barab{\'a}si, Albert-L{\'a}szl{\'o} and Albert, R{\'e}ka},
  year = {1999},
  month = oct,
  journal = {Science},
  volume = {286},
  number = {5439},
  pages = {509--512},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.286.5439.509},
  urldate = {2020-02-09},
  abstract = {Systems as diverse as genetic networks or the World Wide Web are best described as networks with complex topology. A common property of many large networks is that the vertex connectivities follow a scale-free power-law distribution. This feature was found to be a consequence of two generic mechanisms: (i) networks expand continuously by the addition of new vertices, and (ii) new vertices attach preferentially to sites that are already well connected. A model based on these two ingredients reproduces the observed stationary scale-free distributions, which indicates that the development of large networks is governed by robust self-organizing phenomena that go beyond the particulars of the individual systems.},
  langid = {english},
  pmid = {10521342},
  file = {D\:\\work\\literature\\library\\Barabási and Albert - 1999 - Emergence of Scaling in Random Networks.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\DLIFXW46\\509.html}
}

@article{barddal2017,
  title = {A Survey on Feature Drift Adaptation: {{Definition}}, Benchmark, Challenges and Future Directions},
  shorttitle = {A Survey on Feature Drift Adaptation},
  author = {Barddal, Jean Paul and Gomes, Heitor Murilo and Enembreck, Fabr{\'i}cio and Pfahringer, Bernhard},
  year = {2017},
  month = may,
  journal = {Journal of Systems and Software},
  volume = {127},
  pages = {278--294},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2016.07.005},
  urldate = {2020-02-15},
  abstract = {Data stream mining is a fast growing research topic due to the ubiquity of data in several real-world problems. Given their ephemeral nature, data stream sources are expected to undergo changes in data distribution, a phenomenon called concept drift. This paper focuses on one specific type of drift that has not yet been thoroughly studied, namely feature drift. Feature drift occurs whenever a subset of features becomes, or ceases to be, relevant to the learning task; thus, learners must detect and adapt to these changes accordingly. We survey existing work on feature drift adaptation with both explicit and implicit approaches. Additionally, we benchmark several algorithms and a naive feature drift detection approach using synthetic and real-world datasets. The results from our experiments indicate the need for future research in this area as even naive approaches produced gains in accuracy while reducing resources usage. Finally, we state current research topics, challenges and future directions for feature drift adaptation.},
  langid = {english},
  keywords = {Data stream mining,Feature drift,Feature selection},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Barddal et al. - 2017 - A survey on feature drift adaptation Definition, .pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\J45R5WV5\\S0164121216301030.html}
}

@article{barros2018,
  title = {A Large-Scale Comparison of Concept Drift Detectors},
  author = {Barros, Roberto Souto Maior and Santos, Silas Garrido T. Carvalho},
  year = {2018},
  month = jul,
  journal = {Information Sciences},
  volume = {451--452},
  pages = {348--370},
  issn = {00200255},
  doi = {10.1016/j.ins.2018.04.014},
  urldate = {2020-04-02},
  langid = {english},
  keywords = {concept drift}
}

@article{barz2019,
  title = {Detecting {{Regions}} of {{Maximal Divergence}} for {{Spatio-Temporal Anomaly Detection}}},
  author = {Barz, Bj{\"o}rn and Rodner, Erik and Garcia, Yanira Guanche and Denzler, Joachim},
  year = {2019},
  month = may,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {41},
  number = {5},
  pages = {1088--1101},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2018.2823766},
  abstract = {Automatic detection of anomalies in space- and time-varying measurements is an important tool in several fields, e.g., fraud detection, climate analysis, or healthcare monitoring. We present an algorithm for detecting anomalous regions in multivariate spatio-temporal time-series, which allows for spotting the interesting parts in large amounts of data, including video and text data. In opposition to existing techniques for detecting isolated anomalous data points, we propose the ``Maximally Divergent Intervals'' (MDI) framework for unsupervised detection of coherent spatial regions and time intervals characterized by a high Kullback-Leibler divergence compared with all other data given. In this regard, we define an unbiased Kullback-Leibler divergence that allows for ranking regions of different size and show how to enable the algorithm to run on large-scale data sets in reasonable time using an interval proposal technique. Experiments on both synthetic and real data from various domains, such as climate analysis, video surveillance, and text forensics, demonstrate that our method is widely applicable and a valuable tool for finding interesting events in different types of data.},
  keywords = {Anomaly detection,data mining,Data models,Medical services,Meteorology,spatio-temporal data,Task analysis,Tensile stress,time series analysis,Tools,unsupervised machine learning},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Barz et al. - 2019 - Detecting Regions of Maximal Divergence for Spatio.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\39QT6HT7\\8352745.html}
}

@article{bauer1972,
  title = {Constructing {{Confidence Sets Using Rank Statistics}}},
  author = {Bauer, David F.},
  year = {1972},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {67},
  number = {339},
  pages = {687--690},
  issn = {0162-1459},
  doi = {10.1080/01621459.1972.10481279},
  urldate = {2018-11-06},
  abstract = {Systematic procedures for constructing confidence bounds and point estimates based on rank statistics are given for the two sample location parameter, two sample scale parameter and one sample location parameter problems.},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Bauer - 1972 - Constructing Confidence Sets Using Rank Statistics.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\4M9NPXIV\\01621459.1972.html}
}

@article{belfodil2020,
  ids = {belfodilIdentifyingExceptionalDis},
  title = {Identifying Exceptional (Dis)Agreement between Groups},
  author = {Belfodil, Adnene and Cazalens, Sylvie and Lamarre, Philippe and Plantevit, Marc},
  year = {2020},
  month = mar,
  journal = {Data Mining and Knowledge Discovery},
  volume = {34},
  number = {2},
  pages = {394--442},
  issn = {1573-756X},
  doi = {10.1007/s10618-019-00665-9},
  urldate = {2021-08-07},
  abstract = {Under the term behavioral data, we consider any type of data featuring individuals performing observable actions on entities. For instance, voting data depict parliamentarians who express their votes w.r.t. legislative procedures. In this work, we address the problem of discovering exceptional (dis)agreement patterns in such data, i.e., groups of individuals that exhibit an unexpected (dis)agreement under specific contexts compared to what is observed in overall terms. To tackle this problem, we design a generic approach, rooted in the Subgroup Discovery/Exceptional Model Mining framework, which enables the discovery of such patterns in two different ways. A branch-and-bound algorithm ensures an efficient exhaustive search of the underlying search space by leveraging closure operators and optimistic estimates on the interestingness measures. A second algorithm abandons the completeness by using a sampling paradigm which provides an alternative when an exhaustive search approach becomes unfeasible. To illustrate the usefulness of discovering exceptional (dis)agreement patterns, we report a comprehensive experimental study on four real-world datasets relevant to three different application domains: political analysis, rating data analysis and healthcare surveillance.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Belfodil et al. - 2020 - Identifying exceptional (dis)agreement between gro.pdf}
}

@article{bergmeir2012,
  title = {On the Use of Cross-Validation for Time Series Predictor Evaluation},
  author = {Bergmeir, Christoph and Ben{\'i}tez, Jos{\'e} M.},
  year = {2012},
  month = may,
  journal = {Information Sciences},
  series = {Data {{Mining}} for {{Software Trustworthiness}}},
  volume = {191},
  pages = {192--213},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2011.12.028},
  urldate = {2020-03-13},
  abstract = {In time series predictor evaluation, we observe that with respect to the model selection procedure there is a gap between evaluation of traditional forecasting procedures, on the one hand, and evaluation of machine learning techniques on the other hand. In traditional forecasting, it is common practice to reserve a part from the end of each time series for testing, and to use the rest of the series for training. Thus it is not made full use of the data, but theoretical problems with respect to temporal evolutionary effects and dependencies within the data as well as practical problems regarding missing values are eliminated. On the other hand, when evaluating machine learning and other regression methods used for time series forecasting, often cross-validation is used for evaluation, paying little attention to the fact that those theoretical problems invalidate the fundamental assumptions of cross-validation. To close this gap and examine the consequences of different model selection procedures in practice, we have developed a rigorous and extensive empirical study. Six different model selection procedures, based on (i) cross-validation and (ii) evaluation using the series' last part, are used to assess the performance of four machine learning and other regression techniques on synthetic and real-world time series. No practical consequences of the theoretical flaws were found during our study, but the use of cross-validation techniques led to a more robust model selection. To make use of the ``best of both worlds'', we suggest that the use of a blocked form of cross-validation for time series evaluation became the standard procedure, thus using all available information and circumventing the theoretical problems.},
  langid = {english},
  keywords = {Cross-validation,Error measures,Machine learning,Predictor evaluation,Regression,Time series},
  file = {D\:\\work\\literature\\library\\Bergmeir and Benítez - 2012 - On the use of cross-validation for time series pre.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\FYSVE8K7\\S0020025511006773.html}
}

@article{berry2004,
  title = {Maximum {{Cardinality Search}} for {{Computing Minimal Triangulations}} of {{Graphs}}},
  author = {Berry, Anne and Blair, Jean R. S. and Heggernes, Pinar and Peyton, Barry W.},
  year = {2004},
  month = aug,
  journal = {Algorithmica},
  volume = {39},
  number = {4},
  pages = {287--298},
  issn = {1432-0541},
  doi = {10.1007/s00453-004-1084-3},
  urldate = {2019-10-30},
  abstract = {We present a new algorithm, called MCS-M, for computing minimal triangulations of graphs. Lex-BFS, a seminal algorithm for recognizing chordal graphs, was the genesis for two other classical algorithms: LEX M and MCS. LEX M extends the fundamental concept used in Lex-BFS, resulting in an algorithm that not only recognizes chordality, but also computes a minimal triangulation of an arbitrary graph. MCS simplifies the fundamental concept used in Lex-BFS, resulting in a simpler algorithm for recognizing chordal graphs. The new algorithm MCS-M combines the extension of LEX M with the simplification of MCS, achieving all the results of LEX M in the same time complexity.},
  langid = {english},
  keywords = {Chordal graphs,Minimal elimination ordering,Minimal fill,Minimal triangulations},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Berry et al. - 2004 - Maximum Cardinality Search for Computing Minimal T.pdf}
}

@article{berry2006,
  title = {A Vertex Incremental Approach for Maintaining Chordality},
  author = {Berry, Anne and Heggernes, Pinar and Villanger, Yngve},
  year = {2006},
  month = feb,
  journal = {Discrete Mathematics},
  series = {Minimal {{Separation}} and {{Minimal Triangulation}}},
  volume = {306},
  number = {3},
  pages = {318--336},
  issn = {0012-365X},
  doi = {10.1016/j.disc.2005.12.002},
  urldate = {2022-10-01},
  abstract = {For a chordal graph G=(V,E), we study the problem of whether a new vertex u{$\not\in$}V and a given set of edges between u and vertices in V can be added to G so that the resulting graph remains chordal. We show how to resolve this efficiently, and at the same time, if the answer is no, specify a maximal subset of the proposed edges that can be added along with u, or conversely, a minimal set of extra edges that can be added in addition to the given set, so that the resulting graph is chordal. In order to do this, we give a new characterization of chordal graphs and, for each potential new edge uv, a characterization of the set of edges incident to u that also must be added to G along with uv. We propose a data structure that can compute and add each such set in O(n) time. Based on these results, we present an algorithm that computes both a minimal triangulation and a maximal chordal subgraph of an arbitrary input graph in O(nm) time, using a totally new vertex incremental approach. In contrast to previous algorithms, our process is on-line in that each new vertex is added without reconsidering any choice made at previous steps, and without requiring any knowledge of the vertices that might be added subsequently.},
  langid = {english},
  keywords = {Chordal graphs,Maximal chordal subgraph,Minimal triangulation},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Berry et al. - 2006 - A vertex incremental approach for maintaining chor.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\FH7WWLHM\\S0012365X05006059.html}
}

@article{berry2011,
  title = {A Simple Algorithm to Generate the Minimal Separators and the Maximal Cliques of a Chordal Graph},
  author = {Berry, Anne and Pogorelcnik, Romain},
  year = {2011},
  month = may,
  journal = {Information Processing Letters},
  volume = {111},
  number = {11},
  pages = {508--511},
  issn = {0020-0190},
  doi = {10.1016/j.ipl.2011.02.013},
  urldate = {2021-12-10},
  abstract = {We present a simple unified algorithmic process which uses either LexBFS or MCS on a chordal graph to generate the minimal separators and the maximal cliques in linear time in a single pass.},
  langid = {english},
  keywords = {Graph algorithms,LexBFS,Maximal clique,MCS,Minimal separator,Moplex ordering},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Berry and Pogorelcnik - 2011 - A simple algorithm to generate the minimal separat.pdf}
}

@misc{bertin-mahieuxt.2011,
  title = {{{UCI Machine Learning Repository}}: {{YearPredictionMSD Data Set}}},
  author = {{Bertin-Mahieux, T.}},
  year = {2011},
  month = feb,
  urldate = {2018-11-08},
  howpublished = {https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\LI6NY9BD\\YearPredictionMSD.html}
}

@inproceedings{bhattacharya2009,
  title = {On {{Low Distortion Embeddings}} of {{Statistical Distance Measures}} into {{Low Dimensional Spaces}}},
  booktitle = {Database and {{Expert Systems Applications}}},
  author = {Bhattacharya, Arnab and Kar, Purushottam and Pal, Manjish},
  editor = {Bhowmick, Sourav S. and K{\"u}ng, Josef and Wagner, Roland},
  year = {2009},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {164--172},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-03573-9\_13},
  abstract = {In this paper, we investigate various statistical distance measures from the point of view of discovering low distortion embeddings into low dimensional spaces. More specifically, we consider the Mahalanobis distance measure, the Bhattacharyya class of divergences and the Kullback-Leibler divergence. We present a dimensionality reduction method based on the Johnson-Lindenstrauss Lemma for the Mahalanobis measure that achieves arbitrarily low distortion. By using the Johnson-Lindenstrauss Lemma again, we further demonstrate that the Bhattacharyya distance admits dimensionality reduction with arbitrarily low additive error. We also examine the question of embeddability into metric spaces for these distance measures due to the availability of efficient indexing schemes on metric spaces. We provide explicit constructions of point sets under the Bhattacharyya and the Kullback-Leibler divergences whose embeddings into any metric space incur arbitrarily large distortions. To the best of our knowledge, this is the first investigation into these distance measures from the point of view of dimensionality reduction and embeddability into metric spaces.},
  isbn = {978-3-642-03573-9},
  langid = {english},
  keywords = {Dimensionality Reduction,Distance Measure,Hellinger Distance,Mahalanobis Distance,Random Projection},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Bhattacharya et al. - 2009 - On Low Distortion Embeddings of Statistical Distan.pdf}
}

@article{bhattacharyya1946,
  title = {On a {{Measure}} of {{Divergence}} between {{Two Multinomial Populations}}},
  author = {Bhattacharyya, A.},
  year = {1946},
  journal = {Sankhy\=a: The Indian Journal of Statistics (1933-1960)},
  volume = {7},
  number = {4},
  eprint = {25047882},
  eprinttype = {jstor},
  pages = {401--406},
  issn = {0036-4452},
  urldate = {2020-02-02}
}

@article{bhattacharyya2018,
  title = {Property {{Testing}} of {{Joint Distributions}} Using {{Conditional Samples}}},
  author = {Bhattacharyya, Rishiraj and Chakraborty, Sourav},
  year = {2018},
  month = aug,
  journal = {ACM Transactions on Computation Theory},
  volume = {10},
  number = {4},
  pages = {16:1--16:20},
  issn = {1942-3454},
  doi = {10.1145/3241377},
  urldate = {2022-07-31},
  abstract = {In this article, we consider the problem of testing properties of joint distributions under the Conditional Sampling framework. In the standard sampling model, sample complexity of testing properties of joint distributions are exponential in the dimension, resulting in inefficient algorithms for practical use. While recent results achieve efficient algorithms for product distributions with significantly smaller sample complexity, no efficient algorithm is expected when the marginals are not independent. In this article, we initialize the study of conditional sampling in the multidimensional setting. We propose a subcube conditional sampling model where the tester can condition on a (adaptively) chosen subcube of the domain. Due to its simplicity, this model is potentially implementable in many practical applications, particularly when the distribution is a joint distribution over {$\Sigma$}n for some set {$\Sigma$}. We present algorithms for various fundamental properties of distributions in the subcube-conditioning model and prove that the sample complexity is polynomial in the dimension n (and not exponential as in the traditional model). We present an algorithm for testing identity to a known distribution using \~O(n2)-subcube-conditional samples, an algorithm for testing identity between two unknown distributions using \~O(n5)-subcube-conditional samples and an algorithm for testing identity to a product distribution using \~O(n5)-subcube-conditional samples. The central concept of our technique involves an elegant chain rule, which can be proved using basic techniques of probability theory, yet it is powerful enough to avoid the curse of dimensionality.},
  keywords = {conditional sampling,Distribution testing,subcube conditioning},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Bhattacharyya and Chakraborty - 2018 - Property Testing of Joint Distributions using Cond.pdf}
}

@inproceedings{bifet2007,
  title = {Learning from {{Time-Changing Data}} with {{Adaptive Windowing}}},
  booktitle = {Proceedings of the 2007 {{SIAM International Conference}} on {{Data Mining}}},
  author = {Bifet, A. and Gavald{\`a}, R.},
  year = {2007},
  month = apr,
  series = {Proceedings},
  pages = {443--448},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611972771.42},
  urldate = {2018-05-23},
  abstract = {We present a new approach for dealing with distribution change and concept drift when learning from data sequences that may vary with time. We use sliding windows whose size, instead of being fixed a priori, is recomputed online according to the rate of change observed from the data in the window itself. This delivers the user or programmer from having to guess a time-scale for change. Contrary to many related works, we provide rigorous guarantees of performance, as bounds on the rates of false positives and false negatives. Using ideas from data stream algorithmics, we develop a time- and memory-efficient version of this algorithm, called ADWIN2. We show how to combine ADWIN2 with the Na\"ive Bayes (NB) predictor, in two ways: one, using it to monitor the error rate of the current model and declare when revision is necessary and, two, putting it inside the NB predictor to maintain up-to-date estimations of conditional probabilities in the data. We test our approach using synthetic and real data streams and compare them to both fixed-size and variable-size window strategies with good results.},
  isbn = {978-0-89871-630-6},
  file = {D\:\\work\\literature\\library\\Bifet and Gavaldà - 2007 - Learning from Time-Changing Data with Adaptive Win.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\5GXTWEVZ\\1.9781611972771.html}
}

@inproceedings{bifet2009,
  title = {Improving {{Adaptive Bagging Methods}} for {{Evolving Data Streams}}},
  booktitle = {Proceedings of the 1st {{Asian Conference}} on {{Machine Learning}}: {{Advances}} in {{Machine Learning}}},
  author = {Bifet, Albert and Holmes, Geoff and Pfahringer, Bernhard and Gavald{\`a}, Ricard},
  year = {2009},
  series = {{{ACML}} '09},
  pages = {23--37},
  publisher = {{Springer-Verlag}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-05224-8\_4},
  urldate = {2018-05-30},
  abstract = {We propose two new improvements for bagging methods on evolving data streams. Recently, two new variants of Bagging were proposed: {$<$}Literal{$>$}ADWIN {$<$}/Literal{$>$} Bagging and Adaptive-Size Hoeffding Tree (ASHT) Bagging. ASHT Bagging uses trees of different sizes, and {$<$}Literal{$>$}ADWIN {$<$}/Literal{$>$} Bagging uses {$<$}Literal{$>$}ADWIN {$<$}/Literal{$>$} as a change detector to decide when to discard underperforming ensemble members. We improve {$<$}Literal{$>$}ADWIN {$<$}/Literal{$>$} Bagging using Hoeffding Adaptive Trees, trees that can adaptively learn from data streams that change over time. To speed up the time for adapting to change of Adaptive-Size Hoeffding Tree (ASHT) Bagging, we add an error change detector for each classifier. We test our improvements by performing an evaluation study on synthetic and real-world datasets comprising up to ten million examples.},
  isbn = {978-3-642-05223-1},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Bifet et al. - 2009 - Improving Adaptive Bagging Methods for Evolving Da.pdf}
}

@inproceedings{bifet2009a,
  title = {New Ensemble Methods for Evolving Data Streams},
  author = {Bifet, Albert and Holmes, Geoff and Pfahringer, Bernhard and Kirkby, Richard and Gavald{\`a}, Ricard},
  year = {2009},
  pages = {139},
  publisher = {{ACM Press}},
  doi = {10.1145/1557019.1557041},
  urldate = {2018-05-24},
  abstract = {Advanced analysis of data streams is quickly becoming a key area of data mining research as the number of applications demanding such processing increases. Online mining when such data streams evolve over time, that is when concepts drift or change completely, is becoming one of the core issues. When tackling non-stationary concepts, ensembles of classifiers have several advantages over single classifier methods: they are easy to scale and parallelize, they can adapt to change quickly by pruning under-performing parts of the ensemble, and they therefore usually also generate more accurate concept descriptions. This paper proposes a new experimental data stream framework for studying concept drift, and two new variants of Bagging: ADWIN Bagging and Adaptive-Size Hoeffding Tree (ASHT) Bagging. Using the new experimental framework, an evaluation study on synthetic and real-world datasets comprising up to ten million examples shows that the new ensemble methods perform very well compared to several known methods.},
  isbn = {978-1-60558-495-9},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Bifet et al. - 2009 - New ensemble methods for evolving data streams.pdf}
}

@inproceedings{bifet2010,
  title = {Leveraging {{Bagging}} for {{Evolving Data Streams}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Bifet, Albert and Holmes, Geoff and Pfahringer, Bernhard},
  editor = {Balc{\'a}zar, Jos{\'e} Luis and Bonchi, Francesco and Gionis, Aristides and Sebag, Mich{\`e}le},
  year = {2010},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {135--150},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Bagging, boosting and Random Forests are classical ensemble methods used to improve the performance of single classifiers. They obtain superior performance by increasing the accuracy and diversity of the single classifiers. Attempts have been made to reproduce these methods in the more challenging context of evolving data streams. In this paper, we propose a new variant of bagging, called leveraging bagging. This method combines the simplicity of bagging with adding more randomization to the input, and output of the classifiers. We test our method by performing an evaluation study on synthetic and real-world datasets comprising up to ten million examples.},
  isbn = {978-3-642-15880-3},
  langid = {english},
  keywords = {Bagging,Ensemble,Leveraging Bag},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Bifet et al. - 2010 - Leveraging Bagging for Evolving Data Streams.pdf}
}

@article{bifet2010a,
  title = {{{MOA}}: {{Massive Online Analysis}}},
  shorttitle = {{{MOA}}},
  author = {Bifet, Albert and Holmes, Geoff and Kirkby, Richard and Pfahringer, Bernhard},
  year = {2010},
  journal = {Journal of Machine Learning Research},
  volume = {11},
  number = {May},
  pages = {1601--1604},
  issn = {ISSN 1533-7928},
  urldate = {2018-05-24},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Bifet et al. - 2010 - MOA Massive Online Analysis.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\GTUXAV68\\bifet10a.html}
}

@inproceedings{bifet2015,
  title = {Efficient {{Online Evaluation}} of {{Big Data Stream Classifiers}}},
  booktitle = {Proceedings of the 21th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Bifet, Albert and {de Francisci Morales}, Gianmarco and Read, Jesse and Holmes, Geoff and Pfahringer, Bernhard},
  year = {2015},
  pages = {59--68},
  publisher = {{ACM Press}},
  doi = {10.1145/2783258.2783372},
  urldate = {2018-03-23},
  abstract = {The evaluation of classifiers in data streams is fundamental so that poorly-performing models can be identified, and either improved or replaced by better-performing models. This is an increasingly relevant and important task as stream data is generated from more sources, in real-time, in large quantities, and is now considered the largest source of big data. Both researchers and practitioners need to be able to effectively evaluate the performance of the methods they employ. However, there are major challenges for evaluation in a stream. Instances arriving in a data stream are usually timedependent, and the underlying concept that they represent may evolve over time. Furthermore, the massive quantity of data also tends to exacerbate issues such as class imbalance. Current frameworks for evaluating streaming and online algorithms are able to give predictions in real-time, but as they use a prequential setting, they build only one model, and are thus not able to compute the statistical significance of results in real-time. In this paper we propose a new evaluation methodology for big data streams. This methodology addresses unbalanced data streams, data where change occurs on different time scales, and the question of how to split the data between training and testing, over multiple models.},
  isbn = {978-1-4503-3664-2},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Bifet et al. - 2015 - Efficient Online Evaluation of Big Data Stream Cla.pdf}
}

@article{black1999,
  title = {Maintaining the {{Performance}} of a {{Learned Classifier Under Concept Drift}}},
  author = {Black, Michaela and Hickey, Ray J.},
  year = {1999},
  month = nov,
  journal = {Intell. Data Anal.},
  volume = {3},
  number = {6},
  pages = {453--474},
  issn = {1088-467X},
  doi = {10.1016/S1088-467X(99)00033-5},
  urldate = {2018-05-24},
  abstract = {On-line learning systems which use incoming batches of training examples to induce rules for a classification task, such as credit card fraud detection, may have to deal with concept drift whereby some of the underlying class definitions change over time. Identifying drift against a background of noise and maintaining accuracy of the learned rules are challenging tasks.We propose a methodology for handling these problems based on the assessment of relevance of a time-stamp attribute TSAR. In place of the time-windowing of examples that tends to be used in current approaches, we employ a new purging mechanism to remove examples that are no longer valid but retain valid examples regardless of age. This allows the example base to grow thus facilitating good classification.We describe one particular TSAR algorithm, CD3, which utilises ID3 with post pruning. We report on trials that show CD3 can cope very well in a variety of batch-drift scenarios.},
  keywords = {Batches,Classification,Concept Drift,Decision Trees,Incremental Learning},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Black and Hickey - 1999 - Maintaining the Performance of a Learned Classifie.pdf}
}

@inproceedings{blair1993,
  title = {An {{Introduction}} to {{Chordal Graphs}} and {{Clique Trees}}},
  booktitle = {Graph {{Theory}} and {{Sparse Matrix Computation}}},
  author = {Blair, Jean R. S. and Peyton, Barry},
  editor = {George, Alan and Gilbert, John R. and Liu, Joseph W. H.},
  year = {1993},
  series = {The {{IMA Volumes}} in {{Mathematics}} and Its {{Applications}}},
  pages = {1--29},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4613-8369-7\_1},
  abstract = {Clique trees and chordal graphs have carved out a niche for themselves in recent work on sparse matrix algorithms, due primarily to research questions associated with advanced computer architectures. This paper is a unified and elementary introduction to the standard characterizations of chordal graphs and clique trees. The pace is leisurely, as detailed proofs of all results are included. We also briefly discuss applications of chordal graphs and clique trees in sparse matrix computations.},
  isbn = {978-1-4613-8369-7},
  langid = {english},
  keywords = {acyclic hypergraphs,Cholesky factorization,chordal graphs,clique trees,maximum cardinality search,minimum spanning tree,Prim's algorithm,sparse linear systems},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Blair and Peyton - 1993 - An Introduction to Chordal Graphs and Clique Trees.pdf}
}

@article{bleuler2020,
  title = {Conditional {{R\'enyi Divergences}} and {{Horse Betting}}},
  author = {Bleuler, C{\'e}dric and Lapidoth, Amos and Pfister, Christoph},
  year = {2020},
  month = mar,
  journal = {Entropy},
  volume = {22},
  number = {3},
  pages = {316},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1099-4300},
  doi = {10.3390/e22030316},
  urldate = {2022-08-02},
  abstract = {Motivated by a horse betting problem, a new conditional R\'enyi divergence is introduced. It is compared with the conditional R\'enyi divergences that appear in the definitions of the dependence measures by Csisz\'ar and Sibson, and the properties of all three are studied with emphasis on their behavior under data processing. In the same way that Csisz\'ar's and Sibson's conditional divergence lead to the respective dependence measures, so does the new conditional divergence lead to the Lapidoth\textendash Pfister mutual information. Moreover, the new conditional divergence is also related to the Arimoto\textendash R\'enyi conditional entropy and to Arimoto's measure of dependence. In the second part of the paper, the horse betting problem is analyzed where, instead of Kelly's expected log-wealth criterion, a more general family of power-mean utility functions is considered. The key role in the analysis is played by the R\'enyi divergence, and in the setting where the gambler has access to side information, the new conditional R\'enyi divergence is key. The setting with side information also provides another operational meaning to the Lapidoth\textendash Pfister mutual information. Finally, a universal strategy for independent and identically distributed races is presented that\textemdash without knowing the winning probabilities or the parameter of the utility function\textemdash asymptotically maximizes the gambler's utility function.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {conditional R\'enyi divergence,horse betting,Kelly gambling,R\'enyi divergence,R\'enyi mutual information},
  file = {D\:\\work\\literature\\library\\Bleuler et al. - 2020 - Conditional Rényi Divergences and Horse Betting.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\GH7RJZEI\\316.html}
}

@inproceedings{blier2020,
  title = {Learning with {{Random Learning Rates}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Blier, L{\'e}onard and Wolinski, Pierre and Ollivier, Yann},
  editor = {Brefeld, Ulf and Fromont, Elisa and Hotho, Andreas and Knobbe, Arno and Maathuis, Marloes and Robardet, C{\'e}line},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {449--464},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-46147-8\_27},
  abstract = {In neural network optimization, the learning rate of the gradient descent strongly affects performance. This prevents reliable out-of-the-box training of a model on a new problem. We propose the All Learning Rates At Once (Alrao) algorithm for deep learning architectures: each neuron or unit in the network gets its own learning rate, randomly sampled at startup from a distribution spanning several orders of magnitude. The network becomes a mixture of slow and fast learning units. Surprisingly, Alrao performs close to SGD with an optimally tuned learning rate, for various tasks and network architectures. In our experiments, all Alrao runs were able to learn well without any tuning.},
  isbn = {978-3-030-46147-8},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Blier et al. - 2020 - Learning with Random Learning Rates.pdf}
}

@article{bonnici2020,
  title = {Kullback-{{Leibler}} Divergence between Quantum Distributions, and Its Upper-Bound},
  author = {Bonnici, Vincenzo},
  year = {2020},
  month = dec,
  journal = {arXiv:2008.05932 [quant-ph]},
  eprint = {2008.05932},
  primaryclass = {quant-ph},
  urldate = {2022-03-01},
  abstract = {This work presents an upper-bound to value that the Kullback-Leibler (KL) divergence can reach for a class of probability distributions called quantum distributions (QD). The aim is to find a distribution \$U\$ which maximizes the KL divergence from a given distribution \$P\$ under the assumption that \$P\$ and \$U\$ have been generated by distributing a given discrete quantity, a quantum. Quantum distributions naturally represent a wide range of probability distributions that are used in practical applications. Moreover, such a class of distributions can be obtained as an approximation of any probability distribution. The retrieving of an upper-bound for the entropic divergence is here shown to be possible under the condition that the compared distributions are quantum distributions over the same quantum value, thus they become comparable. Thus, entropic divergence acquires a more powerful meaning when it is applied to comparable distributions. This aspect should be taken into account in future developments of divergences. The theoretical findings are used for proposing a notion of normalized KL divergence that is empirically shown to behave differently from already known measures.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Quantum Physics},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Bonnici - 2020 - Kullback-Leibler divergence between quantum distri.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\K3SSI54V\\2008.html}
}

@inproceedings{boracchi2018,
  title = {{{QuantTree}}: {{Histograms}} for {{Change Detection}} in {{Multivariate Data Streams}}},
  shorttitle = {{{QuantTree}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Boracchi, Giacomo and Carrera, Diego and Cervellera, Cristiano and Macci{\`o}, Danilo},
  year = {2018},
  month = jul,
  pages = {639--648},
  urldate = {2020-02-03},
  abstract = {We address the problem of detecting distribution changes in multivariate data streams by means of histograms. Histograms are very general and flexible models, which have been relatively ignored in ...},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Boracchi et al. - 2018 - QuantTree Histograms for Change Detection in Mult.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\Q2GX8CEU\\boracchi18a.html}
}

@inproceedings{bottou1994,
  title = {Comparison of Classifier Methods: A Case Study in Handwritten Digit Recognition},
  shorttitle = {Comparison of Classifier Methods},
  booktitle = {Proceedings of the 12th {{IAPR International Conference}} on {{Pattern Recognition}}, {{Vol}}. 3 - {{Conference C}}: {{Signal Processing}} ({{Cat}}. {{No}}.{{94CH3440-5}})},
  author = {Bottou, L. and Cortes, C. and Denker, J.S. and Drucker, H. and Guyon, I. and Jackel, L.D. and LeCun, Y. and Muller, U.A. and Sackinger, E. and Simard, P. and Vapnik, V.},
  year = {1994},
  month = oct,
  volume = {2},
  pages = {77-82 vol.2},
  doi = {10.1109/ICPR.1994.576879},
  abstract = {This paper compares the performance of several classifier algorithms on a standard database of handwritten digits. We consider not only raw accuracy, but also training time, recognition time, and memory requirements. When available, we report measurements of the fraction of patterns that must be rejected so that the remaining patterns have misclassification rates less than a given threshold.},
  keywords = {Computer aided software engineering,Databases,Handwriting recognition,Laboratories,Machine learning,NIST,Pattern recognition,Testing,Time measurement,Training data},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Bottou et al. - 1994 - Comparison of classifier methods a case study in .pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\LH7NABF9\\576879.html}
}

@article{bregman1967,
  title = {The Relaxation Method of Finding the Common Point of Convex Sets and Its Application to the Solution of Problems in Convex Programming},
  author = {Bregman, L. M.},
  year = {1967},
  month = jan,
  journal = {USSR Computational Mathematics and Mathematical Physics},
  volume = {7},
  number = {3},
  pages = {200--217},
  issn = {0041-5553},
  doi = {10.1016/0041-5553(67)90040-7},
  urldate = {2023-03-30},
  abstract = {IN this paper we consider an iterative method of finding the common point of convex sets. This method can be regarded as a generalization of the methods discussed in [1\textendash 4]. Apart from problems which can be reduced to finding some point of the intersection of convex sets, the method considered can be applied to the approximate solution of problems in linear and convex programming.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Bregman - 1967 - The relaxation method of finding the common point .pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\22CZ4B37\\0041555367900407.html}
}

@article{breiman2001,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  year = {2001},
  month = oct,
  journal = {Machine Learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  issn = {1573-0565},
  doi = {10.1023/A:1010933404324},
  urldate = {2023-06-20},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\textendash 156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  langid = {english},
  keywords = {classification,ensemble,regression},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\2UB6NTR8\\Breiman - 2001 - Random Forests.pdf;D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Breiman - 2001 - Random Forests.pdf}
}

@article{brzezinski2014,
  title = {Reacting to {{Different Types}} of {{Concept Drift}}: {{The Accuracy Updated Ensemble Algorithm}}},
  shorttitle = {Reacting to {{Different Types}} of {{Concept Drift}}},
  author = {Brzezinski, D. and Stefanowski, J.},
  year = {2014},
  month = jan,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {25},
  number = {1},
  pages = {81--94},
  issn = {2162-237X},
  doi = {10.1109/TNNLS.2013.2251352},
  abstract = {Data stream mining has been receiving increased attention due to its presence in a wide range of applications, such as sensor networks, banking, and telecommunication. One of the most important challenges in learning from data streams is reacting to concept drift, i.e., unforeseen changes of the stream's underlying data distribution. Several classification algorithms that cope with concept drift have been put forward, however, most of them specialize in one type of change. In this paper, we propose a new data stream classifier, called the Accuracy Updated Ensemble (AUE2), which aims at reacting equally well to different types of drift. AUE2 combines accuracy-based weighting mechanisms known from block-based ensembles with the incremental nature of Hoeffding Trees. The proposed algorithm is experimentally compared with 11 state-of-the-art stream methods, including single classifiers, block-based and online ensembles, and hybrid approaches in different drift scenarios. Out of all the compared algorithms, AUE2 provided best average classification accuracy while proving to be less memory consuming than other ensemble approaches. Experimental results show that AUE2 can be considered suitable for scenarios, involving many types of drift as well as static environments.},
  keywords = {accuracy updated ensemble algorithm,accuracy-based weighting mechanisms,AUE2,block-based ensembles,classification algorithms,concept drift,Concept drift,data mining,data stream classifier,data stream learning,data stream mining,ensemble classifier,Hoeffding trees,learning (artificial intelligence),nonstationary environments,online ensembles,pattern classification,single classifiers,trees (mathematics)},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Brzezinski and Stefanowski - 2014 - Reacting to Different Types of Concept Drift The .pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\SBAGZQXX\\6494309.html}
}

@article{bu2018,
  title = {Estimation of {{KL Divergence}}: {{Optimal Minimax Rate}}},
  shorttitle = {Estimation of {{KL Divergence}}},
  author = {Bu, Yuheng and Zou, Shaofeng and Liang, Yingbin and Veeravalli, Venugopal V.},
  year = {2018},
  month = apr,
  journal = {IEEE Transactions on Information Theory},
  volume = {64},
  number = {4},
  pages = {2648--2674},
  issn = {1557-9654},
  doi = {10.1109/TIT.2018.2805844},
  abstract = {The problem of estimating the Kullback-Leibler divergence D(P{$\parallel$}Q) between two unknown distributions P and Q is studied, under the assumption that the alphabet size k of the distributions can scale to infinity. The estimation is based on m independent samples drawn from P and n independent samples drawn from Q. It is first shown that there does not exist any consistent estimator that guarantees asymptotically small worst case quadratic risk over the set of all pairs of distributions. A restricted set that contains pairs of distributions, with density ratio bounded by a function f (k) is further considered. An augmented plug-in estimator is proposed, and its worst case quadratic risk is shown to be within a constant factor of ((k/m) + (kf (k)/n))2 + (log2f (k)/m) + ( f (k)/n), if m and n exceed a constant factor of k and kf (k), respectively. Moreover, the minimax quadratic risk is characterized to be within a constant factor of ((k/(m log k)) + (kf (k)/(n log k)))2 + (log2f (k)/m) + ( f (k)/n), if m and n exceed a constant factor of k/ log(k) and kf (k)/ log k, respectively. The lower bound on the minimax quadratic risk is characterized by employing a generalized Le Cam's method. A minimax optimal estimator is then constructed by employing both the polynomial approximation and the plug-in approaches.},
  keywords = {augmented plug-in estimator,Complexity theory,constant factor,density ratio,Electronic mail,Entropy,Estimation,estimation theory,Functional approximation,Histograms,Information theory,KL divergence,Kullback-Leibler divergence estimation,Le Cam's method,mean squared error,minimax lower bound,minimax optimal estimator,minimax quadratic risk,minimax techniques,optimal minimax rate,plug-in estimator,polynomial approximation,Upper bound},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Bu et al. - 2018 - Estimation of KL Divergence Optimal Minimax Rate.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\2FJM97TY\\8291025.html}
}

@inproceedings{buntine2014,
  title = {Experiments with {{Non-parametric Topic Models}}},
  booktitle = {Proceedings of the 20th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Buntine, Wray L. and Mishra, Swapnil},
  year = {2014},
  series = {{{KDD}} '14},
  pages = {881--890},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2623330.2623691},
  urldate = {2019-08-15},
  abstract = {In topic modelling, various alternative priors have been developed, for instance asymmetric and symmetric priors for the document-topic and topic-word matrices respectively, the hierarchical Dirichlet process prior for the document-topic matrix and the hierarchical Pitman-Yor process prior for the topic-word matrix. For information retrieval, language models exhibiting word burstiness are important. Indeed, this burstiness effect has been show to help topic models as well, and this requires additional word probability vectors for each document. Here we show how to combine these ideas to develop high-performing non-parametric topic models exhibiting burstiness based on standard Gibbs sampling. Experiments are done to explore the behavior of the models under different conditions and to compare the algorithms with previously published. The full non-parametric topic models with burstiness are only a small factor slower than standard Gibbs sampling for LDA and require double the memory, making them very competitive. We look at the comparative behaviour of different models and present some experimental insights.},
  isbn = {978-1-4503-2956-9},
  keywords = {experimental results,non-parametric prior,text,topic modelling},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Buntine and Mishra - 2014 - Experiments with Non-parametric Topic Models.pdf}
}

@article{cai2006,
  title = {Universal {{Divergence Estimation}} for {{Finite-Alphabet Sources}}},
  author = {Cai, H. and Kulkarni, S. R. and Verdu, S.},
  year = {2006},
  month = aug,
  journal = {IEEE Transactions on Information Theory},
  volume = {52},
  number = {8},
  pages = {3456--3475},
  issn = {1557-9654},
  doi = {10.1109/TIT.2006.878182},
  abstract = {This paper studies universal estimation of divergence from the realizations of two unknown finite-alphabet sources. Two algorithms that borrow techniques from data compression are presented. The first divergence estimator applies the Burrows-Wheeler block sorting transform to the concatenation of the two realizations; consistency of this estimator is shown for all finite-memory sources. The second divergence estimator is based on the Context Tree Weighting method; consistency is shown for all sources whose memory length does not exceed a known bound. Experimental results show that both algorithms perform similarly and outperform string-matching and plug-in methods},
  keywords = {Bioinformatics,Block sorting,Burrows-Wheeler block sorting,Burrows\textendash Wheeler transform,context tree weighting method,Convergence,data compression,Data compression,divergence estimation,Entropy,finite-alphabet source,Genomics,information divergence,Markov processes,Markov sources,Mutual information,Phylogeny,sorting,Sorting,Source coding,universal divergence estimation,universal methods},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Cai et al. - 2006 - Universal Divergence Estimation for Finite-Alphabe.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\IECLHBC5\\1661829.html;C\:\\Users\\Loong Kuan\\Zotero\\storage\\ZE5KY3VQ\\1661829.html}
}

@article{cai2019,
  title = {Conditional {{R\'enyi Divergence Saddlepoint}} and the {{Maximization}} of {$\alpha$}-{{Mutual Information}}},
  author = {Cai, Changxiao and Verd{\'u}, Sergio},
  year = {2019},
  month = oct,
  journal = {Entropy},
  volume = {21},
  number = {10},
  pages = {969},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1099-4300},
  doi = {10.3390/e21100969},
  urldate = {2022-08-08},
  abstract = {R\'enyi-type generalizations of entropy, relative entropy and mutual information have found numerous applications throughout information theory and beyond. While there is consensus that the ways A. R\'enyi generalized entropy and relative entropy in 1961 are the ``right'' ones, several candidates have been put forth as possible mutual informations of order    {$\alpha$}   . In this paper we lend further evidence to the notion that a Bayesian measure of statistical distinctness introduced by R. Sibson in 1969 (closely related to Gallager's     E 0     function) is the most natural generalization, lending itself to explicit computation and maximization, as well as closed-form formulas. This paper considers general (not necessarily discrete) alphabets and extends the major analytical results on the saddle-point and saddle-level of the conditional relative entropy to the conditional R\'enyi divergence. Several examples illustrate the main application of these results, namely, the maximization of    {$\alpha$}   -mutual information with and without constraints.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {channel capacity,conditional relative entropy,information measures,minimax redundancy,mutual information,relative entropy,R\'enyi divergence,{$\alpha$}-mutual information},
  file = {D\:\\work\\literature\\library\\Cai and Verdú - 2019 - Conditional Rényi Divergence Saddlepoint and the M.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\IZVDUKGS\\htm.html}
}

@article{cao2014,
  title = {Hybrid Probabilistic Sampling with Random Subspace for Imbalanced Data Learning},
  author = {Cao, Peng and Zhao, Dazhe and Zaiane, Osmar},
  year = {2014},
  month = oct,
  journal = {Intelligent Data Analysis},
  volume = {18},
  number = {6},
  pages = {1089--1108},
  issn = {15714128, 1088467X},
  doi = {10.3233/IDA-140686},
  urldate = {2023-03-22},
  abstract = {Class imbalance is one of the challenging problems for machine learning in many real-world applications. Other issues, such as within-class imbalance and high dimensionality, can exacerbate the problem. We propose a method HPS-DRS that combines two ideas: Hybrid Probabilistic Sampling technique ensemble with Diverse Random Subspace to address these issues. HPS improves the performance of traditional re-sampling algorithms with the aid of probability function, since it is not sufficient to simply manipulate the class sizes for imbalanced data with complex distribution. Moreover, DRS ensemble employs the minimum overlapping mechanism to provide diversity and weighted voting, so as to improve the generalization performance. The experimental results demonstrate that our method is efficient for learning from imbalanced data and can achieve better results than state-of-the-art methods for imbalanced data.},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Cao et al. - 2014 - Hybrid probabilistic sampling with random subspace.pdf}
}

@article{capdevila2018,
  title = {Experiments with Learning Graphical Models on Text},
  author = {Capdevila, Joan and Zhao, He and Petitjean, Fran{\c c}ois and Buntine, Wray},
  year = {2018},
  month = oct,
  journal = {Behaviormetrika},
  volume = {45},
  number = {2},
  pages = {363--387},
  issn = {1349-6964},
  doi = {10.1007/s41237-018-0050-3},
  urldate = {2019-08-28},
  abstract = {A rich variety of models are now in use for unsupervised modelling of text documents, and, in particular, a rich variety of graphical models exist, with and without latent variables. To date, there is inadequate understanding about the comparative performance of these, partly because they are subtly different, and they have been proposed and evaluated in different contexts. This paper reports on our experiments with a representative set of state of the art models: chordal graphs, matrix factorisation, and hierarchical latent tree models. For the chordal graphs, we use different scoring functions. For matrix factorisation models, we use different hierarchical priors, asymmetric priors on components. We use Boolean matrix factorisation rather than topic models, so we can do comparable evaluations. The experiments perform a number of evaluations: probability for each document, omni-directional prediction which predicts different variables, and anomaly detection. We find that matrix factorisation performed well at anomaly detection but poorly on the prediction task. Chordal graph learning performed the best generally, and probably due to its lower bias, often out-performed hierarchical latent trees.},
  langid = {english},
  keywords = {Document analysis,Evaluation,Graphical models,Latent variables,Matrix factorisation,Unsupervised learning},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Capdevila et al. - 2018 - Experiments with learning graphical models on text.pdf}
}

@article{carrera2018,
  title = {Generating {{High-Dimensional Datastreams}} for {{Change Detection}}},
  author = {Carrera, Diego and Boracchi, Giacomo},
  year = {2018},
  month = mar,
  journal = {Big Data Research},
  series = {Selected Papers from the 2nd {{INNS Conference}} on {{Big Data}}: {{Big Data}} \& {{Neural Networks}}},
  volume = {11},
  pages = {11--21},
  issn = {2214-5796},
  doi = {10.1016/j.bdr.2017.09.001},
  urldate = {2020-02-03},
  abstract = {A popular testbed for change-detection algorithms consists in detecting changes that have been synthetically injected in real-world datastreams. Unfortunately, most of experimental practices in the literature lead to injecting changes whose magnitude is unknown and can not be controlled. As a consequence, results are difficult to interpret, reproduce, and compare with. Most importantly, controlling the change magnitude is a primary requirement to investigate the change-detection performance when data dimension scales, which is an issue to be typically addressed in big data scenarios. Here we present a best practice to inject changes in multivariate/high-dimensional datastreams: ``Controlling Change Magnitude'' (CCM) is a rigorous method to generate datastreams affected by a change having a desired magnitude at a known location. In CCM, changes are introduced by directly applying a roto-translation to the data, and the change magnitude is measured by the symmetric Kullback\textendash Leibler divergence between the pre- and post-change data distributions. The roto-translation parameters yielding the desired change magnitude are identified by two iterative algorithms whose convergence is here proven. Our experiments show that CCM can effectively control the change magnitude in real-world datastreams, while traditional experimental practices might not be appropriate for assessing the performance of change-detection algorithms in high-dimensional data.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Carrera and Boracchi - 2018 - Generating High-Dimensional Datastreams for Change.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\VCTMMEHD\\S2214579617300163.html}
}

@incollection{carrera2020,
  title = {Learning and {{Adaptation}} to {{Detect Changes}} and {{Anomalies}} in {{High-Dimensional Data}}},
  booktitle = {Special {{Topics}} in {{Information Technology}}},
  author = {Carrera, Diego},
  editor = {Pernici, Barbara},
  year = {2020},
  series = {{{SpringerBriefs}} in {{Applied Sciences}} and {{Technology}}},
  pages = {63--75},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-32094-2\_5},
  urldate = {2020-02-03},
  abstract = {The problem of monitoring a datastream and detecting whether the data generating process changes from normal to novel and possibly anomalous conditions has relevant applications in many real scenarios, such as health monitoring and quality inspection of industrial processes. A general approach often adopted in the literature is to learn a model to describe normal data and detect as anomalous those data that do not conform to the learned model. However, several challenges have to be addressed to make this approach effective in real world scenarios, where acquired data are often characterized by high dimension and feature complex structures (such as signals and images). We address this problem from two perspectives corresponding to different modeling assumptions on the data-generating process. At first, we model data as realization of random vectors, as it is customary in the statistical literature. In this settings we focus on the change detection problem, where the goal is to detect whether the datastream permanently departs from normal conditions. We theoretically prove the intrinsic difficulty of this problem when the data dimension increases and propose a novel non-parametric and multivariate change-detection algorithm. In the second part, we focus on data having complex structure and we adopt dictionaries yielding sparse representations to model normal data. We propose novel algorithms to detect anomalies in such datastreams and to adapt the learned model when the process generating normal data changes.},
  isbn = {978-3-030-32094-2},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Carrera - 2020 - Learning and Adaptation to Detect Changes and Anom.pdf}
}

@book{casella2001,
  title = {Statistical Inference},
  author = {Casella, George and Berger, Roger L.},
  year = {2001},
  month = jun,
  edition = {2nd ed},
  publisher = {{Cengage Learning}},
  address = {{Australia ; Pacific Grove, CA}},
  isbn = {978-0-534-24312-8},
  keywords = {Mathematical statistics,Probabilities},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Casella and Berger - 2001 - Statistical inference.pdf}
}

@article{casey2008,
  title = {Analysis of {{Minimum Distances}} in {{High-Dimensional Musical Spaces}}},
  author = {Casey, Michael and Rhodes, Christophe and Slaney, Malcolm},
  year = {2008},
  month = jul,
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {16},
  number = {5},
  pages = {1015--1028},
  issn = {1558-7916},
  doi = {10.1109/TASL.2008.925883},
  urldate = {2020-02-02},
  abstract = {We propose an automatic method for measuring content-based music similarity, enhancing the current generation of music search engines and recommender systems. Many previous approaches to track similarity require brute-force, pair-wise processing between all audio features in a database and therefore are not practical for large collections. However, in an Internet-connected world, where users have access to millions of musical tracks, efficiency is crucial. Our approach uses features extracted from unlabeled audio data and near-neigbor retrieval using a distance threshold, determined by analysis, to solve a range of retrieval tasks. The tasks require temporal features\textemdash analogous to the technique of shingling used for text retrieval. To measure similarity, we count pairs of audio shingles, between a query and target track, that are below a distance threshold. The distribution of between-shingle distances is different for each database; therefore, we present an analysis of the distribution of minimum distances between shingles and a method for estimating a distance threshold for optimal retrieval performance. The method is compatible with locality-sensitive hashing (LSH)\textemdash allowing implementation with retrieval times several orders of magnitude faster than those using exhaustive distance computations. We evaluate the performance of our proposed method on three contrasting music similarity tasks: retrieval of mis-attributed recordings (fingerprint), retrieval of the same work performed by different artists (cover songs), and retrieval of edited and sampled versions of a query track by remix artists (remixes). Our method achieves near-perfect performance in the first two tasks and 75\% precision at 70\% recall in the third task. Each task was performed on a test database comprising 4.5 million audio shingles.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Casey et al. - 2008 - Analysis of Minimum Distances in High-Dimensional .pdf}
}

@article{cattral2002,
  title = {Evolutionary {{Data Mining With Automatic Rule Generalization}}},
  author = {Cattral, Robert and Oppacher, Franz and Deugo, Dwight},
  year = {2002},
  pages = {5},
  abstract = {This paper describes RAGA, a data mining system that combines evolutionary and symbolic machine learning methods, and discusses recent extensions required to extract comprehensible and strong rules from a very challenging dataset. RAGA relies on evolutionary search to highlight strong rules to which symbolic generalization techniques are applied between generations. We present some experimental results and a comparison of RAGA with other data mining systems.},
  langid = {english},
  keywords = {dataset,poker-hand},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Cattral et al. - 2002 - Evolutionary Data Mining With Automatic Rule Gener.pdf}
}

@incollection{cerf1997,
  title = {Negative {{Entropy}} in {{Quantum Information Theory}}},
  booktitle = {New {{Developments}} on {{Fundamental Problems}} in {{Quantum Physics}}},
  author = {Cerf, Nicolas J. and Adami, Chris},
  editor = {Ferrero, Miguel and {van der Merwe}, Alwyn},
  year = {1997},
  series = {Fundamental {{Theories}} of {{Physics}}},
  pages = {77--84},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-011-5886-2\_11},
  urldate = {2022-08-15},
  abstract = {We present a quantum information theory that allows for the consistent description of quantum entanglement. It parallels classical (Shannon) information theory but is based entirely on density matrices, rather than probability distributions, for the description of quantum ensembles. We find that, unlike in Shannon theory, conditional entropies can be negative when considering quantum entangled systems such as an Einstein-Podolsky-Rosen pair, which leads to a violation of well-known bounds of classical information theory. Negative quantum entropy can be traced back to ``conditional'' density matrices which admit eigenvalues larger than unity. A straightforward definition of mutual quantum entropy, or ``mutual entanglement,'' can also be constructed using a ``mutual'' density matrix. Such a unified information-theoretic description of classical correlation and quantum entanglement clarifies the link between them: the latter can be viewed as ``super-correlation'' which can induce classical correlation when considering a ternary or larger system.},
  isbn = {978-94-011-5886-2},
  langid = {english},
  keywords = {quantum entanglement,quantum entropy,quantum information theory},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Cerf and Adami - 1997 - Negative Entropy in Quantum Information Theory.pdf}
}

@article{cervantes2018,
  title = {Evaluating and {{Characterizing Incremental Learning}} from {{Non-Stationary Data}}},
  author = {Cervantes, Alejandro and Gagn{\'e}, Christian and Isasi, Pedro and Parizeau, Marc},
  year = {2018},
  month = jun,
  urldate = {2022-03-03},
  abstract = {Incremental learning from non-stationary data poses special challenges to the field of machine learning. Although new algorithms have been developed for this, assessment of results and comparison of behaviors are still open problems, mainly because evaluation metrics, adapted from more traditional tasks, can be ineffective in this context. Overall, there is a lack of common testing practices. This paper thus presents a testbed for incremental non-stationary learning algorithms, based on specially designed synthetic datasets. Also, test results are reported for some well-known algorithms to show that the proposed methodology is effective at characterizing their strengths and weaknesses. It is expected that this methodology will provide a common basis for evaluating future contributions in the field.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Cervantes et al. - 2018 - Evaluating and Characterizing Incremental Learning.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\UW3Z5ATE\\1806.html}
}

@inproceedings{charikar2019,
  title = {Efficient Profile Maximum Likelihood for Universal Symmetric Property Estimation},
  booktitle = {Proceedings of the 51st {{Annual ACM SIGACT Symposium}} on {{Theory}} of {{Computing}}},
  author = {Charikar, Moses and Shiragur, Kirankumar and Sidford, Aaron},
  year = {2019},
  month = jun,
  series = {{{STOC}} 2019},
  pages = {780--791},
  publisher = {{Association for Computing Machinery}},
  address = {{Phoenix, AZ, USA}},
  doi = {10.1145/3313276.3316398},
  urldate = {2020-05-12},
  abstract = {Estimating symmetric properties of a distribution, e.g. support size, coverage, entropy, distance to uniformity, are among the most fundamental problems in algorithmic statistics. While these properties have been studied extensively and separate optimal estimators have been produced, in striking recent work Acharya et al. provided a single estimator that is competitive for each. They showed that the value of the property on the distribution that approximately maximizes profile likelihood (PML), i.e. the probability of observed frequency of frequencies, is sample competitive with respect to a broad class of estimators. Unfortunately, prior to this work, there was no known polynomial time algorithm to compute such an approximation or use PML to obtain a universal plug-in estimator. In this paper we provide an algorithm that, given n samples from a distribution, computes an approximate PML distribution up to a multiplicative error of exp(n2/3poly log(n)) in nearly linear time. Generalizing work of Acharya et al. we show that our algorithm yields a universal plug-in estimator that is competitive with a broad range of estimators up to accuracy \cyrchar\cyrie{} = {$\Omega$}(n-0.166). Further, we provide efficient polynomial-time algorithms for computing a d-dimensional generalization of PML (for constant d) that allows for universal plug-in estimation of symmetric relationships between distributions.},
  isbn = {978-1-4503-6705-9},
  keywords = {Profile maximum likelihood,symmetric property estimation,universal estimator},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Charikar et al. - 2019 - Efficient profile maximum likelihood for universal.pdf;D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Charikar et al. - 2019 - Efficient profile maximum likelihood for universal2.pdf}
}

@article{chen2020,
  title = {Aggregated {{Wasserstein Distance}} and {{State Registration}} for {{Hidden Markov Models}}},
  author = {Chen, Yukun and Ye, Jianbo and Li, Jia},
  year = {2020},
  month = sep,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {42},
  number = {9},
  pages = {2133--2147},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2019.2908635},
  abstract = {We propose a framework, named Aggregated Wasserstein, for computing a dissimilarity measure or distance between two Hidden Markov Models with state conditional distributions being Gaussian. For such HMMs, the marginal distribution at any time position follows a Gaussian mixture distribution, a fact exploited to softly match, aka register, the states in two HMMs. We refer to such HMMs as HMM. The registration of states is inspired by the intrinsic relationship of optimal transport and the Wasserstein metric between distributions. Specifically, the components of the marginal GMMs are matched by solving an optimal transport problem where the cost between components is the Wasserstein metric for Gaussian distributions. The solution of the optimization problem is a fast approximation to the Wasserstein metric between two GMMs. The new Aggregated Wasserstein distance is a semi-metric and can be computed without generating Monte Carlo samples. It is invariant to relabeling or permutation of states. The distance is defined meaningfully even for two HMMs that are estimated from data of different dimensionality, a situation that can arise due to missing variables. This distance quantifies the dissimilarity of HMMs by measuring both the difference between the two marginal GMMs and that between the two transition matrices. Our new distance is tested on tasks of retrieval, classification, and t-SNE visualization of time series. Experiments on both synthetic and real data have demonstrated its advantages in terms of accuracy as well as efficiency in comparison with existing distances based on the Kullback-Leibler divergence.},
  keywords = {Approximation methods,Computational modeling,Gaussian distribution,Gaussian mixture model,Hidden Markov model,Hidden Markov models,Markov processes,Measurement,Monte Carlo methods,optimal transport,Wasserstein distance},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\JX5G4P7T\\8678434.html}
}

@inproceedings{choi2005,
  title = {On {{Bayesian}} Network Approximation by Edge Deletion},
  booktitle = {Proceedings of the {{Twenty-First Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Choi, Arthur and Chan, Hei and Darwiche, Adnan},
  year = {2005},
  month = jul,
  series = {{{UAI}}'05},
  pages = {128--135},
  publisher = {{AUAI Press}},
  address = {{Arlington, Virginia, USA}},
  urldate = {2022-07-28},
  abstract = {We consider the problem of deleting edges from a Bayesian network for the purpose of simplifying models in probabilistic inference. In particular, we propose a new method for deleting network edges, which is based on the evidence at hand. We provide some interesting bounds on the KL-divergence between original and approximate networks, which highlight the impact of given evidence on the quality of approximation and shed some light on good and bad candidates for edge deletion. We finally demonstrate empirically the promise of the proposed edge deletion technique as a basis for approximate inference.},
  isbn = {978-0-9749039-1-0},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Choi et al. - 2005 - On Bayesian network approximation by edge deletion.pdf}
}

@book{christensen2006,
  title = {Log-{{Linear Models}} and {{Logistic Regression}}},
  author = {Christensen, Ronald},
  year = {2006},
  month = apr,
  publisher = {{Springer Science \& Business Media}},
  abstract = {As the new title indicates, this second edition of Log-Linear Models has been modi?ed to place greater emphasis on logistic regression. In addition to new material, the book has been radically rearranged. The fundamental material is contained in Chapters 1-4. Intermediate topics are presented in Chapters 5 through 8. Generalized linear models are presented in Ch- ter 9. The matrix approach to log-linear models and logistic regression is presented in Chapters 10-12, with Chapters 10 and 11 at the applied Ph.D. level and Chapter 12 doing theory at the Ph.D. level. The largest single addition to the book is Chapter 13 on Bayesian bi- mial regression. This chapter includes not only logistic regression but also probit and complementary log-log regression. With the simplicity of the Bayesian approach and the ability to do (almost) exact small sample s- tistical inference, I personally ?nd it hard to justify doing traditional large sample inferences. (Another possibility is to do exact conditional inference, but that is another story.) Naturally,Ihavecleaneduptheminor?awsinthetextthatIhavefound. All examples, theorems, proofs, lemmas, etc. are numbered consecutively within each section with no distinctions between them, thus Example 2.3.1 willcomebeforeProposition2.3.2.Exercisesthatdonotappearinasection at the end have a separate numbering scheme. Within the section in which it appears, an equation is numbered with a single value, e.g., equation (1).},
  googlebooks = {vifrBwAAQBAJ},
  isbn = {978-0-387-22624-8},
  langid = {english},
  keywords = {Mathematics / Algebra / General,Mathematics / Applied,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes}
}

@article{cichocki2010,
  title = {Families of {{Alpha- Beta-}} and {{Gamma- Divergences}}: {{Flexible}} and {{Robust Measures}} of {{Similarities}}},
  shorttitle = {Families of {{Alpha- Beta-}} and {{Gamma- Divergences}}},
  author = {Cichocki, Andrzej and Amari, Shun-ichi},
  year = {2010},
  month = jun,
  journal = {Entropy},
  volume = {12},
  number = {6},
  pages = {1532--1568},
  publisher = {{Molecular Diversity Preservation International}},
  doi = {10.3390/e12061532},
  urldate = {2020-03-18},
  abstract = {In this paper, we extend and overview wide families of Alpha-, Beta- and Gamma-divergences and discuss their fundamental properties. In literature usually only one single asymmetric (Alpha, Beta or Gamma) divergence is considered. We show in this paper that there exist families of such divergences with the same consistent properties. Moreover, we establish links and correspondences among these divergences by applying suitable nonlinear transformations. For example, we can generate the Beta-divergences directly from Alpha-divergences and vice versa. Furthermore, we show that a new wide class of Gamma-divergences can be generated not only from the family of Beta-divergences but also from a family of Alpha-divergences. The paper bridges these divergences and shows also their links to Tsallis and R\'enyi entropies. Most of these divergences have a natural information theoretic interpretation.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Csisz\'ar\textendash Morimoto and Bregman divergences,extended Itakura\textendash Saito like divergences,generalized divergences,Similarity measures,Tsallis and R\'enyi entropies},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Cichocki and Amari - 2010 - Families of Alpha- Beta- and Gamma- Divergences F.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\3EFJX478\\1532.html}
}

@article{cichocki2011,
  title = {Generalized {{Alpha-Beta Divergences}} and {{Their Application}} to {{Robust Nonnegative Matrix Factorization}}},
  author = {Cichocki, Andrzej and Cruces, Sergio and Amari, Shun-ichi},
  year = {2011},
  month = jan,
  journal = {Entropy},
  volume = {13},
  number = {1},
  pages = {134--170},
  publisher = {{Molecular Diversity Preservation International}},
  doi = {10.3390/e13010134},
  urldate = {2020-06-30},
  abstract = {We propose a class of multiplicative algorithms for Nonnegative Matrix Factorization (NMF) which are robust with respect to noise and outliers. To achieve this, we formulate a new family generalized divergences referred to as the Alpha-Beta-divergences (AB-divergences), which are parameterized by the two tuning parameters, alpha and beta, and smoothly connect the fundamental Alpha-, Beta- and Gamma-divergences. By adjusting these tuning parameters, we show that a wide range of standard and new divergences can be obtained. The corresponding learning algorithms for NMF are shown to integrate and generalize many existing ones, including the Lee-Seung, ISRA (Image Space Reconstruction Algorithm), EMML (Expectation Maximization Maximum Likelihood), Alpha-NMF, and Beta-NMF. Owing to more degrees of freedom in tuning the parameters, the proposed family of AB-multiplicative NMF algorithms is shown to improve robustness with respect to noise and outliers. The analysis illuminates the links of between AB-divergence and other divergences, especially Gamma- and Itakura-Saito divergences.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Alpha-,Beta-,extended Itakura-Saito like divergences,Gamma- divergences,generalized divergences,generalized Kullback-Leibler divergence,nonnegative matrix factorization (NMF),robust multiplicative NMF algorithms,similarity measures},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Cichocki et al. - 2011 - Generalized Alpha-Beta Divergences and Their Appli.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\22A6V8K3\\htm.html}
}

@inproceedings{clifford1990,
  title = {Markov Random Fields in Statistics},
  booktitle = {Disorder in {{Physical Systems}}. {{A Volume}} in {{Honour}} of {{John M}}. {{Hammersley}}},
  author = {Clifford, Peter},
  year = {1990},
  publisher = {{Clarendon Press}},
  abstract = {For nearly a century, statisticians have been intrigued by the problems of developing a satisfactory methodology for the analysis of spatial data; see Student (1914), for an early example. It is only since the early 1970's, however, that the statistical analysis of large data sets, using flexible para-},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\TCW3BUQP\\Clifford - 1990 - Markov random fields in statistics.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\I6EA6C2N\\summary.html}
}

@misc{coppersmith2002,
  title = {An Approximate {{Fourier}} Transform Useful in Quantum Factoring},
  author = {Coppersmith, D.},
  year = {2002},
  month = jan,
  number = {arXiv:quant-ph/0201067},
  eprint = {quant-ph/0201067},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.quant-ph/0201067},
  urldate = {2023-06-28},
  abstract = {We define an approximate version of the Fourier transform on \$2\^L\$ elements, which is computationally attractive in a certain setting, and which may find application to the problem of factoring integers with a quantum computer as is currently under investigation by Peter Shor. (1994 IBM Internal Report)},
  archiveprefix = {arxiv},
  keywords = {Quantum Physics},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\I9MLJXWV\\Coppersmith - 2002 - An approximate Fourier transform useful in quantum.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\7VT27BLT\\0201067.html}
}

@article{cortez2009,
  title = {Modeling Wine Preferences by Data Mining from Physicochemical Properties},
  author = {Cortez, Paulo and Cerdeira, Ant{\'o}nio and Almeida, Fernando and Matos, Telmo and Reis, Jos{\'e}},
  year = {2009},
  month = nov,
  journal = {Decision Support Systems},
  series = {Smart {{Business Networks}}: {{Concepts}} and {{Empirical Evidence}}},
  volume = {47},
  number = {4},
  pages = {547--553},
  issn = {0167-9236},
  doi = {10.1016/j.dss.2009.05.016},
  urldate = {2018-11-08},
  abstract = {We propose a data mining approach to predict human wine taste preferences that is based on easily available analytical tests at the certification step. A large dataset (when compared to other studies in this domain) is considered, with white and red vinho verde samples (from Portugal). Three regression techniques were applied, under a computationally efficient procedure that performs simultaneous variable and model selection. The support vector machine achieved promising results, outperforming the multiple regression and neural network methods. Such model is useful to support the oenologist wine tasting evaluations and improve wine production. Furthermore, similar techniques can help in target marketing by modeling consumer tastes from niche markets.},
  keywords = {Model selection,Neural networks,Regression,Sensory preferences,Support vector machines,Variable selection},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\X6ZB6IXC\\S0167923609001377.html}
}

@book{cover2006,
  title = {Elements of {{Information Theory}}},
  author = {Cover, Thomas M. and Thomas, Joy A.},
  year = {2006},
  month = jul,
  edition = {2nd edition},
  publisher = {{Wiley-Interscience}},
  address = {{Hoboken, N.J}},
  isbn = {978-0-471-24195-9},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Cover and Thomas - 2006 - Elements of Information Theory.pdf}
}

@book{cowell1999,
  title = {Probabilistic Networks and Expert Systems},
  author = {Cowell, Robert G. and Dawid, A. Philip and Lauritzen, Steffen L. and Spiegelhalter, David J.},
  year = {1999},
  series = {Statistics for Engineering and Information Science},
  edition = {1st},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  isbn = {0-387-98767-3},
  lccn = {QA76.76.E95 P755 1999},
  keywords = {Expert systems (Computer science),Probabilities},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Cowell - 1999 - Probabilistic networks and expert systems.pdf}
}

@article{csiszar1967,
  title = {Information-Type Measures of Difference of Probability Distributions and Indirect Observation},
  author = {Csiszar, I.},
  year = {1967},
  month = jan,
  journal = {Studia Scientiarum Mathematicarum Hungarica},
  volume = {2},
  pages = {229--318},
  urldate = {2023-03-30},
  abstract = {I. Csiszar},
  langid = {english}
}

@article{csiszar1995,
  title = {Generalized Cutoff Rates and {{Renyi}}'s Information Measures},
  author = {Csiszar, I.},
  year = {1995},
  month = jan,
  journal = {IEEE Transactions on Information Theory},
  volume = {41},
  number = {1},
  pages = {26--34},
  issn = {1557-9654},
  doi = {10.1109/18.370121},
  abstract = {Renyi's (1961) entropy and divergence of order a are given operational characterizations in terms of block coding and hypothesis testing, as so-called /spl beta/-cutoff rates, with /spl alpha/=(1+/spl beta/)/sup -1/ for entropy and /spl alpha/=(1-/spl beta/)/sup -1/ for divergence. Out of several possible definitions of mutual information and channel capacity of order /spl alpha/, our approach distinguishes one that admits an operational characterization as /spl beta/-cutoff rate for channel coding, with /spl alpha/=(1-/spl beta/)/sup -1/. The ordinary cutoff rate of a DMC corresponds to /spl beta/=-1.{$<>$}},
  keywords = {Block codes,Books,Channel capacity,Channel coding,Entropy,Information theory,Mutual information,Q measurement,Terminology,Testing},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Csiszar - 1995 - Generalized cutoff rates and Renyi's information m.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\IXUEL93X\\370121.html}
}

@article{dahlhauser2021,
  title = {Modeling Noisy Quantum Circuits Using Experimental Characterization},
  author = {Dahlhauser, Megan L. and Humble, Travis S.},
  year = {2021},
  month = apr,
  journal = {Physical Review A},
  volume = {103},
  number = {4},
  pages = {042603},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevA.103.042603},
  urldate = {2023-07-09},
  abstract = {Noisy intermediate-scale quantum (NISQ) devices offer unique platforms to test and evaluate the behavior of quantum computing. However, validating circuits on NISQ devices is difficult due to fluctuations in the underlying noise sources and other nonreproducible behaviors that generate computational errors. Here we present a test-driven approach that decomposes a noisy, application-specific circuit into a series of bootstrapped experiments on a NISQ device. By characterizing individual subcircuits, we generate a composite noise model for the original quantum circuit. We demonstrate this approach to model applications of Greenberger-Horne-Zeilinger(GHZ)-state preparation and the Bernstein-Vazirani algorithm on a family of superconducting transmon devices. We measure the model accuracy using the total variation distance between predicted and experimental results, and we demonstrate that the composite model works well across multiple circuit instances. Our approach is shown to be computationally efficient and offers a trade-off in model complexity that can be tailored to the desired predictive accuracy.},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Dahlhauser and Humble - 2021 - Modeling noisy quantum circuits using experimental.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\44WAU5UL\\PhysRevA.103.html}
}

@inproceedings{daskalakis2017,
  title = {Square {{Hellinger Subadditivity}} for {{Bayesian Networks}} and Its {{Applications}} to {{Identity Testing}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Learning Theory}}},
  author = {Daskalakis, Constantinos and Pan, Qinxuan},
  year = {2017},
  month = jun,
  pages = {697--703},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-07-31},
  abstract = {We show that the square Hellinger distance between two Bayesian networks on the same directed graph, GGG, is subadditive with respect to the neighborhoods of GGG. Namely, if PPP and QQQ are the probability distributions defined by two Bayesian networks on the same DAG, our inequality states that the square Hellinger distance, H2(P,Q)H2(P,Q)H\^2(P,Q), between PPP and QQQ is upper bounded by the sum, {$\sum$}vH2(Pv{$\cup\Pi$}v,Qv{$\cup\Pi$}v){$\sum$}vH2(Pv{$\cup\Pi$}v,Qv{$\cup\Pi$}v)\textbackslash sum\_v H\^2(P\_\{v\} {$\cup\backslash$}Pi\_v, Q\_\{v\} {$\cup\backslash$}Pi\_v), of the square Hellinger distances between the marginals of PPP and QQQ on every node vvv and its parents {$\Pi$}v{$\Pi$}v\textbackslash Pi\_v in the DAG. Importantly, our bound does not involve the conditionals but the marginals of PPP and QQQ. We derive a similar inequality for more general Markov Random Fields. As an application of our inequality, we show that distinguishing whether two (unknown) Bayesian networks PPP and QQQ on the same (but potentially unknown) DAG satisfy P=QP=QP=Q vs dTV(P,Q){$>\epsilon$}dTV(P,Q){$>\epsilon$}d\_\textbackslash rm TV(P,Q){$>\epsilon$} can be performed from O\textasciitilde (|{$\Sigma$}|3/4(d+1){$\cdot$}n/{$\epsilon$}2)O\textasciitilde (|{$\Sigma$}|3/4(d+1){$\cdot$}n/{$\epsilon$}2)\textbackslash tilde\{O\}(|{$\Sigma$}|\^3/4(d+1) {$\cdot$}n/{$\epsilon\sphat$}2) samples, where ddd is the maximum in-degree of the DAG and {$\Sigma\Sigma\Sigma$} the domain of each variable of the Bayesian networks. If PPP and QQQ are defined on potentially different and potentially unknown trees, the sample complexity becomes O\textasciitilde (|{$\Sigma$}|4.5n/{$\epsilon$}2)O\textasciitilde (|{$\Sigma$}|4.5n/{$\epsilon$}2)\textbackslash tilde\{O\}(|{$\Sigma$}|\^4.5 n/{$\epsilon\sphat$}2). In both cases the dependence of the sample complexity on n,{$\epsilon$}n,{$\epsilon$}n, {$\epsilon$} is optimal up to logarithmic factors. Lastly, if PPP and QQQ are product distributions over 0,1n0,1n\{0,1\}\^n and QQQ is known, the sample complexity becomes O(n--{$\surd$}/{$\epsilon$}2)O(n/{$\epsilon$}2)O(\textbackslash sqrt\{n\}/{$\epsilon\sphat$}2), which is optimal up to constant factors.},
  langid = {english},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\8RHH9GID\\Daskalakis and Pan - 2017 - Square Hellinger Subadditivity for Bayesian Networ.pdf}
}

@inproceedings{daviddestephenlavaire2015,
  title = {Dimensional Scalability of Supervised and Unsupervised Concept Drift Detection: {{An}} Empirical Study},
  shorttitle = {Dimensional Scalability of Supervised and Unsupervised Concept Drift Detection},
  booktitle = {2015 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {David Destephen Lavaire, Jorge and Singh, Anshuman and Yousef, Mahmoud and Singh, Sumi and Yue, Xiaodong},
  year = {2015},
  month = oct,
  pages = {2212--2218},
  issn = {null},
  doi = {10.1109/BigData.2015.7364009},
  abstract = {Big Data presents challenges for predictive analytic algorithms due to the possibility of non-stationary populations. Concept drift detection algorithms can be used to detect changes in underlying distribution in order to retrain. Most concept drift detection methods are known to scale to a relatively low number of features (a few hundred). However, in many areas, datasets with thousands or even tens of thousands of features are becoming common. This paper studies the behavior of supervised concept drift detection algorithms (Drift Detection Method (DDM), Early Drift Detection Method (EDDM)) and unsupervised algorithm (Friedman and Rafsky's algorithm) on high-dimensional datatsets. Our goal was to find if these algorithms can scale, first by studying the growth of execution time with the dimension of the dataset, and second by studying their comparative accuracy on high-dimensional datasets. The algorithms were run on datasets consisting of up to 100,000 features. Results show a linear growth of the execution time with respect to the dimension in each algorithm. The performance of unsupervised algorithm degraded significantly on datasets close to 100,000 dimensions. Our results also show that the drift detection accuracy of the three algorithms did not degrade as the number of features increase.},
  keywords = {Algorithm design and analysis,big data,Big data,Big Data,concept drift detection algorithms,Detection algorithms,dimensional scalability,early drift detection method,EDDM,high-dimensional datatsets,nonstationary populations,Prediction algorithms,predictive analytic algorithms,Scalability,Sociology,Standards,supervised concept drift detection,unsupervised algorithm,unsupervised concept drift detection,unsupervised learning},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\David Destephen Lavaire et al. - 2015 - Dimensional scalability of supervised and unsuperv.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\YCBKDWU4\\7364009.html}
}

@article{dawid1979,
  title = {Conditional {{Independence}} in {{Statistical Theory}}},
  author = {Dawid, A. Philip},
  year = {1979},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {41},
  number = {1},
  eprint = {2984718},
  eprinttype = {jstor},
  pages = {1--31},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  urldate = {2022-06-27},
  abstract = {Some simple heuristic properties of conditional independence are shown to form a conceptual framework for much of the theory of statistical inference. This framework is illustrated by an examination of the role of conditional independence in several diverse areas of the field of statistics. Topics covered include sufficiency and ancillarity, parameter identification, causal inference, prediction sufficiency, data selection mechanisms, invariant statistical models and a subjectivist approach to model-building.},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Dawid - 1979 - Conditional Independence in Statistical Theory.pdf}
}

@article{dawid1980,
  title = {Conditional {{Independence}} for {{Statistical Operations}}},
  author = {Dawid, A. Philip},
  year = {1980},
  journal = {The Annals of Statistics},
  volume = {8},
  number = {3},
  eprint = {2240595},
  eprinttype = {jstor},
  pages = {598--617},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364},
  urldate = {2022-06-27},
  abstract = {A general calculus of conditional independence is developed, suitable for application to a wide range of statistical concepts such as sufficiency, parameter-identification, adequacy and ancillarity. A vehicle for this theory is the statistical operation, a structure-preserving map between statistical spaces. Concepts such as completeness and identifiability of mixtures arise naturally and play an important part. Some general theorems are exemplified by applications to ancillarity, including a study of a Bayesian definition of ancillarity in the presence of nuisance parameters.},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Dawid - 1980 - Conditional Independence for Statistical Operation.pdf}
}

@article{demsar2018,
  title = {Detecting Concept Drift in Data Streams Using Model Explanation},
  author = {Dem{\v s}ar, Jaka and Bosni{\'c}, Zoran},
  year = {2018},
  month = feb,
  journal = {Expert Systems with Applications},
  volume = {92},
  pages = {546--559},
  issn = {09574174},
  doi = {10.1016/j.eswa.2017.10.003},
  urldate = {2020-04-02},
  langid = {english},
  keywords = {concept drift}
}

@inproceedings{deshpande2001,
  title = {Efficient {{Stepwise Selection}} in {{Decomposable Models}}},
  booktitle = {Proceedings of the {{Seventeenth Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Deshpande, Amol and Garofalakis, Minos and Jordan, Michael I.},
  year = {2001},
  series = {{{UAI}}'01},
  pages = {128--135},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  address = {{San Francisco, CA, USA}},
  urldate = {2019-07-23},
  abstract = {In this paper, we present an efficient algorithm for performing stepwise selection in the class of decomposable models. We focus on the forward selection procedure, but we also discuss how backward selection and the combination of the two can be performed efficiently. The main contributions of this paper are (1) a simple characterization for the edges that can be added to a decomposable model while retaining its decomposability and (2) an efficient algorithm for enumerating all such edges for a given decomposable model in O(n2) time, where n is the number of variables in the model. We also analyze the complexity of the overall stepwise selection procedure (which includes the complexity of enumerating eligible edges as well as the complexity of deciding how to "progress"). We use the KL divergence of the model from the saturated model as our metric, but the results we present here extend to many other metrics as well.},
  isbn = {978-1-55860-800-9},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Deshpande et al. - 2001 - Efficient Stepwise Selection in Decomposable Model.pdf}
}

@misc{dheeru2017,
  title = {{{UCI Machine Learning Repository}}},
  author = {Dheeru, Dua and Karra Taniskidou, Efi},
  year = {2017},
  howpublished = {http://archive.ics.uci.edu/ml}
}

@book{diestel2017,
  title = {Graph Theory},
  author = {Diestel, Reinhard},
  year = {2017},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{New York, NY}},
  isbn = {978-3-662-53621-6},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Diestel - 2017 - Graph theory.pdf}
}

@article{dimitrova2018,
  title = {Graphical {{Models Over Heterogeneous Domains}} and for {{Multilevel Networks}}},
  author = {Dimitrova, Tamara and Kocarev, Ljupco},
  year = {2018},
  journal = {IEEE Access},
  volume = {6},
  pages = {69682--69701},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2880840},
  abstract = {We review models for analyzing multivariate data of mixed (heterogeneous) domains such as binary, categorical, ordinal, counts, continuous, and/or skewed continuous, and methods for modeling various graphs including multiplex, multilevel, and multilayer networks. Data are modeled with Markov random fields which encode Markov property between nodes: two nodes are not connected with an edge if and only if random variables associated with these nodes are conditionally independent, given the other variables. Inferring dependence structure through graphical models (both directed and undirected) is essential for discovering multivariate interaction among high-dimensional data, which could potentially be associated with several diseases. Networks are modeled with exponential random graph models which encode Markov property between edges: two edges are conditionally dependent, given the rest of the network, if they have a common vertex. Studying and understanding multilayer and/or multilevel representations of various phenomena, including social and natural phenomena, could lead to predictive models of these phenomena. Modeling data of heterogeneous domains and multilevel and/or multilayer networks pose challenges which are reviewed. Addressing these challenges within a unified framework stresses open problems and points out new directions for research.},
  keywords = {Analytical models,Biological system modeling,Computational modeling,Data models,exponential random graph models,Graphical models,heterogeneous domains,Markov processes,multilayer networks,multilevel networks,multiplex networks,Nonhomogeneous media},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Dimitrova and Kocarev - 2018 - Graphical Models Over Heterogeneous Domains and fo.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\FS7Z53D9\\8531618.html}
}

@inproceedings{ditzler2011,
  title = {Hellinger Distance Based Drift Detection for Nonstationary Environments},
  booktitle = {2011 {{IEEE Symposium}} on {{Computational Intelligence}} in {{Dynamic}} and {{Uncertain Environments}} ({{CIDUE}})},
  author = {Ditzler, Gregory and Polikar, Robi},
  year = {2011},
  month = apr,
  pages = {41--48},
  issn = {null},
  doi = {10.1109/CIDUE.2011.5948491},
  abstract = {Most machine learning algorithms, including many online learners, assume that the data distribution to be learned is fixed. There are many real-world problems where the distribution of the data changes as a function of time. Changes in nonstationary data distributions can significantly reduce the generalization ability of the learning algorithm on new or field data, if the algorithm is not equipped to track such changes. When the stationary data distribution assumption does not hold, the learner must take appropriate actions to ensure that the new/relevant information is learned. On the other hand, data distributions do not necessarily change continuously, necessitating the ability to monitor the distribution and detect when a significant change in distribution has occurred. In this work, we propose and analyze a feature based drift detection method using the Hellinger distance to detect gradual or abrupt changes in the distribution.},
  keywords = {Algorithm design and analysis,concept drift,Current measurement,data mining,Detection algorithms,drift detection,generalisation (artificial intelligence),generalization ability,Hellinger distance,Histograms,learning (artificial intelligence),machine learning algorithm,Monitoring,nonstationary data distribution,nonstationary environment,nonstationary environments,Training},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Ditzler and Polikar - 2011 - Hellinger distance based drift detection for nonst.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\SM6VGGY8\\5948491.html}
}

@misc{djordjilovic2021,
  title = {Searching for a Source of Difference in {{Gaussian}} Graphical Models},
  author = {Djordjilovi{\'c}, Vera and Chiogna, Monica},
  year = {2021},
  month = nov,
  number = {arXiv:1811.02503},
  eprint = {1811.02503},
  primaryclass = {stat},
  publisher = {{arXiv}},
  urldate = {2022-08-09},
  abstract = {We look at a two-sample problem within the framework of decomposable graphical models. When the global hypothesis of equality of two distributions is rejected, the interest is usually in localizing the source of difference. Motivated by the idea that diseases can be seen as system perturbations, and by the need to distinguish between the origin of perturbation and components affected by the perturbation, we introduce the concept of a minimal seed set, and its graphical counterpart a graphical seed set. They intuitively consist of variables driving the difference between the two conditions. We propose a simple testing procedure, linear in the number of nodes, to estimate the graphical seed set from data. We illustrate our approach in the context of gene set analysis, where we show that is possible to zoom in on the origin of perturbation in a gene network.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Methodology},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\2UMW2IWM\\Djordjilović and Chiogna - 2021 - Searching for a source of difference in Gaussian g.pdf}
}

@inproceedings{domingos2000,
  title = {Mining {{High-speed Data Streams}}},
  booktitle = {Proceedings of the {{Sixth ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Domingos, Pedro and Hulten, Geoff},
  year = {2000},
  series = {{{KDD}} '00},
  pages = {71--80},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/347090.347107},
  urldate = {2018-05-24},
  isbn = {978-1-58113-233-5},
  keywords = {decision trees,disk-based algorithms,Hoeffding bounds,incremental learning,subsampling},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Domingos and Hulten - 2000 - Mining High-speed Data Streams.pdf}
}

@inproceedings{duong2022,
  title = {Bivariate {{Causal Discovery}} via {{Conditional Divergence}}},
  booktitle = {Proceedings of the {{First Conference}} on {{Causal Learning}} and {{Reasoning}}},
  author = {Duong, Bao and Nguyen, Thin},
  year = {2022},
  month = jun,
  pages = {236--252},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-07-31},
  abstract = {Telling apart cause and effect is a fundamental problem across many science disciplines. However, the randomized controlled trial, which is the golden-standard solution for this, is not always physically feasible or ethical. Therefore, we can only rely on passively observational data in such cases, making the problem highly challenging. Inspired by the observation that the conditional distribution of effect given cause, also known as the causal mechanism, is typically invariant in shape, we aim to capture the mechanism through estimating the stability of the conditional distribution. In particular, based on the inverse of stability \textendash{} the divergence \textendash{} we propose Conditional Divergence based Causal Inference (CDCI), a novel algorithm for detecting causal direction in purely observational data. By doing this, we can relax multiple strict assumptions commonly adopted in the causal discovery literature, including functional form and noise model. The proposed approach is generic and applicable to arbitrary measures of distribution divergence. The effectiveness of our method is demonstrated on a variety of both synthetic and real data sets, which compares favorably with existing state-of-the-art methods.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Duong and Nguyen - 2022 - Bivariate Causal Discovery via Conditional Diverge.pdf}
}

@article{eguchi1985,
  title = {A Differential Geometric Approach to Statistical Inference on the Basis of Contrast Functionals},
  author = {Eguchi, Shinto},
  year = {1985},
  journal = {Hiroshima Mathematical Journal},
  volume = {15},
  number = {2},
  pages = {341--391},
  publisher = {{Hiroshima University, Department of Mathematics}},
  issn = {0018-2079},
  doi = {10.32917/hmj/1206130775},
  urldate = {2020-06-30},
  abstract = {Project Euclid - mathematics and statistics online},
  langid = {english},
  mrnumber = {MR805058},
  zmnumber = {0625.62004},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Eguchi - 1985 - A differential geometric approach to statistical i.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\QLN5SD5E\\1206130775.html}
}

@misc{fang2022,
  title = {Is {{Out-of-Distribution Detection Learnable}}?},
  author = {Fang, Zhen and Li, Yixuan and Lu, Jie and Dong, Jiahua and Han, Bo and Liu, Feng},
  year = {2022},
  month = oct,
  number = {arXiv:2210.14707},
  eprint = {2210.14707},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.14707},
  urldate = {2022-10-27},
  abstract = {Supervised learning aims to train a classifier under the assumption that training and test data are from the same distribution. To ease the above assumption, researchers have studied a more realistic setting: out-of-distribution (OOD) detection, where test data may come from classes that are unknown during training (i.e., OOD data). Due to the unavailability and diversity of OOD data, good generalization ability is crucial for effective OOD detection algorithms. To study the generalization of OOD detection, in this paper, we investigate the probably approximately correct (PAC) learning theory of OOD detection, which is proposed by researchers as an open problem. First, we find a necessary condition for the learnability of OOD detection. Then, using this condition, we prove several impossibility theorems for the learnability of OOD detection under some scenarios. Although the impossibility theorems are frustrating, we find that some conditions of these impossibility theorems may not hold in some practical scenarios. Based on this observation, we next give several necessary and sufficient conditions to characterize the learnability of OOD detection in some practical scenarios. Lastly, we also offer theoretical supports for several representative OOD detection works based on our OOD theory.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Fang et al. - 2022 - Is Out-of-Distribution Detection Learnable.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\6A7YZXCQ\\2210.html}
}

@article{fawaz2019,
  title = {Adversarial {{Attacks}} on {{Deep Neural Networks}} for {{Time Series Classification}}},
  author = {Fawaz, H. Ismail and Forestier, G. and Weber, J. and Idoumghar, L. and Muller, P.},
  year = {2019},
  month = mar,
  journal = {arXiv:1903.07054 [cs, stat]},
  eprint = {1903.07054},
  primaryclass = {cs, stat},
  urldate = {2019-03-21},
  abstract = {Time Series Classification (TSC) problems are encountered in many real life data mining tasks ranging from medicine and security to human activity recognition and food safety. With the recent success of deep neural networks in various domains such as computer vision and natural language processing, researchers started adopting these techniques for solving time series data mining problems. However, to the best of our knowledge, no previous work has considered the vulnerability of deep learning models to adversarial time series examples, which could potentially make them unreliable in situations where the decision taken by the classifier is crucial such as in medicine and security. For computer vision problems, such attacks have been shown to be very easy to perform by altering the image and adding an imperceptible amount of noise to trick the network into wrongly classifying the input image. Following this line of work, we propose to leverage existing adversarial attack mechanisms to add a special noise to the input time series in order to decrease the network's confidence when classifying instances at test time. Our results reveal that current state-of-the-art deep learning time series classifiers are vulnerable to adversarial attacks which can have major consequences in multiple domains such as food safety and quality assurance.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Fawaz et al. - 2019 - Adversarial Attacks on Deep Neural Networks for Ti.pdf}
}

@article{fehr2014,
  title = {On the {{Conditional R\'enyi Entropy}}},
  author = {Fehr, Serge and Berens, Stefan},
  year = {2014},
  month = nov,
  journal = {IEEE Transactions on Information Theory},
  volume = {60},
  number = {11},
  pages = {6801--6810},
  issn = {1557-9654},
  doi = {10.1109/TIT.2014.2357799},
  abstract = {The R\'enyi entropy of general order unifies the well-known Shannon entropy with several other entropy notions, like the min-entropy or collision entropy. In contrast to the Shannon entropy, there seems to be no commonly accepted definition for the conditional R\'enyi entropy: several versions have been proposed and used in the literature. In this paper, we reconsider the definition for the conditional R\'enyi entropy of general order as proposed by Arimoto in the seventies. We show that this particular notion satisfies several natural properties. In particular, we show that it satisfies monotonicity under conditioning, meaning that conditioning can only reduce the entropy, and (a weak form of) chain rule, which implies that the decrease in entropy due to conditioning is bounded by the number of bits one conditions on. None of the other suggestions for the conditional R\'enyi entropy satisfies both these properties. Finally, we show a natural interpretation of the conditional R\'enyi entropy in terms of (unconditional) R\'enyi divergence, and we show consistency with a recently proposed notion of conditional R\'enyi entropy in the quantum setting.},
  keywords = {Biomedical measurement,Conditial R\'enyi entropy,Entropy,Information theory,Joints,Measurement uncertainty,monotonicity and chain rule,quantum R\'enyi entropy,Random variables,R\'enyi divergence,Uncertainty},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\2IYR25IP\\Fehr and Berens - 2014 - On the Conditional Rényi Entropy.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\R7LWY368\\6898022.html}
}

@inproceedings{feremans2020,
  title = {Pattern-{{Based Anomaly Detection}} in {{Mixed-Type Time Series}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Feremans, Len and Vercruyssen, Vincent and Cule, Boris and Meert, Wannes and Goethals, Bart},
  editor = {Brefeld, Ulf and Fromont, Elisa and Hotho, Andreas and Knobbe, Arno and Maathuis, Marloes and Robardet, C{\'e}line},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {240--256},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-46150-8\_15},
  abstract = {The present-day accessibility of technology enables easy logging of both sensor values and event logs over extended periods. In this context, detecting abnormal segments in time series data has become an important data mining task. Existing work on anomaly detection focuses either on continuous time series or discrete event logs and not on the combination. However, in many practical applications, the patterns extracted from the event log can reveal contextual and operational conditions of a device that must be taken into account when predicting anomalies in the continuous time series. This paper proposes an anomaly detection method that can handle mixed-type time series. The method leverages frequent pattern mining techniques to construct an embedding of mixed-type time series on which an isolation forest is trained. Experiments on several real-world univariate and multivariate time series, as well as a synthetic mixed-type time series, show that our anomaly detection algorithm outperforms state-of-the-art anomaly detection techniques such as MatrixProfile, Pav, Mifpod and Fpof.},
  isbn = {978-3-030-46150-8},
  langid = {english},
  keywords = {Anomaly detection,Distance measure,Frequent pattern mining,Pattern-based embedding,Time series},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Feremans et al. - 2020 - Pattern-Based Anomaly Detection in Mixed-Type Time.pdf}
}

@incollection{fernandes2015,
  title = {A {{Proactive Intelligent Decision Support System}} for {{Predicting}} the {{Popularity}} of {{Online News}}},
  booktitle = {Progress in {{Artificial Intelligence}}},
  author = {Fernandes, Kelwin and Vinagre, Pedro and Cortez, Paulo},
  editor = {Pereira, Francisco and Machado, Penousal and Costa, Ernesto and Cardoso, Am{\'i}lcar},
  year = {2015},
  volume = {9273},
  pages = {535--546},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-23485-4\_53},
  urldate = {2018-11-08},
  abstract = {Due to the Web expansion, the prediction of online news popularity is becoming a trendy research topic. In this paper, we propose a novel and proactive Intelligent Decision Support System (IDSS) that analyzes articles prior to their publication. Using a broad set of extracted features (e.g., keywords, digital media content, earlier popularity of news referenced in the article) the IDSS first predicts if an article will become popular. Then, it optimizes a subset of the articles features that can more easily be changed by authors, searching for an enhancement of the predicted popularity probability. Using a large and recently collected dataset, with 39,000 articles from the Mashable website, we performed a robust rolling windows evaluation of five state of the art models. The best result was provided by a Random Forest with a discrimination power of 73\%. Moreover, several stochastic hill climbing local searches were explored. When optimizing 1000 articles, the best optimization method obtained a mean gain improvement of 15 percentage points in terms of the estimated popularity probability. These results attest the proposed IDSS as a valuable tool for online news authors.},
  isbn = {978-3-319-23484-7 978-3-319-23485-4},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Fernandes et al. - 2015 - A Proactive Intelligent Decision Support System fo.pdf}
}

@article{fevotte2009,
  title = {Nonnegative {{Matrix Factorization}} with the {{Itakura-Saito Divergence}}: {{With Application}} to {{Music Analysis}}},
  shorttitle = {Nonnegative {{Matrix Factorization}} with the {{Itakura-Saito Divergence}}},
  author = {F{\'e}votte, C{\'e}dric and Bertin, Nancy and Durrieu, Jean-Louis},
  year = {2009},
  month = mar,
  journal = {Neural Computation},
  volume = {21},
  number = {3},
  pages = {793--830},
  issn = {0899-7667},
  doi = {10.1162/neco.2008.04-08-771},
  abstract = {This letter presents theoretical, algorithmic, and experimental results about nonnegative matrix factorization (NMF) with the Itakura-Saito (IS) divergence. We describe how IS-NMF is underlaid by a well-defined statistical model of superimposed gaussian components and is equivalent to maximum likelihood estimation of variance parameters. This setting can accommodate regularization constraints on the factors through Bayesian priors. In particular, inverse-gamma and gamma Markov chain priors are considered in this work. Estimation can be carried out using a space-alternating generalized expectation-maximization (SAGE) algorithm; this leads to a novel type of NMF algorithm, whose convergence to a stationary point of the IS cost function is guaranteed. We also discuss the links between the IS divergence and other cost functions used in NMF, in particular, the Euclidean distance and the generalized Kullback-Leibler (KL) divergence. As such, we describe how IS-NMF can also be performed using a gradient multiplicative algorithm (a standard algorithm structure in NMF) whose convergence is observed in practice, though not proven. Finally, we report a furnished experimental comparative study of Euclidean-NMF, KL-NMF, and IS-NMF algorithms applied to the power spectrogram of a short piano sequence recorded in real conditions, with various initializations and model orders. Then we show how IS-NMF can successfully be employed for denoising and upmix (mono to stereo conversion) of an original piece of early jazz music. These experiments indicate that IS-NMF correctly captures the semantics of audio and is better suited to the representation of music signals than NMF with the usual Euclidean and KL costs.},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Févotte et al. - 2009 - Nonnegative Matrix Factorization with the Itakura-.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\9N4FRCHP\\6797100.html}
}

@inproceedings{fomin2004,
  title = {Exact ({{Exponential}}) {{Algorithms}} for {{Treewidth}} and {{Minimum Fill-In}}},
  booktitle = {Automata, {{Languages}} and {{Programming}}},
  author = {Fomin, Fedor V. and Kratsch, Dieter and Todinca, Ioan},
  editor = {D{\'i}az, Josep and Karhum{\"a}ki, Juhani and Lepist{\"o}, Arto and Sannella, Donald},
  year = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {568--580},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-27836-8\_49},
  abstract = {We show that for a graph G on n vertices its treewidth and minimum fill-in can be computed roughly in 1.9601 n time. Our result is based on a combinatorial proof that the number of minimal separators in a graph is O(n{$\cdot$}1.7087n)O(n{$\cdot$}1.7087n)\textbackslash mathcal O(n \textbackslash cdot 1.7087\^n) and that the number of potential maximal cliques s is O(n4{$\cdot$}1.9601n)O(n4{$\cdot$}1.9601n)\textbackslash mathcal O(n\^4 \textbackslash cdot 1.9601\^n).},
  isbn = {978-3-540-27836-8},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Fomin et al. - 2004 - Exact (Exponential) Algorithms for Treewidth and M.pdf}
}

@article{friedman1997,
  title = {Bayesian {{Network Classifiers}}},
  author = {Friedman, Nir and Geiger, Dan and Goldszmidt, Moises},
  year = {1997},
  month = nov,
  journal = {Machine Learning},
  volume = {29},
  number = {2},
  pages = {131--163},
  issn = {1573-0565},
  doi = {10.1023/A:1007465528199},
  urldate = {2019-10-23},
  abstract = {Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features, called naive Bayes, is competitive with state-of-the-art classifiers such as C4.5. This fact raises the question of whether a classifier with less restrictive assumptions can perform even better. In this paper we evaluate approaches for inducing classifiers from data, based on the theory of learning Bayesian networks. These networks are factored representations of probability distributions that generalize the naive Bayesian classifier and explicitly represent statements about independence. Among these approaches we single out a method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness that characterize naive Bayes. We experimentally tested these approaches, using problems from the University of California at Irvine repository, and compared them to C4.5, naive Bayes, and wrapper methods for feature selection.},
  langid = {english},
  keywords = {Bayesian networks,classification},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Friedman et al. - 1997 - Bayesian Network Classifiers.pdf}
}

@inproceedings{gama2004,
  title = {Learning with {{Drift Detection}}},
  booktitle = {Advances in {{Artificial Intelligence}} \textendash{} {{SBIA}} 2004},
  author = {Gama, Jo{\~a}o and Medas, Pedro and Castillo, Gladys and Rodrigues, Pedro},
  year = {2004},
  month = sep,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {286--295},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-28645-5\textbackslash\_29},
  urldate = {2018-05-23},
  abstract = {Most of the work in machine learning assume that examples are generated at random according to some stationary probability distribution. In this work we study the problem of learning when the distribution that generate the examples changes over time. We present a method for detection of changes in the probability distribution of examples. The idea behind the drift detection method is to control the online error-rate of the algorithm. The training examples are presented in sequence. When a new training example is available, it is classified using the actual model. Statistical theory guarantees that while the distribution is stationary, the error will decrease. When the distribution changes, the error will increase. The method controls the trace of the online error of the algorithm. For the actual context we define a warning level, and a drift level. A new context is declared, if in a sequence of examples, the error increases reaching the warning level at example k w , and the drift level at example k d . This is an indication of a change in the distribution of the examples. The algorithm learns a new model using only the examples since k w . The method was tested with a set of eight artificial datasets and a real world dataset. We used three learning algorithms: a perceptron, a neural network and a decision tree. The experimental results show a good performance detecting drift and with learning the new concept. We also observe that the method is independent of the learning algorithm.},
  isbn = {978-3-540-23237-7},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Gama et al. - 2004 - Learning with Drift Detection.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\S2A5YWX9\\978-3-540-28645-5_29.html}
}

@inproceedings{gama2011,
  title = {Learning {{About}} the {{Learning Process}}},
  booktitle = {Proceedings of the 10th {{International Conference}} on {{Advances}} in {{Intelligent Data Analysis X}}},
  author = {Gama, Jo{\~a}o and Kosina, Petr},
  year = {2011},
  series = {{{IDA}}'11},
  pages = {162--172},
  publisher = {{Springer-Verlag}},
  address = {{Berlin, Heidelberg}},
  urldate = {2018-05-29},
  abstract = {This work addresses the problem of mining data stream generated in dynamic environments where the distribution underlying the observations may change over time. We present a system that monitors the evolution of the learning process. The system is able to self-diagnosis degradations of this process, using change detection mechanisms, and self-repairs the decision models. The system uses meta-learning techniques that characterize the domain of applicability of previously learned models. The meta-learns can detect re-occurrence of contexts, using unlabeled examples, and take pro-active actions by activating previously learned models.},
  isbn = {978-3-642-24799-6},
  keywords = {concept drift,Data streams,meta-learning,recurrent concepts}
}

@article{gama2013,
  title = {On Evaluating Stream Learning Algorithms},
  author = {Gama, Jo{\~a}o and Sebasti{\~a}o, Raquel and Rodrigues, Pedro Pereira},
  year = {2013},
  month = mar,
  journal = {Machine Learning},
  volume = {90},
  number = {3},
  pages = {317--346},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-012-5320-9},
  urldate = {2018-04-11},
  abstract = {Most streaming decision models evolve continuously over time, run in resource-aware environments, and detect and react to changes in the environment generating data. One important issue, not yet convincingly addressed, is the design of experimental work to evaluate and compare decision models that evolve over time. This paper proposes a general framework for assessing predictive stream learning algorithms. We defend the use of prequential error with forgetting mechanisms to provide reliable error estimators. We prove that, in stationary data and for consistent learning algorithms, the holdout estimator, the prequential error and the prequential error estimated over a sliding window or using fading factors, all converge to the Bayes error. The use of prequential error with forgetting mechanisms reveals to be advantageous in assessing performance and in comparing stream learning algorithms. It is also worthwhile to use the proposed methods for hypothesis testing and for change detection. In a set of experiments in drift scenarios, we evaluate the ability of a standard change detection algorithm to detect change using three prequential error estimators. These experiments point out that the use of forgetting mechanisms (sliding windows or fading factors) are required for fast and efficient change detection. In comparison to sliding windows, fading factors are faster and memoryless, both important requirements for streaming applications. Overall, this paper is a contribution to a discussion on best practice for performance assessment when learning is a continuous process, and the decision models are dynamic and evolve over time.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Gama et al. - 2013 - On evaluating stream learning algorithms.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\RFRUW7EL\\s10994-012-5320-9.html}
}

@article{gama2014,
  title = {A {{Survey}} on {{Concept Drift Adaptation}}},
  author = {Gama, Jo{\~a}o and {\v Z}liobaite, Indre and Bifet, Albert and Pechenizkiy, Mykola and Bouchachia, Abdelhamid},
  year = {2014},
  month = mar,
  journal = {ACM Comput. Surv.},
  volume = {46},
  number = {4},
  pages = {44:1--44:37},
  issn = {0360-0300},
  doi = {10.1145/2523813},
  urldate = {2018-03-17},
  abstract = {Concept drift primarily refers to an online supervised learning scenario when the relation between the input data and the target variable changes over time. Assuming a general knowledge of supervised learning in this article, we characterize adaptive learning processes; categorize existing strategies for handling concept drift; overview the most representative, distinct, and popular techniques and algorithms; discuss evaluation methodology of adaptive algorithms; and present a set of illustrative applications. The survey covers the different facets of concept drift in an integrated way to reflect on the existing scattered state of the art. Thus, it aims at providing a comprehensive introduction to the concept drift adaptation for researchers, industry analysts, and practitioners.},
  keywords = {adaptive learning,change detection,Concept drift,data streams},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Gama et al. - 2014 - A Survey on Concept Drift Adaptation.pdf}
}

@incollection{gao2007,
  title = {A {{General Framework}} for {{Mining Concept-Drifting Data Streams}} with {{Skewed Distributions}}},
  booktitle = {Proceedings of the 2007 {{SIAM International Conference}} on {{Data Mining}}},
  author = {Gao, J. and Fan, W. and Han, J. and Yu, P.},
  year = {2007},
  month = apr,
  series = {Proceedings},
  pages = {3--14},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611972771.1},
  urldate = {2018-05-23},
  abstract = {In recent years, there have been some interesting studies on predictive modeling in data streams. However, most such studies assume relatively balanced and stable data streams but cannot handle well rather skewed (e.g., few positives but lots of negatives) and stochastic distributions, which are typical in many data stream applications. In this paper, we propose a new approach to mine data streams by estimating reliable posterior probabilities using an ensemble of models to match the distribution over under-samples of negatives and repeated samples of positives. We formally show some interesting and important properties of the proposed framework, e.g., reliability of estimated probabilities on skewed positive class, accuracy of estimated probabilities, efficiency and scalability. Experiments are performed on several synthetic as well as real-world datasets with skewed distributions, and they demonstrate that our framework has substantial advantages over existing approaches in estimation reliability and predication accuracy.},
  isbn = {978-0-89871-630-6},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Gao et al. - 2007 - A General Framework for Mining Concept-Drifting Da.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\NDC7NJNF\\1.9781611972771.html}
}

@article{gavril1974,
  title = {The Intersection Graphs of Subtrees in Trees Are Exactly the Chordal Graphs},
  author = {Gavril, F{\v a}nic{\v a}},
  year = {1974},
  month = feb,
  journal = {Journal of Combinatorial Theory, Series B},
  volume = {16},
  number = {1},
  pages = {47--56},
  issn = {0095-8956},
  doi = {10.1016/0095-8956(74)90094-X},
  urldate = {2022-02-14},
  abstract = {The intersection graph of a family of subtrees in an undirected tree is called a subtree graph. A graph is called chordal if every simple circuit with more than three vertices has an edge connecting two non-consecutive vertices. In this paper, we prove that, for a graph G, the following conditions are equivalent: (i) G is a chordal graph; (ii) G is a subtree graph; (iii) G is a proper subtree graph. Consider a chordal graph G. We give an efficient algorithm for constructing a representation of G by a family of subtrees in a tree.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Gavril - 1974 - The intersection graphs of subtrees in trees are e.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\YL88ZHHQ\\009589567490094X.html}
}

@inproceedings{geiger1989,
  title = {D-{{Separation}}: {{From Theorems}} to {{Algorithms}}},
  shorttitle = {D-{{Separation}}},
  booktitle = {{{UAI}} '89: {{Proceedings}} of the {{Fifth Annual Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}, {{Windsor}}, {{Ontario}}, {{Canada}}, {{August}} 18-20, 1989},
  author = {Geiger, Dan and Verma, Thomas and Pearl, Judea},
  editor = {Henrion, Max and Shachter, Ross D. and Kanal, Laveen N. and Lemmer, John F.},
  year = {1989},
  pages = {139--148},
  publisher = {{North-Holland}},
  urldate = {2022-06-22},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Geiger et al. - 1989 - d-Separation From Theorems to Algorithms.pdf}
}

@article{geiger1990,
  title = {Identifying Independence in Bayesian Networks},
  author = {Geiger, Dan and Verma, Thomas and Pearl, Judea},
  year = {1990},
  journal = {Networks},
  volume = {20},
  number = {5},
  pages = {507--534},
  issn = {1097-0037},
  doi = {10.1002/net.3230200504},
  urldate = {2022-06-22},
  abstract = {An important feature of Bayesian networks is that they facilitate explicit encoding of information about independencies in the domain, information that is indispensable for efficient inferencing. This article characterizes all independence assertions that logically follow from the topology of a network and develops a linear time algorithm that identifies these assertions. The algorithm's correctness is based on the soundness of a graphical criterion, called d-separation, and its optimality stems from the completeness of d-separation. An enhanced version of d-separation, called D-separation, is defined, extending the algorithm to networks that encode functional dependencies. Finally, the algorithm is shown to work for a broad class of nonprobabilistic independencies.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Geiger et al. - 1990 - Identifying independence in bayesian networks.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\SAY4H45I\\net.html}
}

@article{georgopoulos2021,
  title = {Modeling and Simulating the Noisy Behavior of Near-Term Quantum Computers},
  author = {Georgopoulos, Konstantinos and Emary, Clive and Zuliani, Paolo},
  year = {2021},
  month = dec,
  journal = {Physical Review A},
  volume = {104},
  number = {6},
  pages = {062432},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevA.104.062432},
  urldate = {2023-07-09},
  abstract = {Noise dominates every aspect of near-term quantum computers, rendering it exceedingly difficult to carry out even small computations. In this paper we are concerned with the modeling of noise in noisy intermediate-scale quantum computers. We focus on three error groups that represent the main sources of noise during a computation and present quantum channels that model each source. We engineer a noise model that combines all three noise channels and simulates the evolution of the quantum computer using its calibrated error rates. We run various experiments of our model, showcasing its behavior compared to other noise models and an IBM quantum computer. We find that our model provides a better approximation of the quantum computer's behavior than the other models. Following this, we use a genetic algorithm to optimize the parameters used by our noise model, bringing the behavior of the model even closer to the quantum computer. Finally, a comparison between the pre- and postoptimization parameters reveals that, according to our model, certain operations can be more or less erroneous than the hardware-calibrated parameters show.},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Georgopoulos et al. - 2021 - Modeling and simulating the noisy behavior of near.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\ECGS2MLC\\PhysRevA.104.html}
}

@inproceedings{ghifary2015,
  title = {Domain {{Generalization}} for {{Object Recognition}} with {{Multi-task Autoencoders}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Ghifary, Muhammad and Kleijn, W. Bastiaan and Zhang, Mengjie and Balduzzi, David},
  year = {2015},
  month = dec,
  pages = {2551--2559},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2015.293},
  abstract = {The problem of domain generalization is to take knowledge acquired from a number of related domains, where training data is available, and to then successfully apply it to previously unseen domains. We propose a new feature learning algorithm, Multi-Task Autoencoder (MTAE), that provides good generalization performance for cross-domain object recognition. The algorithm extends the standard denoising autoencoder framework by substituting artificially induced corruption with naturally occurring inter-domain variability in the appearance of objects. Instead of reconstructing images from noisy versions, MTAE learns to transform the original image into analogs in multiple related domains. It thereby learns features that are robust to variations across domains. The learnt features are then used as inputs to a classifier. We evaluated the performance of the algorithm on benchmark image recognition datasets, where the task is to learn features from multiple datasets and to then predict the image label from unseen datasets. We found that (denoising) MTAE outperforms alternative autoencoder-based models as well as the current state-of-the-art algorithms for domain generalization.},
  keywords = {Feature extraction,Image reconstruction,Noise reduction,Object recognition,Robustness,Standards,Training},
  file = {D\:\\work\\literature\\library\\Ghifary et al. - 2015 - Domain Generalization for Object Recognition with .pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\GQNYA25F\\7410650.html}
}

@article{ghomeshi2019,
  ids = {ghomeshi},
  title = {{{EACD}}: Evolutionary Adaptation to Concept Drifts in Data Streams},
  shorttitle = {{{EACD}}},
  author = {Ghomeshi, Hossein and Gaber, Mohamed Medhat and Kovalchuk, Yevgeniya},
  year = {2019},
  month = may,
  journal = {Data Mining and Knowledge Discovery},
  volume = {33},
  number = {3},
  pages = {663--694},
  issn = {1573-756X},
  doi = {10.1007/s10618-019-00614-6},
  urldate = {2021-08-07},
  abstract = {This paper presents a novel ensemble learning method based on evolutionary algorithms to cope with different types of concept drifts in non-stationary data stream classification tasks. In ensemble learning, multiple learners forming an ensemble are trained to obtain a better predictive performance compared to that of a single learner, especially in non-stationary environments, where data evolve over time. The evolution of data streams can be viewed as a problem of changing environment, and evolutionary algorithms offer a natural solution to this problem. The method proposed in this paper uses random subspaces of features from a pool of features to create different classification types in the ensemble. Each such type consists of a limited number of classifiers (decision trees) that have been built at different times over the data stream. An evolutionary algorithm (replicator dynamics) is used to adapt to different concept drifts; it allows the types with a higher performance to increase and those with a lower performance to decrease in size. Genetic algorithm is then applied to build a two-layer architecture based on the proposed technique to dynamically optimise the combination of features in each type to achieve a better adaptation to new concepts. The proposed method, called EACD, offers both implicit and explicit mechanisms to deal with concept drifts. A set of experiments employing four artificial and five real-world data streams is conducted to compare its performance with that of the state-of-the-art algorithms using the immediate and delayed prequential evaluation methods. The results demonstrate favourable performance of the proposed EACD method in different environments.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Ghomeshi et al. - 2019 - EACD evolutionary adaptation to concept drifts in.pdf}
}

@article{gillis2014,
  title = {The {{Why}} and {{How}} of {{Nonnegative Matrix Factorization}}},
  author = {Gillis, Nicolas},
  year = {2014},
  month = jan,
  journal = {arXiv:1401.5226 [cs, math, stat]},
  eprint = {1401.5226},
  primaryclass = {cs, math, stat},
  urldate = {2019-02-26},
  abstract = {Nonnegative matrix factorization (NMF) has become a widely used tool for the analysis of high-dimensional data as it automatically extracts sparse and meaningful features from a set of nonnegative data vectors. We first illustrate this property of NMF on three applications, in image processing, text mining and hyperspectral imaging \textendash this is the why. Then we address the problem of solving NMF, which is NP-hard in general. We review some standard NMF algorithms, and also present a recent subclass of NMF problems, referred to as near-separable NMF, that can be solved efficiently (that is, in polynomial time), even in the presence of noise \textendash this is the how. Finally, we briefly describe some problems in mathematics and computer science closely related to NMF via the nonnegative rank.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Gillis - 2014 - The Why and How of Nonnegative Matrix Factorizatio.pdf}
}

@article{goldenberg2019,
  title = {Survey of Distance Measures for Quantifying Concept Drift and Shift in Numeric Data},
  author = {Goldenberg, Igor and Webb, Geoffrey I.},
  year = {2019},
  month = aug,
  journal = {Knowledge and Information Systems},
  volume = {60},
  number = {2},
  pages = {591--615},
  issn = {0219-3116},
  doi = {10.1007/s10115-018-1257-z},
  urldate = {2022-07-31},
  abstract = {Deployed machine learning systems are necessarily learned from historical data and are often applied to current data. When the world changes, the learned models can lose fidelity. Such changes to the statistical properties of data over time are known as concept drift. Similarly, models are often learned in one context, but need to be applied in another. This is called concept shift. Quantifying the magnitude of drift or shift, especially in the context of covariate drift or shift, or unsupervised learning, requires use of measures of distance between distributions. In this paper, we survey such distance measures with respect to their suitability for estimating drift and shift magnitude between samples of numeric data.},
  langid = {english},
  keywords = {Hellinger distance,Hotelling distance,Kullback\textendash Leibler divergence,Mahalanobis distance,Multivariate concept drift},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\Z34ZT59K\\Goldenberg and Webb - 2019 - Survey of distance measures for quantifying concep.pdf}
}

@article{goldenberg2020,
  title = {{{PCA-based}} Drift and Shift Quantification Framework for Multidimensional Data},
  author = {Goldenberg, Igor and Webb, Geoffrey I.},
  year = {2020},
  month = jul,
  journal = {Knowledge and Information Systems},
  volume = {62},
  number = {7},
  pages = {2835--2854},
  issn = {0219-3116},
  doi = {10.1007/s10115-020-01438-3},
  urldate = {2022-11-01},
  abstract = {Concept drift is a serious problem confronting machine learning systems in a dynamic and ever-changing world. In order to manage concept drift it may be useful to first quantify it by measuring the distance between distributions that generate data before and after a drift. There is a paucity of methods to do so in the case of multidimensional numeric data. This paper provides an in-depth analysis of the PCA-based change detection approach, identifies shortcomings of existing methods and shows how this approach can be used to measure a drift, not merely detect it.},
  langid = {english},
  keywords = {Drift detection,Hellinger distance,Principal component analysis},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Goldenberg and Webb - 2020 - PCA-based drift and shift quantification framework.pdf}
}

@article{gomes2017,
  title = {Adaptive Random Forests for Evolving Data Stream Classification},
  author = {Gomes, Heitor M. and Bifet, Albert and Read, Jesse and Barddal, Jean Paul and Enembreck, Fabr{\'i}cio and Pfharinger, Bernhard and Holmes, Geoff and Abdessalem, Talel},
  year = {2017},
  month = oct,
  journal = {Machine Learning},
  volume = {106},
  number = {9-10},
  pages = {1469--1495},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-017-5642-8},
  urldate = {2020-04-02},
  langid = {english},
  keywords = {concept drift},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Gomes et al. - 2017 - Adaptive random forests for evolving data stream c.pdf}
}

@inproceedings{gomes2019,
  title = {Streaming {{Random Patches}} for {{Evolving Data Stream Classification}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  author = {Gomes, Heitor Murilo and Read, Jesse and Bifet, Albert},
  year = {2019},
  month = nov,
  pages = {240--249},
  issn = {1550-4786},
  doi = {10.1109/ICDM.2019.00034},
  abstract = {Ensemble methods are a popular choice for learning from evolving data streams. This popularity is due to (i) the ability to simulate simple, yet, successful ensemble learning strategies, such as bagging and random forests; (ii) the possibility of incorporating drift detection and recovery in conjunction to the ensemble algorithm; (iii) the availability of efficient incremental base learners, such as Hoeffding Trees. In this work, we introduce the Streaming Random Patches (SRP) algorithm, an ensemble method specially adapted to stream classification which combines random subspaces and online bagging. We provide theoretical insights and empirical results illustrating different aspects of SRP. In particular, we explain how the widely adopted incremental Hoeffding trees are not, in fact, unstable learners, unlike their batch counterparts, and how this fact significantly influences ensemble methods design and performance. We compare SRP against state-of-the-art ensemble variants for streaming data in a multitude of datasets. The results show how SRP produce a high predictive performance for both real and synthetic datasets. Besides, we analyze the diversity over time and the average tree depth, which provides insights on the differences between local subspace randomization (as in random forest) and global subspace randomization (as in random subspaces).},
  keywords = {{Stream Data Mining, Ensemble Learning, Random Subspaces, Random Patches}},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Gomes et al. - 2019 - Streaming Random Patches for Evolving Data Stream .pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\QARZWQS7\\8970784.html}
}

@article{goncalvesjr2013,
  ids = {goncalvesjrRCDRecurringConcept2013},
  title = {{{RCD}}: {{A}} Recurring Concept Drift Framework},
  shorttitle = {{{RCD}}},
  author = {Gon{\c c}alves Jr, Paulo Mauricio and de Barros, Roberto Souto Maior},
  year = {2013},
  month = jul,
  journal = {Pattern Recognition Letters},
  volume = {34},
  number = {9},
  pages = {1018--1025},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2013.02.005},
  urldate = {2020-02-16},
  abstract = {This paper presents recurring concept drifts (RCD), a framework that offers an alternative approach to handle data streams that suffer from recurring concept drifts (on-line learning). It creates a new classifier to each context found and stores a sample of data used to build it. When a new concept drift occurs, the algorithm compares the new context to previous ones using a non-parametric multivariate statistical test to verify if both contexts come from the same distribution. If so, the corresponding classifier is reused. The RCD framework is compared with several algorithms (among single and ensemble approaches), in both artificial and real data sets, chosen from frequently used algorithms and data sets in the concept drift research area. We claim the proposed framework had better average ranks in data sets with abrupt and gradual concept drifts compared to both the single classifiers and the ensemble approaches that use the same base learner.},
  langid = {english},
  keywords = {Concept drift,Data streams,Multivariate non-parametric statistical test,On-line learning,Recurring contexts},
  file = {D\:\\work\\literature\\library\\Gonçalves Jr and Barros - 2013 - RCD A recurring concept drift framework.pdf;D\:\\work\\literature\\library\\Gonçalves Jr and Barros - 2013 - RCD A recurring concept drift framework2.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\6DA63R35\\S0167865513000494.html;C\:\\Users\\Loong Kuan\\Zotero\\storage\\X7K5BI3M\\S0167865513000494.html}
}

@article{gonzalez-castro2013,
  title = {Class Distribution Estimation Based on the {{Hellinger}} Distance},
  author = {{Gonz{\'a}lez-Castro}, V{\'i}ctor and {Alaiz-Rodr{\'i}guez}, Roc{\'i}o and Alegre, Enrique},
  year = {2013},
  month = jan,
  journal = {Information Sciences},
  volume = {218},
  pages = {146--164},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2012.05.028},
  urldate = {2022-08-02},
  abstract = {Class distribution estimation (quantification) plays an important role in many practical classification problems. Firstly, it is important in order to adapt the classifier to the operational conditions when they differ from those assumed in learning. Additionally, there are some real domains where the quantification task is itself valuable due to the high variability of the class prior probabilities. Our novel quantification approach for two-class problems is based on distributional divergence measures. The mismatch between the test data distribution and validation distributions generated in a fully controlled way is measured by the Hellinger distance in order to estimate the prior probability that minimizes this divergence. Experimental results on several binary classification problems show the benefits of this approach when compared to such approaches as counting the predicted class labels and other methods based on the classifier confusion matrix or on posterior probability estimations. We also illustrate these techniques as well as their robustness against the base classifier performance (a neural network) with a boar semen quality control setting. Empirical results show that the quantification can be conducted with a mean absolute error lower than 0.008, which seems very promising in this field.},
  langid = {english},
  keywords = {Class distribution shift,Class prior probability estimation,Hellinger distance,Quantification},
  file = {D\:\\work\\literature\\library\\González-Castro et al. - 2013 - Class distribution estimation based on the Helling.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\MN6J5QDQ\\S0020025512004069.html}
}

@inproceedings{grossman2004,
  title = {Learning {{Bayesian Network Classifiers}} by {{Maximizing Conditional Likelihood}}},
  booktitle = {Proceedings of the {{Twenty-first International Conference}} on {{Machine Learning}}},
  author = {Grossman, Daniel and Domingos, Pedro and Domingos, Pedro},
  year = {2004},
  series = {{{ICML}} '04},
  pages = {46--},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1015330.1015339},
  urldate = {2019-10-23},
  abstract = {Bayesian networks are a powerful probabilistic representation, and their use for classification has received considerable attention. However, they tend to perform poorly when learned in the standard way. This is attributable to a mismatch between the objective function used (likelihood or a function thereof) and the goal of classification (maximizing accuracy or conditional likelihood). Unfortunately, the computational cost of optimizing structure and parameters for conditional likelihood is prohibitive. In this paper we show that a simple approximation---choosing structures by maximizing conditional likelihood while setting parameters by maximum likelihood---yields good results. On a large suite of benchmark datasets, this approach produces better class probability estimates than naive Bayes, TAN, and generatively-trained Bayesian networks.},
  isbn = {978-1-58113-838-2},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Grossman et al. - 2004 - Learning Bayesian Network Classifiers by Maximizin.pdf}
}

@misc{grother1995,
  title = {{{NIST Special Database}} 19: {{Handprinted}} Forms and Characters Database},
  author = {Grother, Patrick J and Hanaoka, Kayee K},
  year = {1995}
}

@book{haberman1977,
  title = {The Analysis of Frequency Data},
  author = {Haberman, Shelby J.},
  year = {1977},
  publisher = {{University of Chicago Press}},
  abstract = {Basic properties of log-linear models; Maximum-likelihood estimation; Numerical evaluation of maximum-likelihood estimates; Asymptotic properties; Complete factorial tables; Social-mobility tables; Incomplete contingency tables; Quantal response models; Some extensions.},
  googlebooks = {TQvvAAAAMAAJ},
  isbn = {978-0-226-31185-2},
  langid = {english},
  keywords = {Estimation theory,Mathematics,Mathematics / Probability \& Statistics / General,Statistical hypothesis testing}
}

@article{hammersley1971,
  title = {Markov Fields on Finite Graphs and Lattices},
  author = {Hammersley, John Michael and Clifford, Peter},
  year = {1971},
  journal = {Unpublished manuscript},
  urldate = {2022-07-05},
  abstract = {Semantic Scholar extracted view of \&quot;Markov fields on finite graphs and lattices\&quot; by J. Hammersley et al.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Hammersley and Clifford - 1971 - Markov fields on finite graphs and lattices.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\BAFVSNJP\\ec75e3ca906681bd900218a348a4a35dfed3d6fd.html}
}

@inproceedings{han2016,
  title = {Minimax Rate-Optimal Estimation of {{KL}} Divergence between Discrete Distributions},
  booktitle = {2016 {{International Symposium}} on {{Information Theory}} and {{Its Applications}} ({{ISITA}})},
  author = {Han, Yanjun and Jiao, Jiantao and Weissman, Tsachy},
  year = {2016},
  month = oct,
  pages = {256--260},
  abstract = {We refine the general methodology in [1] for the construction and analysis of essentially minimax estimators for a wide class of functionals of finite dimensional parameters, and elaborate on the case of discrete distributions with support size S comparable with the number of observations n. Specifically, we determine the "smooth" and "non-smooth" regimes based on the confidence set and the smoothness of the functional. In the "non-smooth" regime, we apply an unbiased estimator for a "suitable" polynomial approximation of the functional. In the "smooth" regime, we construct a bias corrected version of the Maximum Likelihood Estimator (MLE) based on Taylor expansion. We apply the general methodology to the problem of estimating the KL divergence between two discrete distributions from empirical data. We construct a minimax rate-optimal estimator which is adaptive in the sense that it does not require the knowledge of the support size nor the upper bound on the likelihood ratio. Moreover, the performance of the optimal estimator with n samples is essentially that of the MLE with n ln n samples, i.e., the effective sample size enlargement phenomenon holds.},
  keywords = {confidence set,discrete distributions,finite dimensional parameters,functional smoothness,KL divergence,Loss measurement,maximum likelihood estimation,Maximum likelihood estimation,maximum likelihood estimator,minimax rate-optimal estimation,minimax techniques,MLE,nonsmooth regime,polynomial approximation,Q measurement,Size measurement,statistical distributions,Taylor expansion,Taylor series,unbiased estimator,Upper bound},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Han et al. - 2016 - Minimax rate-optimal estimation of KL divergence b.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\9YM97IXX\\7840425.html}
}

@article{han2020,
  title = {Minimax {{Estimation}} of {{Divergences Between Discrete Distributions}}},
  author = {Han, Yanjun and Jiao, Jiantao and Weissman, Tsachy},
  year = {2020},
  month = nov,
  journal = {IEEE Journal on Selected Areas in Information Theory},
  volume = {1},
  number = {3},
  pages = {814--823},
  issn = {2641-8770},
  doi = {10.1109/JSAIT.2020.3041036},
  abstract = {We study the minimax estimation of \textsuperscript{2\$} -divergences as special examples. Dropping the usual theoretical tricks to acquire independence, we construct the first minimax rate-optimal estimator which does not require any Poissonization, sample splitting, or explicit construction of approximating polynomials. The estimator uses a hybrid approach which solves a problem-independent linear program based on moment matching in the non-smooth regime, and applies a problem-dependent bias-corrected plug-in estimator in the smooth regime, with a soft decision boundary between these regimes.},
  keywords = {Estimation,Functional estimation,information measures,Information theory,linear programming,minimax estimation,polynomial approximation,Sociology,Statistics,Support vector machines,Two dimensional displays,Upper bound},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Han et al. - 2020 - Minimax Estimation of Divergences Between Discrete.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\X66UWLDV\\9273044.html}
}

@article{hao2019,
  title = {The {{Broad Optimality}} of {{Profile Maximum Likelihood}}},
  author = {Hao, Yi and Orlitsky, Alon},
  year = {2019},
  journal = {Advances in Neural Information Processing Systems},
  volume = {32},
  pages = {10991--11003},
  urldate = {2021-01-24},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Hao and Orlitsky - 2019 - The Broad Optimality of Profile Maximum Likelihood.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\57KUKF9X\\f9fd5ec4c141a95257aa99ef1b590672-Abstract.html}
}

@article{harper2020,
  title = {Efficient Learning of Quantum Noise},
  author = {Harper, Robin and Flammia, Steven T. and Wallman, Joel J.},
  year = {2020},
  month = dec,
  journal = {Nature Physics},
  volume = {16},
  number = {12},
  pages = {1184--1188},
  publisher = {{Nature Publishing Group}},
  issn = {1745-2481},
  doi = {10.1038/s41567-020-0992-8},
  urldate = {2023-07-09},
  abstract = {Noise is the central obstacle to building large-scale quantum computers. Quantum systems with sufficiently uncorrelated and weak noise could be used to solve computational problems that are intractable with current digital computers. There has been substantial progress towards engineering such systems1\textendash 8. However, continued progress depends on the ability to characterize quantum noise reliably and efficiently with high precision9. Here, we describe such a protocol and report its experimental implementation on a 14-qubit superconducting quantum architecture. The method returns an estimate of the effective noise and can detect correlations within arbitrary sets of qubits. We show how to construct a quantum noise correlation matrix allowing the easy visualization of correlations between all pairs of qubits, enabling the discovery of long-range two-qubit correlations in the 14-qubit device that had not previously been detected. Our results are the first implementation of a provably rigorous and comprehensive diagnostic protocol capable of being run on state-of-the-art devices and beyond. These results pave the way for noise metrology in next-generation quantum devices, calibration in the presence of crosstalk, bespoke quantum error-correcting codes10 and customized fault-tolerance protocols11 that can greatly reduce the overhead in a quantum computation.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Quantum information,Quantum mechanics,Qubits},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Harper et al. - 2020 - Efficient learning of quantum noise.pdf}
}

@article{harries1998,
  title = {Extracting {{Hidden Context}}},
  author = {Harries, Michael Bonnell and Sammut, Claude and Horn, Kim},
  year = {1998},
  month = aug,
  journal = {Machine Learning},
  volume = {32},
  number = {2},
  pages = {101--126},
  issn = {0885-6125, 1573-0565},
  doi = {10.1023/A:1007420529897},
  urldate = {2018-05-23},
  abstract = {Concept drift due to hidden changes in context complicates learning in many domains including financial prediction, medical diagnosis, and communication network performance. Existing machine learning approaches to this problem use an incremental learning, on-line paradigm. Batch, off-line learners tend to be ineffective in domains with hidden changes in context as they assume that the training set is homogeneous. An off-line, meta-learning approach for the identification of hidden context is presented. The new approach uses an existing batch learner and the process of contextual clustering to identify stable hidden contexts and the associated context specific, locally stable concepts. The approach is broadly applicable to the extraction of context reflected in time and spatial attributes. Several algorithms for the approach are presented and evaluated. A successful application of the approach to a complex flight simulator control task is also presented.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Harries et al. - 1998 - Extracting Hidden Context.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\YIEWDD7Q\\A1007420529897.html}
}

@techreport{harries1999,
  type = {Technical Report},
  title = {{{SPLICE-2 Comparative Evaluation}}: {{Electricity Pricing}}},
  shorttitle = {{{SPLICE-2 Comparative Evaluation}}},
  author = {Harries, Michael},
  year = {1999},
  abstract = {SPLICE-2 is a machine learning method designed for batch learning  in domains with hidden changes in context. This report characterises  the performance of SPLICE-2 on a real world dataset in comparison  with C4.5, an on-line learner (emulated by C4.5), and an unsupervised  learning system.},
  keywords = {dataset,ELEC2},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Harries - 1999 - SPLICE-2 Comparative Evaluation Electricity Prici.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\8GFN5UIF\\summary.html}
}

@incollection{harvey2006,
  title = {Chapter 7 {{Forecasting}} with {{Unobserved Components Time Series Models}}},
  booktitle = {Handbook of {{Economic Forecasting}}},
  author = {Harvey, Andrew},
  editor = {Elliott, G. and Granger, C. W. J. and Timmermann, A.},
  year = {2006},
  month = jan,
  volume = {1},
  pages = {327--412},
  publisher = {{Elsevier}},
  doi = {10.1016/S1574-0706(05)01007-4},
  urldate = {2018-04-05},
  abstract = {Structural time series models are formulated in terms of components, such as trends, seasonals and cycles, that have a direct interpretation. As well as providing a framework for time series decomposition by signal extraction, they can be used for forecasting and for `nowcasting'. The structural interpretation allows extensions to classes of models that are able to deal with various issues in multivariate series and to cope with non-Gaussian observations and nonlinear models. The statistical treatment is by the state space form and hence data irregularities such as missing observations are easily handled. Continuous time models offer further flexibility in that they can handle irregular spacing. The paper compares the forecasting performance of structural time series models with ARIMA and autoregressive models. Results are presented showing how observations in linear state space models are implicitly weighted in making forecasts and hence how autoregressive and vector error correction representations can be obtained. The use of an auxiliary series in forecasting and nowcasting is discussed. A final section compares stochastic volatility models with GARCH.},
  keywords = {continuous time,cycles,Kalman filter,non-Gaussian models,state space,stochastic trend,stochastic volatility},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Harvey - 2006 - Chapter 7 Forecasting with Unobserved Components T.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\IM8WYP6K\\S1574070605010074.html}
}

@inproceedings{haug2021,
  title = {Learning {{Parameter Distributions}} to {{Detect Concept Drift}} in {{Data Streams}}},
  booktitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Haug, Johannes and Kasneci, Gjergji},
  year = {2021},
  month = jan,
  eprint = {2010.09388},
  primaryclass = {cs, stat},
  pages = {9452--9459},
  doi = {10.1109/ICPR48806.2021.9412499},
  urldate = {2022-07-27},
  abstract = {Data distributions in streaming environments are usually not stationary. In order to maintain a high predictive quality at all times, online learning models need to adapt to distributional changes, which are known as concept drift. The timely and robust identification of concept drift can be difficult, as we never have access to the true distribution of streaming data. In this work, we propose a novel framework for the detection of real concept drift, called ERICS. By treating the parameters of a predictive model as random variables, we show that concept drift corresponds to a change in the distribution of optimal parameters. To this end, we adopt common measures from information theory. The proposed framework is completely model-agnostic. By choosing an appropriate base model, ERICS is also capable to detect concept drift at the input level, which is a significant advantage over existing approaches. An evaluation on several synthetic and real-world data sets suggests that the proposed framework identifies concept drift more effectively and precisely than various existing works.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Haug and Kasneci - 2021 - Learning Parameter Distributions to Detect Concept.pdf}
}

@article{heggernes2006,
  title = {Minimal Triangulations of Graphs: {{A}} Survey},
  shorttitle = {Minimal Triangulations of Graphs},
  author = {Heggernes, Pinar},
  year = {2006},
  month = feb,
  journal = {Discrete Mathematics},
  series = {Minimal {{Separation}} and {{Minimal Triangulation}}},
  volume = {306},
  number = {3},
  pages = {297--317},
  issn = {0012-365X},
  doi = {10.1016/j.disc.2005.12.003},
  urldate = {2022-10-10},
  abstract = {Any given graph can be embedded in a chordal graph by adding edges, and the resulting chordal graph is called a triangulation of the input graph. In this paper we study minimal triangulations, which are the result of adding an inclusion minimal set of edges to produce a triangulation. This topic was first studied from the standpoint of sparse matrices and vertex elimination in graphs. Today we know that minimal triangulations are closely related to minimal separators of the input graph. Since the first papers presenting minimal triangulation algorithms appeared in 1976, several characterizations of minimal triangulations have been proved, and a variety of algorithms exist for computing minimal triangulations of both general and restricted graph classes. This survey presents and ties together these results in a unified modern notation, keeping an emphasis on the algorithms.},
  langid = {english},
  keywords = {Chordal graphs,Minimal fill,Minimal triangulation},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\UJ4S45E9\\Heggernes - 2006 - Minimal triangulations of graphs A survey.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\4RKVCQRX\\S0012365X05006060.html}
}

@article{hellinger1909,
  title = {{Neue Begr\"undung der Theorie quadratischer Formen von unendlichvielen Ver\"anderlichen.}},
  author = {Hellinger, E.},
  year = {1909},
  journal = {Journal f\"ur die reine und angewandte Mathematik},
  volume = {136},
  pages = {210--271},
  issn = {0075-4102},
  urldate = {2020-02-02},
  langid = {ngerman},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\67VSFC7E\\img.html}
}

@article{helly1923,
  title = {{\"Uber Mengen konvexer K\"orper mit gemeinschaftlichen Punkte.}},
  author = {Helly, Ed},
  year = {1923},
  journal = {Jahresbericht der Deutschen Mathematiker-Vereinigung},
  volume = {32},
  pages = {175--176},
  issn = {0012-0456; 1869-7135},
  urldate = {2022-03-05},
  langid = {und},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\WI2HHF9M\\145659.html}
}

@article{helmbold1994,
  title = {Tracking Drifting Concepts by Minimizing Disagreements},
  author = {Helmbold, David P. and Long, Philip M.},
  year = {1994},
  month = jan,
  journal = {Machine Learning},
  volume = {14},
  number = {1},
  pages = {27--45},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00993161},
  urldate = {2018-05-23},
  abstract = {In this paper we consider the problem of tracking a subset of a domain (called thetarget) which changes gradually over time. A single (unknown) probability distribution over the domain is used to generate random examples for the learning algorithm and measure the speed at which the target changes. Clearly, the more rapidly the target moves, the harder it is for the algorithm to maintain a good approximation of the target. Therefore we evaluate algorithms based on how much movement of the target can be tolerated between examples while predicting with accuracy {$\epsilon$}. Furthermore, the complexity of the classH of possible targets, as measured byd, its VC-dimension, also effects the difficulty of tracking the target concept. We show that if the problem of minimizing the number of disagreements with a sample from among concepts in a classH can be approximated to within a factork, then there is a simple tracking algorithm forH which can achieve a probability {$\epsilon$} of making a mistake if the target movement rate is at most a constant times {$\epsilon$}2/(k(d +k) ln 1/{$\epsilon$}), whered is the Vapnik-Chervonenkis dimension ofH. Also, we show that ifH is properly PAC-learnable, then there is an efficient (randomized) algorithm that with high probability approximately minimizes disagreements to within a factor of 7d + 1, yielding an efficient tracking algorithm forH which tolerates drift rates up to a constant times {$\epsilon$}2/(d2 ln 1/{$\epsilon$}). In addition, we prove complementary results for the classes of halfspaces and axisaligned hyperrectangles showing that the maximum rate of drift that any algorithm (even with unlimited computational power) can tolerate is a constant times {$\epsilon$}2/d.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Helmbold and Long - 1994 - Tracking drifting concepts by minimizing disagreem.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\XFTC6R3R\\BF00993161.html}
}

@incollection{hemmecke2010,
  title = {Nonlinear {{Integer Programming}}},
  booktitle = {50 {{Years}} of {{Integer Programming}} 1958-2008: {{From}} the {{Early Years}} to the {{State-of-the-Art}}},
  author = {Hemmecke, Raymond and K{\"o}ppe, Matthias and Lee, Jon and Weismantel, Robert},
  editor = {J{\"u}nger, Michael and Liebling, Thomas M. and Naddef, Denis and Nemhauser, George L. and Pulleyblank, William R. and Reinelt, Gerhard and Rinaldi, Giovanni and Wolsey, Laurence A.},
  year = {2010},
  pages = {561--618},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-68279-0\_15},
  urldate = {2021-08-07},
  abstract = {Research efforts of the past fifty years have led to a development of linear integer programming as a mature discipline of mathematical optimization. Such a level of maturity has not been reached when one considers nonlinear systems subject to integrality requirements for the variables. This chapter is dedicated to this topic. The primary goal is a study of a simple version of general nonlinear integer problems, where all constraints are still linear. Our focus is on the computational complexity of the problem, which varies significantly with the type of nonlinear objective function in combination with the underlying combinatorial structure. Numerous boundary cases of complexity emerge, which sometimes surprisingly lead even to polynomial time algorithms.We also cover recent successful approaches for more general classes of problems. Though no positive theoretical efficiency results are available, nor are they likely to ever be available, these seem to be the currently most successful and interesting approaches for solving practical problems. It is our belief that the study of algorithms motivated by theoretical considerations and those motivated by our desire to solve practical instances should and do inform one another. So it is with this viewpoint that we present the subject, and it is in this direction that we hope to spark further research.},
  isbn = {978-3-540-68279-0},
  langid = {english},
  keywords = {Convex Envelope,Fully Polynomial Time Approximation Scheme,Graver Basis,Independence System,Integer Point},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Hemmecke et al. - 2010 - Nonlinear Integer Programming.pdf}
}

@inproceedings{hengwang2015,
  title = {Concept Drift Detection for Streaming Data},
  booktitle = {2015 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {{Heng Wang} and Abraham, Z.},
  year = {2015},
  month = jul,
  pages = {1--9},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2015.7280398},
  abstract = {Common statistical prediction models often require and assume stationarity in the data. However, in many practical applications, changes in the relationship of the response and predictor variables are regularly observed over time, resulting in the deterioration of the predictive performance of these models. This paper presents Linear Four Rates (LFR), a framework for detecting these concept drifts and subsequently identifying the data points that belong to the new concept (for relearning the model). Unlike conventional concept drift detection approaches, LFR can be applied to both batch and stream data; is not limited by the distribution properties of the response variable (e.g., datasets with imbalanced labels); is independent of the underlying statistical-model; and uses user-specified parameters that are intuitively comprehensible. The performance of LFR is compared to benchmark approaches using both simulated and commonly used public datasets that span the gamut of concept drift types. The results show LFR significantly outperforms benchmark approaches in terms of recall, accuracy and delay in detection of concept drifts across datasets.},
  keywords = {\_tablet,Data models,Radio frequency,Yttrium},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\BIP8DCYL\\Heng Wang and Abraham - 2015 - Concept drift detection for streaming data.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\HG5NJR7Z\\7280398.html}
}

@article{hennequin2011,
  title = {Beta-{{Divergence}} as a {{Subclass}} of {{Bregman Divergence}}},
  author = {Hennequin, Romain and David, Bertrand and Badeau, Roland},
  year = {2011},
  month = feb,
  journal = {IEEE Signal Processing Letters},
  volume = {18},
  number = {2},
  pages = {83--86},
  issn = {1558-2361},
  doi = {10.1109/LSP.2010.2096211},
  abstract = {In this paper, we present a complete proof that the {$\beta$}-divergence is a particular case of Bregman divergence. This little-known result makes it possible to straightforwardly apply theorems about Bregman divergences to {$\beta$}-divergences. This is of interest for numerous applications since these divergences are widely used, for instance in non-negative matrix factorization (NMF).},
  keywords = {Approximation methods,Beta-divergence,Bregman divergence,Cost function,Electronic mail,Materials,non-negative matrix factorization,Signal processing,Spectroscopy,USA Councils},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\7CXAERCK\\Hennequin et al. - 2011 - Beta-Divergence as a Subclass of Bregman Divergenc.pdf;D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Hennequin et al. - 2011 - Beta-Divergence as a Subclass of Bregman Divergenc.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\BBVVVMHH\\5654542.html}
}

@article{hestness2017,
  title = {Deep {{Learning Scaling}} Is {{Predictable}}, {{Empirically}}},
  author = {Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
  year = {2017},
  month = dec,
  journal = {arXiv:1712.00409 [cs, stat]},
  eprint = {1712.00409},
  primaryclass = {cs, stat},
  urldate = {2019-05-28},
  abstract = {Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Hestness et al. - 2017 - Deep Learning Scaling is Predictable, Empirically.pdf}
}

@misc{hinder2020,
  title = {Analysis of {{Drifting Features}}},
  author = {Hinder, Fabian and Jakob, Jonathan and Hammer, Barbara},
  year = {2020},
  month = dec,
  number = {arXiv:2012.00499},
  eprint = {2012.00499},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2012.00499},
  urldate = {2023-04-24},
  abstract = {The notion of concept drift refers to the phenomenon that the distribution, which is underlying the observed data, changes over time. We are interested in an identification of those features, that are most relevant for the observed drift. We distinguish between drift inducing features, for which the observed feature drift cannot be explained by any other feature, and faithfully drifting features, which correlate with the present drift of other features. This notion gives rise to minimal subsets of the feature space, which are able to characterize the observed drift as a whole. We relate this problem to the problems of feature selection and feature relevance learning, which allows us to derive a detection algorithm. We demonstrate its usefulness on different benchmarks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Hinder et al. - 2020 - Analysis of Drifting Features.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\AXI2T4XX\\2012.html}
}

@inproceedings{hinder2022,
  title = {Localization of {{Concept Drift}}: {{Identifying}} the {{Drifting Datapoints}}},
  shorttitle = {Localization of {{Concept Drift}}},
  booktitle = {2022 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Hinder, Fabian and Vaquet, Valerie and Brinkrolf, Johannes and Artelt, Andr{\'e} and Hammer, Barbara},
  year = {2022},
  month = jul,
  pages = {1--9},
  issn = {2161-4407},
  doi = {10.1109/IJCNN55064.2022.9892374},
  abstract = {The notion of concept drift refers to the phenomenon that the distribution which is underlying the observed data changes over time. As a consequence machine learning models may become inaccurate and need adjustment. While there do exist methods to detect concept drift, to find change points in data streams, or to adjust models in the presence of observed drift, the problem of localizing drift, i.e. identifying it in data space, is yet widely unsolved - in particular from a formal perspective. This problem however is of importance, since it enables an inspection of the most prominent characteristics, e.g. features, where drift manifests itself and can therefore be used to make informed decisions, e.g. efficient updates of the training set of online learning algorithms, and perform precise adjustments of the learning model. In this paper we present a general theoretical framework that reduces drift localization to a supervised machine learning problem. We construct a new method for drift localization thereon and demonstrate the usefulness of our theory and the performance of our algorithm by comparing it to other methods from the literature.},
  keywords = {Analysis of Concept Drift,Concept Drift,Data models,Inspection,Localization of Concept Drift,Location awareness,Machine learning,Machine learning algorithms,Neural networks,Probabilistic Approaches,Supervised Learning,Training},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\DGA4J9M8\\9892374.html}
}

@misc{hinder2023,
  title = {Model {{Based Explanations}} of {{Concept Drift}}},
  author = {Hinder, Fabian and Vaquet, Valerie and Brinkrolf, Johannes and Hammer, Barbara},
  year = {2023},
  month = mar,
  number = {arXiv:2303.09331},
  eprint = {2303.09331},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-24},
  abstract = {The notion of concept drift refers to the phenomenon that the distribution generating the observed data changes over time. If drift is present, machine learning models can become inaccurate and need adjustment. While there do exist methods to detect concept drift or to adjust models in the presence of observed drift, the question of explaining drift, i.e., describing the potentially complex and high dimensional change of distribution in a human-understandable fashion, has hardly been considered so far. This problem is of importance since it enables an inspection of the most prominent characteristics of how and where drift manifests itself. Hence, it enables human understanding of the change and it increases acceptance of life-long learning models. In this paper, we present a novel technology characterizing concept drift in terms of the characteristic change of spatial features based on various explanation techniques. To do so, we propose a methodology to reduce the explanation of concept drift to an explanation of models that are trained in a suitable way extracting relevant information regarding the drift. This way a large variety of explanation schemes is available. Thus, a suitable method can be selected for the problem of drift explanation at hand. We outline the potential of this approach and demonstrate its usefulness in several examples.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Hinder et al. - 2023 - Model Based Explanations of Concept Drift.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\HNB8LZIF\\2303.html}
}

@inproceedings{hoens2011,
  title = {Heuristic {{Updatable Weighted Random Subspaces}} for {{Non-stationary Environments}}},
  booktitle = {2011 {{IEEE}} 11th {{International Conference}} on {{Data Mining}}},
  author = {Hoens, T. Ryan and Chawla, Nitesh V. and Polikar, Robi},
  year = {2011},
  month = dec,
  pages = {241--250},
  issn = {1550-4786},
  doi = {10.1109/ICDM.2011.75},
  abstract = {Learning in non-stationary environments is an increasingly important problem in a wide variety of real-world applications. In non-stationary environments data arrives incrementally, however the underlying generating function may change over time. While there is a variety of research into such environments, the research mainly consists of detecting concept drift (and then relearning the model), or developing classifiers which adapt to drift incrementally. We introduce Heuristic Up datable Weighted Random Subspaces (HUWRS), a new technique based on the Random Subspace Method that detects drift in individual features via the use of Hellinger distance, a distributional divergence metric. Through the use of subspaces, HUWRS allows for a more fine-grained approach to dealing with concept drift which is robust to feature drift even without class labels. We then compare our approach to two state of the art algorithms, concluding that for a wide range of datasets and window sizes HUWRS outperforms the other methods.},
  keywords = {Accuracy,Classification algorithms,Concept Drift,Context,data mining,Data mining,distributional divergence matrix,Feature extraction,fine-grained approach,Hellinger distance,Hellinger Distance,heuristic updatable weighted random subspaces,learning (artificial intelligence),Non-stationary learning,nonstationary learning environments,random processes,Random Subspaces,Testing,Training},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Hoens et al. - 2011 - Heuristic Updatable Weighted Random Subspaces for .pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\AQCKF6Q8\\6137228.html}
}

@inproceedings{hoens2012,
  title = {Learning in Non-Stationary Environments with Class Imbalance},
  booktitle = {Proceedings of the 18th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Hoens, Thomas Ryan and Chawla, Nitesh V.},
  year = {2012},
  month = aug,
  pages = {168--176},
  publisher = {{ACM}},
  address = {{Beijing China}},
  doi = {10.1145/2339530.2339558},
  urldate = {2023-02-25},
  abstract = {Learning in non-stationary environments is an increasingly important problem in a wide variety of real-world applications. In non-stationary environments data arrives incrementally, however the underlying generating function may change over time. In addition to the environments being non-stationary, they also often exhibit class imbalance. That is one class (the majority class) vastly outnumbers the other class (the minority class). This combination of class imbalance with non-stationary environments poses significant and interesting practical problems for classification. To overcome these issues, we introduce a novel instance selection mechanism, as well as provide a modification to the Heuristic Updatable Weighted Random Subspaces (HUWRS) method for the class imbalance problem. We then compare our modifications of HUWRS (called HUWRS.IP) to other state of the art algorithms, concluding that HUWRS. IP often achieves vastly superior performance.},
  isbn = {978-1-4503-1462-6},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Hoens and Chawla - 2012 - Learning in non-stationary environments with class.pdf}
}

@article{horn1972,
  title = {Three Results for Trees, Using Mathematical Induction},
  author = {Horn, W. A.},
  year = {1972},
  month = jan,
  journal = {Journal of Research of the National Bureau of Standards, Section B: Mathematical Sciences},
  volume = {76B},
  number = {1-2},
  pages = {39},
  issn = {0098-8979},
  doi = {10.6028/jres.076B.002},
  urldate = {2022-03-03},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Horn - 1972 - Three results for trees, using mathematical induct.pdf}
}

@article{horodecki2005,
  title = {Partial Quantum Information},
  author = {Horodecki, Micha{\l} and Oppenheim, Jonathan and Winter, Andreas},
  year = {2005},
  month = aug,
  journal = {Nature},
  volume = {436},
  number = {7051},
  pages = {673--676},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature03909},
  urldate = {2022-08-15},
  abstract = {Information\textemdash be it classical1 or quantum2\textemdash is measured by the amount of communication needed to convey it. In the classical case, if the receiver has some prior information about the messages being conveyed, less communication is needed3. Here we explore the concept of prior quantum information: given an unknown quantum state distributed over two systems, we determine how much quantum communication is needed to transfer the full state to one system. This communication measures the partial information one system needs, conditioned on its prior information. We find that it is given by the conditional entropy\textemdash a quantity that was known previously, but lacked an operational meaning. In the classical case, partial information must always be positive, but we find that in the quantum world this physical quantity can be negative. If the partial information is positive, its sender needs to communicate this number of quantum bits to the receiver; if it is negative, then sender and receiver instead gain the corresponding potential for future quantum communication. We introduce a protocol that we term `quantum state merging' which optimally transfers partial information. We show how it enables a systematic understanding of quantum network theory, and discuss several important applications including distributed compression, noiseless coding with side information, multiple access channels and assisted entanglement distillation.},
  copyright = {2005 Nature Publishing Group},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Horodecki et al. - 2005 - Partial quantum information.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\YCFYA4VR\\nature03909.html}
}

@inproceedings{huang2017,
  title = {Densely {{Connected Convolutional Networks}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Huang, Gao and Liu, Zhuang and {van der Maaten}, Laurens and Weinberger, Kilian Q.},
  year = {2017},
  pages = {4700--4708},
  urldate = {2019-08-22},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Huang et al. - 2017 - Densely Connected Convolutional Networks.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\4VH5RE9N\\Huang_Densely_Connected_Convolutional_CVPR_2017_paper.html}
}

@inproceedings{hulten2001,
  title = {Mining {{Time-changing Data Streams}}},
  booktitle = {Proceedings of the {{Seventh ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Hulten, Geoff and Spencer, Laurie and Domingos, Pedro},
  year = {2001},
  series = {{{KDD}} '01},
  pages = {97--106},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/502512.502529},
  urldate = {2018-03-25},
  abstract = {Most statistical and machine-learning algorithms assume that the data is a random sample drawn from a stationary distribution. Unfortunately, most of the large databases available for mining today violate this assumption. They were gathered over months or years, and the underlying processes generating them changed during this time, sometimes radically. Although a number of algorithms have been proposed for learning time-changing concepts, they generally do not scale well to very large databases. In this paper we propose an efficient algorithm for mining decision trees from continuously-changing data streams, based on the ultra-fast VFDT decision tree learner. This algorithm, called CVFDT, stays current while making the most of old data by growing an alternative subtree whenever an old one becomes questionable, and replacing the old with the new when the new becomes more accurate. CVFDT learns a model which is similar in accuracy to the one that would be learned by reapplying VFDT to a moving window of examples every time a new example arrives, but with O(1) complexity per example, as opposed to O(w), where w is the size of the window. Experiments on a set of large time-changing data streams demonstrate the utility of this approach.},
  isbn = {978-1-58113-391-2},
  keywords = {concept drift,data streams,Decision trees,Hoeffding bounds,incremental learning,subsampling},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Hulten et al. - 2001 - Mining Time-changing Data Streams.pdf}
}

@inproceedings{huynh2017,
  title = {A {{New Adaptive Learning Algorithm}} and {{Its Application}} to {{Online Malware Detection}}},
  booktitle = {Discovery {{Science}}},
  author = {Huynh, Ngoc Anh and Ng, Wee Keong and Ariyapala, Kanishka},
  year = {2017},
  month = oct,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {18--32},
  publisher = {{Springer, Cham}},
  doi = {10.1007/978-3-319-67786-6\_2},
  urldate = {2018-02-26},
  abstract = {Nowadays, the number of new malware samples discovered every day is in millions, which undermines the effectiveness of the traditional signature-based approach towards malware detection. To address this problem, machine learning methods have become an attractive and almost imperative solution. In most of the previous work, the application of machine learning to this problem is batch learning. Due to its fixed setting during the learning phase, batch learning often results in low detection accuracy when encountered zero-day samples with obfuscated appearance or unseen behavior. Therefore, in this paper, we propose the FTRL-DP online algorithm to address the problem of malware detection under concept drift when the behavior of malware changes over time. The experimental results show that online learning outperforms batch learning in all settings, either with or without retrainings.},
  isbn = {978-3-319-67785-9 978-3-319-67786-6},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Huynh et al. - 2017 - A New Adaptive Learning Algorithm and Its Applicat.pdf}
}

@article{ilic2017,
  title = {On a {{General Definition}} of {{Conditional R\'enyi Entropies}}},
  author = {Ili{\'c}, Velimir M. and Djordjevi{\'c}, Ivan B. and Stankovi{\'c}, Miomir},
  year = {2017},
  journal = {Proceedings},
  volume = {2},
  number = {4},
  pages = {166},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2504-3900},
  doi = {10.3390/ecea-4-05030},
  urldate = {2022-10-11},
  abstract = {In recent decades, different definitions of conditional R\'enyi entropy (CRE) have been introduced. Thus, Arimoto proposed a definition that found an application in information theory, Jizba and Arimitsu proposed a definition that found an application in time series analysis and Renner-Wolf, Hayashi and Cachin proposed definitions that are suitable for cryptographic applications. However, there is still no a commonly accepted definition, nor a general treatment of the CRE-s, which can essentially and intuitively be represented as an average uncertainty about a random variable X if a random variable Y is given. In this paper we fill the gap and propose a three-parameter CRE, which contains all of the previous definitions as special cases that can be obtained by a proper choice of the parameters. Moreover, it satisfies all of the properties that are simultaneously satisfied by the previous definitions, so that it can successfully be used in aforementioned applications. Thus, we show that the proposed CRE is positive, continuous, symmetric, permutation invariant, equal to R\'enyi entropy for independent X and Y, equal to zero for     X = Y     and monotonic. In addition, as an example for the further usage, we discuss the properties of generalized mutual information, which is defined using proposed CRE.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {conditional entropy,mutual information,R\'enyi entropy},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\KTTSWIZF\\Ilić et al. - 2017 - On a General Definition of Conditional Rényi Entro.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\94LJY42E\\166.html}
}

@inproceedings{itakura1968,
  title = {Analysis Synthesis Telephony Based on the Maximum Likelihood Method},
  booktitle = {Proceedings of the 6th {{International Congress}} on {{Acoustics}}},
  author = {Itakura, Fumitada and Saito, Shuzo},
  year = {1968},
  month = aug,
  pages = {17--20},
  address = {{Tokyo, Japan}},
  urldate = {2023-03-15},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\Q99CQQXF\\1574231875716873856.html}
}

@article{janhunen2017,
  title = {Learning Discrete Decomposable Graphical Models via Constraint Optimization},
  author = {Janhunen, Tomi and Gebser, Martin and Rintanen, Jussi and Nyman, Henrik and Pensar, Johan and Corander, Jukka},
  year = {2017},
  month = jan,
  journal = {Statistics and Computing},
  volume = {27},
  number = {1},
  pages = {115--130},
  issn = {1573-1375},
  doi = {10.1007/s11222-015-9611-4},
  urldate = {2022-09-19},
  abstract = {Statistical model learning problems are traditionally solved using either heuristic greedy optimization or stochastic simulation, such as Markov chain Monte Carlo or simulated annealing. Recently, there has been an increasing interest in the use of combinatorial search methods, including those based on computational logic. Some of these methods are particularly attractive since they can also be successful in proving the global optimality of solutions, in contrast to stochastic algorithms that only guarantee optimality at the limit. Here we improve and generalize a recently introduced constraint-based method for learning undirected graphical models. The new method combines perfect elimination orderings with various strategies for solution pruning and offers a dramatic improvement both in terms of time and memory complexity. We also show that the method is capable of efficiently handling a more general class of models, called stratified/labeled graphical models, which have an astronomically larger model space.},
  langid = {english},
  keywords = {Answer set programming,Constraint programming,Graphical models,MAXSAT,Satisfiability,Structure learning},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Janhunen et al. - 2017 - Learning discrete decomposable graphical models vi.pdf}
}

@article{jaynes1957,
  title = {Information {{Theory}} and {{Statistical Mechanics}}},
  author = {Jaynes, E. T.},
  year = {1957},
  month = may,
  journal = {Physical Review},
  volume = {106},
  number = {4},
  pages = {620--630},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRev.106.620},
  urldate = {2021-04-28},
  abstract = {Information theory provides a constructive criterion for setting up probability distributions on the basis of partial knowledge, and leads to a type of statistical inference which is called the maximum-entropy estimate. It is the least biased estimate possible on the given information; i.e., it is maximally noncommittal with regard to missing information. If one considers statistical mechanics as a form of statistical inference rather than as a physical theory, it is found that the usual computational rules, starting with the determination of the partition function, are an immediate consequence of the maximum-entropy principle. In the resulting "subjective statistical mechanics," the usual rules are thus justified independently of any physical argument, and in particular independently of experimental verification; whether or not the results agree with experiment, they still represent the best estimates that could have been made on the basis of the information available.},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\L677KLEZ\\PhysRev.106.html}
}

@book{jeffreys1998,
  title = {The {{Theory}} of {{Probability}}},
  author = {Jeffreys, Sir Harold},
  year = {1998},
  month = aug,
  series = {Oxford {{Classic Texts}} in the {{Physical Sciences}}},
  edition = {Third Edition, Third Edition},
  publisher = {{Oxford University Press}},
  address = {{Oxford, New York}},
  abstract = {Published in 1939, this book was the first to develop a fundamental theory of scientific inference based on the ideas of Bayesian statistics. Recent advances in computer power and availability have brought Bayesian statistics into the limelight and make this book a must for all serious statisticians.                                                        ,                Published in 1939, this book was the first to develop a fundamental theory of scientific inference based on the ideas of Bayesian statistics. Recent advances in computer power and availability have brought Bayesian statistics into the limelight and make this book a must for all serious statisticians.},
  isbn = {978-0-19-850368-2},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\25PUKPTZ\\the-theory-of-probability-9780198503682cc=gb&lang=en&.html}
}

@article{jensen1990,
  title = {An Algebra of Bayesian Belief Universes for Knowledge-Based Systems},
  author = {Jensen, Finn Verner and Olesen, Kristian G. and Andersen, Stig Kjaer},
  year = {1990},
  journal = {Networks},
  volume = {20},
  number = {5},
  pages = {637--659},
  issn = {1097-0037},
  doi = {10.1002/net.3230200509},
  urldate = {2019-10-20},
  abstract = {Causal probabilistic networks (CPNs) have proved to be a useful knowledge representation tool for modeling domains where causal relations-in a broad sense-are a natural way of relating domain concepts and where uncertainty is inherited in these relations. The domain is modeled in a CPN by use of a directed graph where the nodes represent concepts in the domain and the arcs represent causal relations. Furthermore, the quantitative relation between a node and its immediate causes is expressed as conditional probabilities. During the last few years, several schemes based on probability theory for incorporating and propagating new information throughout a CPN has emerged. As long as the domain can be modeled by use of a singly connected CPN (i. e., no more than one path between any pair of nodes), the schemes operate directly in the CPN and perform conceptually simple operations in this structure. When it comes to more complicated structures such as multiply connected CPNs (i. e., more than one path is allowed between pairs of nodes), the schemes operate in derived structures where the embedded domain knowledge no longer is as explicit and transparent as in the CPN. Furthermore, the simplicity in the operations is lost also. This report outlines a scheme-the algebra of Bayesian belief universes-for absorbing and propagating evidence in multiply connected CPNs. The scheme provides a secondary structure, a junction tree, and a simple set of algebraic operations between objects in this structure, Collect Evidence and Distribute Evidence. These are the basic tools for making inference in a CPN domain model and yield a calculus as simple as in the case of singly connected CPNs.},
  copyright = {Copyright \textcopyright{} 1990 Wiley Periodicals, Inc., A Wiley Company},
  langid = {english},
  keywords = {sum-product-divide},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\NAM9MTNC\\net.html}
}

@article{jiang2019,
  title = {Identifying and {{Correcting Label Bias}} in {{Machine Learning}}},
  author = {Jiang, Heinrich and Nachum, Ofir},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.04966 [cs, stat]},
  eprint = {1901.04966},
  primaryclass = {cs, stat},
  urldate = {2019-06-21},
  abstract = {Datasets often contain biases which unfairly disadvantage certain groups, and classifiers trained on such datasets can inherit these biases. In this paper, we provide a mathematical formulation of how this bias can arise. We do so by assuming the existence of underlying, unknown, and unbiased labels which are overwritten by an agent who intends to provide accurate labels but may have biases against certain groups. Despite the fact that we only observe the biased labels, we are able to show that the bias may nevertheless be corrected by re-weighting the data points without changing the labels. We show, with theoretical guarantees, that training on the re-weighted dataset corresponds to training on the unobserved but unbiased labels, thus leading to an unbiased machine learning classifier. Our procedure is fast and robust and can be used with virtually any learning algorithm. We evaluate on a number of standard machine learning fairness datasets and a variety of fairness notions, finding that our method outperforms standard approaches in achieving fair classification.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\6H3X2J3Q\\Jiang et al_2019_Identifying and Correcting Label Bias in Machine Learning.pdf}
}

@article{jiao2014,
  title = {Information {{Measures}}: {{The Curious Case}} of the {{Binary Alphabet}}},
  shorttitle = {Information {{Measures}}},
  author = {Jiao, Jiantao and Courtade, Thomas A. and No, Albert and Venkat, Kartik and Weissman, Tsachy},
  year = {2014},
  month = dec,
  journal = {IEEE Transactions on Information Theory},
  volume = {60},
  number = {12},
  pages = {7616--7626},
  issn = {1557-9654},
  doi = {10.1109/TIT.2014.2360184},
  abstract = {Four problems related to information divergence measures defined on finite alphabets are considered. In three of the cases we consider, we illustrate a contrast that arises between the binary-alphabet and larger alphabet settings. This is surprising in some instances, since characterizations for the larger alphabet settings do not generalize their binary-alphabet counterparts. In particular, we show that f-divergences are not the unique decomposable divergences on binary alphabets that satisfy the data processing inequality, thereby clarifying claims that have previously appeared in the literature. We also show that Kullback-Leibler (KL) divergence is the unique Bregman divergence, which is also an f-divergence for any alphabet size. We show that KL divergence is the unique Bregman divergence, which is invariant to statistically sufficient transformations of the data, even when nondecomposable divergences are considered. Like some of the problems we consider, this result holds only when the alphabet size is at least three.},
  keywords = {\$f\$ -divergence,Atmospheric measurements,Binary alphabet,Binary Alphabet,Bregman divergence,Bregman Divergence,Convex functions,Data processing,data processing inequality,Data Processing Inequality,decomposable divergence,Decomposable Divergence,f- Divergence,Information theory,Kullback-Leibler (KL) divergence,Particle measurements,Q measurement,Size measurement,sufficiency property,Sufficiency Property},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Jiao et al. - 2014 - Information Measures The Curious Case of the Bina.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\2QIPN82E\\6910242.html}
}

@article{jiao2015,
  title = {Minimax {{Estimation}} of {{Functionals}} of {{Discrete Distributions}}},
  author = {Jiao, Jiantao and Venkat, Kartik and Han, Yanjun and Weissman, Tsachy},
  year = {2015},
  month = may,
  journal = {IEEE transactions on information theory},
  volume = {61},
  number = {5},
  pages = {2835--2885},
  issn = {0018-9448},
  doi = {10.1109/tit.2015.2412945},
  urldate = {2020-03-24},
  abstract = {We propose a general methodology for the construction and analysis of essentially minimax estimators for a wide class of functionals of finite dimensional parameters, and elaborate on the case of discrete distributions, where the support size S is unknown and may be comparable with or even much larger than the number of observations n. We treat the respective regions where the functional is nonsmooth and smooth separately. In the nonsmooth regime, we apply an unbiased estimator for the best polynomial approximation of the functional whereas, in the smooth regime, we apply a bias-corrected version of the maximum likelihood estimator (MLE). We illustrate the merit of this approach by thoroughly analyzing the performance of the resulting schemes for estimating two important information measures: 1) the entropy  H(P)={$\sum$}i=1S-pilnpi and 2)  F{$\alpha$}(P)={$\sum$}i=1Spi{$\alpha$}, {$\alpha$} {$>$} 0. We obtain the minimax L2 rates for estimating these functionals. In particular, we demonstrate that our estimator achieves the optimal sample complexity n {$\asymp$} S/ln S for entropy estimation. We also demonstrate that the sample complexity for estimating F{$\alpha$}(P), 0 {$<$} {$\alpha$} {$<$} 1, is n {$\asymp$} S1/{$\alpha$}/ln S, which can be achieved by our estimator but not the MLE. For 1 {$<$} {$\alpha$} {$<$} 3/2, we show the minimax L2 rate for estimating F{$\alpha$}(P) is (n ln n)-2({$\alpha-$}1) for infinite support size, while the maximum L2 rate for the MLE is n-2({$\alpha-$}1). For all the above cases, the behavior of the minimax rate-optimal estimators with n samples is essentially that of the MLE (plug-in rule) with n ln n samples, which we term ``effective sample size enlargement.'' We highlight the practical advantages of our schemes for the estimation of entropy and mutual information. We compare our performance with various existing approaches, and demonstrate that our approach reduces running time and boosts the accuracy. Moreover, we show that the minimax rate-optimal mutual information estimator yielded by our framework leads to significant performance boosts over the Chow\textendash Liu algorithm in learning graphical models. The wide use of information measure estimation suggests that the insights and estimators obtained in this paper could be broadly applicable.},
  pmcid = {PMC5786426},
  pmid = {29375152},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Jiao et al. - 2015 - Minimax Estimation of Functionals of Discrete Dist.pdf}
}

@article{jiao2018,
  title = {Minimax {{Estimation}} of the {{L1 Distance}}},
  author = {Jiao, Jiantao and Han, Yanjun and Weissman, Tsachy},
  year = {2018},
  month = oct,
  journal = {IEEE Transactions on Information Theory},
  volume = {64},
  number = {10},
  pages = {6672--6706},
  issn = {1557-9654},
  doi = {10.1109/TIT.2018.2846245},
  abstract = {We consider the problem of estimating the L1 distance between two discrete probability measures P and Q from empirical data in a nonasymptotic and large alphabet setting. When Q is known and one obtains n samples from P, we show that for every Q, the minimax rate-optimal estimator with n samples achieves performance comparable to that of the maximum likelihood estimator with n ln n samples. When both P and Q are unknown, we construct minimax rate-optimal estimators, whose worst case performance is essentially that of the known Q case with Q being uniform, implying that Q being uniform is essentially the most difficult case. The effective sample size enlargement phenomenon, identified by Jiao et al., holds both in the known Q case for every Q and the Q unknown case. However, the construction of optimal estimators for {$\parallel$}P - Q{$\parallel$}1 requires new techniques and insights beyond the approximation-based method of functional estimation by Jiao et al.},
  keywords = {Approximation error,approximation theory,approximation-based method,data analysis,Data processing,discrete probability measures P,Divergence estimation,empirical data,Entropy,Error analysis,functional estimation,high-dimensional statistics,known Q case,L1 distance,large alphabet setting,maximum likelihood estimation,Maximum likelihood estimation,maximum likelihood estimator,minimax estimation,minimax rate-optimal estimator,minimax techniques,multivariate approximation theory,nonasymptotic alphabet setting,optimal classification error,probability,Q measurement,Q unknown case,sample size enlargement phenomenon,total variation distance,worst case performance},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Jiao et al. - 2018 - Minimax Estimation of the L1 Distance.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\47NSDYKR\\8379458.html}
}

@article{JSSv035i03,
  title = {Learning Bayesian Networks with the Bnlearn {{R}} Package},
  author = {Scutari, Marco},
  year = {2010},
  journal = {Journal of Statistical Software},
  volume = {35},
  number = {3},
  pages = {1--22},
  doi = {10.18637/jss.v035.i03},
  abstract = {\&amp;lt;b\&amp;gt;bnlearn\&amp;lt;/b\&amp;gt; is an \&amp;lt;b\&amp;gt;R\&amp;lt;/b\&amp;gt; package (R Development Core Team 2010) which includes several algorithms for learning the structure of Bayesian networks with either discrete or continuous variables. Both constraint-based and score-based algorithms are implemented, and can use the functionality provided by the \&amp;lt;b\&amp;gt;snow\&amp;lt;/b\&amp;gt; package (Tierney et al. 2008) to improve their performance via parallel computing. Several network scores and conditional independence algorithms are available for both the learning algorithms and independent use. Advanced plotting options are provided by the \&amp;lt;b\&amp;gt;Rgraphviz\&amp;lt;/b\&amp;gt; package (Gentry et al. 2010).}
}

@article{karlsson2016,
  title = {Generalized Random Shapelet Forests},
  author = {Karlsson, Isak and Papapetrou, Panagiotis and Bostr{\"o}m, Henrik},
  year = {2016},
  month = sep,
  journal = {Data Mining and Knowledge Discovery},
  volume = {30},
  number = {5},
  pages = {1053--1085},
  issn = {1573-756X},
  doi = {10.1007/s10618-016-0473-y},
  urldate = {2019-03-13},
  abstract = {Shapelets are discriminative subsequences of time series, usually embedded in shapelet-based decision trees. The enumeration of time series shapelets is, however, computationally costly, which in addition to the inherent difficulty of the decision tree learning algorithm to effectively handle high-dimensional data, severely limits the applicability of shapelet-based decision tree learning from large (multivariate) time series databases. This paper introduces a novel tree-based ensemble method for univariate and multivariate time series classification using shapelets, called the generalized random shapelet forest algorithm. The algorithm generates a set of shapelet-based decision trees, where both the choice of instances used for building a tree and the choice of shapelets are randomized. For univariate time series, it is demonstrated through an extensive empirical investigation that the proposed algorithm yields predictive performance comparable to the current state-of-the-art and significantly outperforms several alternative algorithms, while being at least an order of magnitude faster. Similarly for multivariate time series, it is shown that the algorithm is significantly less computationally costly and more accurate than the current state-of-the-art.},
  langid = {english},
  keywords = {Decision trees,Ensemble methods,Multivariate time series,Time series classification,Time series shapelets},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Karlsson et al. - 2016 - Generalized random shapelet forests.pdf}
}

@inproceedings{kelly1999,
  title = {The {{Impact}} of {{Changing Populations}} on {{Classifier Performance}}},
  booktitle = {Proceedings of the {{Fifth ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Kelly, Mark G. and Hand, David J. and Adams, Niall M.},
  year = {1999},
  series = {{{KDD}} '99},
  pages = {367--371},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/312129.312285},
  urldate = {2018-05-23},
  isbn = {978-1-58113-143-7},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Kelly et al. - 1999 - The Impact of Changing Populations on Classifier P.pdf}
}

@misc{ketterer2023,
  title = {Characterizing Crosstalk of Superconducting Transmon Processors},
  author = {Ketterer, Andreas and Wellens, Thomas},
  year = {2023},
  month = mar,
  number = {arXiv:2303.14103},
  eprint = {2303.14103},
  primaryclass = {quant-ph},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.14103},
  urldate = {2023-06-26},
  abstract = {Currently available quantum computing hardware based on superconducting transmon architectures realizes networks of hundreds of qubits with the possibility of controlled nearest-neighbor interactions. However, the inherent noise and decoherence effects of such quantum chips considerably alter basic gate operations and lead to imperfect outputs of the targeted quantum computations. In this work, we focus on the characterization of crosstalk effects which manifest themselves in correlations between simultaneously executed quantum gates on neighboring qubits. After a short explanation of the physical origin of such correlations, we show how to efficiently and systematically characterize the magnitude of such crosstalk effects on an entire quantum chip using the randomized benchmarking protocol. We demonstrate the introduced protocol by running it on real quantum hardware provided by IBM observing significant alterations in gate fidelities due to crosstalk. Lastly, we use the gained information in order to propose more accurate means to simulate noisy quantum hardware by devising an appropriate crosstalk-aware noise model.},
  archiveprefix = {arxiv},
  keywords = {Quantum Physics},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\GEJ8D6P5\\Ketterer and Wellens - 2023 - Characterizing crosstalk of superconducting transm.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\92MGNCXV\\2303.html}
}

@article{kirkpatrick1983,
  title = {Optimization by {{Simulated Annealing}}},
  author = {Kirkpatrick, S. and Gelatt, C. D. and Vecchi, M. P.},
  year = {1983},
  month = may,
  journal = {Science},
  volume = {220},
  number = {4598},
  pages = {671--680},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.220.4598.671},
  urldate = {2018-09-16},
  abstract = {There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.},
  copyright = {\textcopyright{} 1983},
  langid = {english},
  pmid = {17813860},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Kirkpatrick et al. - 1983 - Optimization by Simulated Annealing.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\J7XU7SFD\\671.html}
}

@inproceedings{klinkenberg2001,
  title = {Using {{Labeled}} and {{Unlabeled Data}} to {{Learn Drifting Concepts}}},
  booktitle = {In {{Workshop}} Notes of {{IJCAI-01 Workshop}} on {{Learning}} from {{Temporal}} and {{Spatial Data}}},
  author = {Klinkenberg, Ralf},
  year = {2001},
  pages = {16--24},
  publisher = {{AAAI Press}},
  abstract = {For many learning tasks, where data is collected  over an extended period of time, one has to cope  two problems. The distribution underlying the data  is likely to change and only little labeled training  data is available at each point in time. A typical  example is information filtering, i. e. the adaptive  classification of documents with respect to a particular  user interest. Both the interest of the user  and the document content change over time. A filtering  system should be able to adapt to such concept  changes. Since users often give little feedback,  a filtering system should also be able to achieve a  good performance, even if only few labeled training  examples are provided. This paper proposes a  method to recognize and handle concept changes  with support vector machines and to use unlabeled  data to reduce the need for labeled data. The  method maintains windows on the training data,  whose size is automatically adjusted so that the estimated  generalization error is minimized. The approach  is both theoretically well-founded as well  as effective and efficient in practice. Since it does  not require complicated parameterization, it is simpler  to use and more robust than comparable heuristics.  Experiments with simulated concept drift scenarios  based on real-world text data compare the  new method with other window management approaches  and show that it can effectively select an  appropriate window size in a robust way. In order  to achieve an acceptable performance with fewer  labeled training examples, the proposed method exploits  unlabeled examples in a transductive way.  1},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Klinkenberg - 2001 - Using Labeled and Unlabeled Data to Learn Drifting.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\HZ8C28YJ\\summary.html}
}

@inproceedings{kohavi1996,
  title = {Scaling {{Up}} the {{Accuracy}} of {{Naive-Bayes Classifiers}}: A {{Decision-Tree Hybrid}}},
  booktitle = {Proceedings of the {{Second International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Kohavi, Ron},
  year = {1996},
  series = {{{KDD}}'96},
  pages = {6},
  publisher = {{AAAI Press}},
  address = {{Portland, Oregon}},
  doi = {10.1.1.57.4952},
  abstract = {Naive-Bayes induction algorithms were previously shown to be surprisingly accurate on many classi cation tasks even when the conditional independence assumption on which they are based is violated. However, most studies were done on small databases. We show that in some larger databases, the accuracy of Naive-Bayes does not scale up as well as decision trees. We then propose a new algorithm, NBTree, which induces a hybrid of decision-tree classi ers and NaiveBayes classi ers: the decision-tree nodes contain univariate splits as regular decision-trees, but the leaves contain Naive-Bayesian classi ers. The approach retains the interpretability of Naive-Bayes and decision trees, while resulting in classi ers that frequently outperform both constituents, especially in the larger databases tested.},
  langid = {english},
  keywords = {adult,dataset},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Kohavi - 1996 - Scaling Up the Accuracy of Naive-Bayes Classifiers.pdf}
}

@misc{kohavironny1996,
  title = {{{UCI Machine Learning Repository}}: {{Adult Data Set}}},
  author = {{Kohavi, Ronny} and {Becker, Barry}},
  year = {1996},
  month = may,
  urldate = {2018-11-08},
  howpublished = {https://archive.ics.uci.edu/ml/datasets/Adult},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\KRQPLHZX\\Adult.html}
}

@book{koller2009,
  title = {Probabilistic {{Graphical Models}}: {{Principles}} and {{Techniques}} - {{Adaptive Computation}} and {{Machine Learning}}},
  shorttitle = {Probabilistic {{Graphical Models}}},
  author = {Koller, Daphne and Friedman, Nir},
  year = {2009},
  publisher = {{The MIT Press}},
  abstract = {Most tasks require a person or an automated system to reasonto reach conclusions based on available information. The framework of probabilistic graphical models, presented in this book, provides a general approach for this task. The approach is model-based, allowing interpretable models to be constructed and then manipulated by reasoning algorithms. These models can also be learned automatically from data, allowing the approach to be used in cases where manually constructing a model is difficult or even impossible. Because uncertainty is an inescapable aspect of most real-world applications, the book focuses on probabilistic models, which make the uncertainty explicit and provide models that are more faithful to reality. Probabilistic Graphical Models discusses a variety of models, spanning Bayesian networks, undirected Markov networks, discrete and continuous models, and extensions to deal with dynamical systems and relational data. For each class of models, the text describes the three fundamental cornerstones: representation, inference, and learning, presenting both basic concepts and advanced techniques. Finally, the book considers the use of the proposed framework for causal reasoning and decision making under uncertainty. The main text in each chapter provides the detailed technical development of the key ideas. Most chapters also include boxes with additional material: skill boxes, which describe techniques; case study boxes, which discuss empirical cases related to the approach described in the text, including applications in computer vision, robotics, natural language understanding, and computational biology; and concept boxes, which present significant concepts drawn from the material in the chapter. Instructors (and readers) can group chapters in various combinations, from core topics to more technically advanced material, to suit their particular needs. Adaptive Computation and Machine Learning series},
  isbn = {978-0-262-01319-2},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Koller and Friedman - 2009 - Probabilistic Graphical Models Principles and Tec.pdf}
}

@inproceedings{korf2009,
  title = {Multi-Way {{Number Partitioning}}},
  booktitle = {Proceedings of the 21st {{International Jont Conference}} on {{Artifical Intelligence}}},
  author = {Korf, Richard E.},
  year = {2009},
  series = {{{IJCAI}}'09},
  pages = {538--543},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  address = {{San Francisco, CA, USA}},
  urldate = {2018-09-16},
  abstract = {The number partitioning problem is to divide a given set of integers into a collection of subsets, so that the sum of the numbers in each subset are as nearly equal as possible. While a very efficient algorithm exists for optimal two-way partitioning, it is not nearly as effective for multi-way partitioning. We develop two new linear-space algorithms for multi-way partitioning, and demonstrate their performance on three, four, and five-way partitioning. In each case, our algorithms outperform the previous state of the art by orders of magnitude, in one case by over six orders of magnitude. Empirical analysis of the running times of our algorithms strongly suggest that their asymptotic growth is less than that of previous algorithms. The key insight behind both our new algorithms is that if an optimal k-way partition includes a particular subset, then optimally partitioning the numbers not in that set k-1 ways results in an optimal k-way partition.},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Korf - 2009 - Multi-way Number Partitioning.pdf}
}

@article{kourtis,
  title = {Notes on {{Generating Probability Distributions}} for a given {{Entropy}} Value},
  author = {Kourtis, Kornilios},
  pages = {4},
  langid = {english},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\WJULJ5H7\\Kourtis - Notes on Generating Probability Distributions for .pdf}
}

@article{krawczyk2017,
  title = {Ensemble Learning for Data Stream Analysis: {{A}} Survey},
  shorttitle = {Ensemble Learning for Data Stream Analysis},
  author = {Krawczyk, Bartosz and Minku, Leandro L. and Gama, Jo{\~a}o and Stefanowski, Jerzy and Wo{\'z}niak, Micha{\l}},
  year = {2017},
  month = sep,
  journal = {Information Fusion},
  volume = {37},
  pages = {132--156},
  issn = {15662535},
  doi = {10.1016/j.inffus.2017.02.004},
  urldate = {2018-02-22},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Krawczyk et al. - 2017 - Ensemble learning for data stream analysis A surv.pdf}
}

@article{krempl2019,
  ids = {kremplTemporalDensityExtrapolation},
  title = {Temporal Density Extrapolation Using a Dynamic Basis Approach},
  author = {Krempl, G. and Lang, D. and Hofer, V.},
  year = {2019},
  month = sep,
  journal = {Data Mining and Knowledge Discovery},
  volume = {33},
  number = {5},
  pages = {1323--1356},
  issn = {1573-756X},
  doi = {10.1007/s10618-019-00636-0},
  urldate = {2021-08-07},
  abstract = {Density estimation is a versatile technique underlying many data mining tasks and techniques, ranging from exploration and presentation of static data, to probabilistic classification, or identifying changes or irregularities in streaming data. With the pervasiveness of embedded systems and digitisation, this latter type of streaming and evolving data becomes more important. Nevertheless, research in density estimation has so far focused on stationary data, leaving the task of of extrapolating and predicting density at time points outside a training window an open problem. For this task, temporal density extrapolation (TDX) is proposed. This novel method models and predicts gradual monotonous changes in a distribution. It is based on the expansion of basis functions, whose weights are modelled as functions of compositional data over time by using an isometric log-ratio transformation. Extrapolated density estimates are then obtained by extrapolating the weights to the requested time point, and querying the density from the basis functions with back-transformed weights. Our approach aims for broad applicability by neither being restricted to a specific parametric distribution, nor relying on cluster structure in the data. It requires only two additional extrapolation-specific parameters, for which reasonable defaults exist. Experimental evaluation on various data streams, synthetic as well as from the real-world domains of credit scoring and environmental health, shows that the model manages to capture monotonous drift patterns accurately and better than existing methods. Thereby, it requires not more than 1.5 times the run time of a corresponding static density estimation approach.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Krempl et al. - 2019 - Temporal density extrapolation using a dynamic bas.pdf}
}

@article{krueger2020,
  title = {Out-of-{{Distribution Generalization}} via {{Risk Extrapolation}} ({{REx}})},
  author = {Krueger, David and Caballero, Ethan and Jacobsen, Joern-Henrik and Zhang, Amy and Binas, Jonathan and Priol, Remi Le and Courville, Aaron},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.00688 [cs, stat]},
  eprint = {2003.00688},
  primaryclass = {cs, stat},
  urldate = {2020-07-05},
  abstract = {Generalizing outside of the training distribution is an open challenge for current machine learning systems. A weak form of out-of-distribution (OoD) generalization is the ability to successfully interpolate between multiple observed distributions. One way to achieve this is through robust optimization, which seeks to minimize the worst-case risk over convex combinations of the training distributions. However, a much stronger form of OoD generalization is the ability of models to extrapolate beyond the distributions observed during training. In pursuit of strong OoD generalization, we introduce the principle of Risk Extrapolation (REx). REx can be viewed as encouraging robustness over affine combinations of training risks, by encouraging strict equality between training risks. We show conceptually how this principle enables extrapolation, and demonstrate the effectiveness and scalability of instantiations of REx on various OoD generalization tasks. Our code can be found at https://github.com/capybaralet/REx\_code\_release.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,concept drift,Statistics - Machine Learning},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Krueger et al. - 2020 - Out-of-Distribution Generalization via Risk Extrap.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\FHWQWXHJ\\2003.html}
}

@article{kullback1951,
  title = {On {{Information}} and {{Sufficiency}}},
  author = {Kullback, S. and Leibler, R. A.},
  year = {1951},
  month = mar,
  journal = {The Annals of Mathematical Statistics},
  volume = {22},
  number = {1},
  pages = {79--86},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729694},
  urldate = {2022-10-11},
  abstract = {The Annals of Mathematical Statistics},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Kullback and Leibler - 1951 - On Information and Sufficiency.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\X33AXU5T\\1177729694.html}
}

@article{kumar2018,
  title = {Data {{Mining Models}} of {{High Dimensional Data Streams}}, and {{Contemporary Concept Drift Detection Methods}}: A {{Comprehensive Review}}},
  shorttitle = {Data {{Mining Models}} of {{High Dimensional Data Streams}}, and {{Contemporary Concept Drift Detection Methods}}},
  author = {Kumar, M. Sankara Prasanna and Kumar, A. P. Siva and Prasanna, K.},
  year = {2018},
  month = jul,
  journal = {International Journal of Engineering \& Technology},
  volume = {7},
  number = {3.6},
  pages = {148--153},
  issn = {2227-524X},
  doi = {10.14419/ijet.v7i3.6.14959},
  urldate = {2020-02-04},
  abstract = {Concept drift is defined as the distributed data across multiple data streams that change over the time. Concept drift is visible only when the type of collected data changes after some stable period. The emergence of concept drift in data streams leads to increase misclassification and performing degradation of data streams. In order to obtain accurate results, identification of such concept drifts must be visible. This paper focused on a review of the issues related to identifying the changes occurred in the various multivariate high dimensional data streams. The insight of the manuscript is probing the inbuilt difficulties of existing contemporary change-detection methods when they encounter during data dimensions scales.},
  copyright = {Copyright (c) 2018 International Journal of Engineering \& Technology},
  langid = {american},
  keywords = {Bayesian Online Change Point Detection.,change-detection tests,concept drift detection,CUSUM,dimensional data streams,Hoteling's t-squared test,streaming ensemble algorithm},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Kumar et al. - 2018 - Data Mining Models of High Dimensional Data Stream.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\STGHHRZQ\\14959.html}
}

@inproceedings{kurt2019,
  title = {Sequential {{Model-Free Anomaly Detection}} for {{Big Data Streams}}},
  booktitle = {2019 57th {{Annual Allerton Conference}} on {{Communication}}, {{Control}}, and {{Computing}} ({{Allerton}})},
  author = {Kurt, Mehmet Necip and Y{\i}lmaz, Yasin and Wang, Xiaodong},
  year = {2019},
  month = sep,
  pages = {421--425},
  issn = {null},
  doi = {10.1109/ALLERTON.2019.8919759},
  abstract = {We study sequential anomaly detection for big data streams where the nominal and anomalous high-dimensional probabilistic data models are unknown. We propose a model-free solution approach in that we firstly compute a set of univariate summary statistics from a nominal dataset in an offline phase where the summary statistics are useful to distinguish anomalous data from nominal data. We then evaluate whether the online summary statistics deviate from the nominal case via a cumulative sum-like detector. Our experiments with real-world data illustrate the advantages of the proposed detector in early and reliable anomaly detection in big data settings compared to the existing alternatives.},
  keywords = {anomalous data,Anomaly detection,Big data,Big Data,big data settings,big data streams,Computational modeling,cumulative sum (CUSUM),data analysis,Data models,Detectors,early detection,expectation-maximisation algorithm,high-dimensional probabilistic data models,model-free,model-free solution approach,nominal case,nominal data,nominal dataset,online summary statistics,pattern clustering,probability,Probability density function,real-world data,Reliability,reliable anomaly detection,sequential anomaly detection,sequential model-free anomaly detection,statistical analysis,summary statistic,univariate summary statistics},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Kurt et al. - 2019 - Sequential Model-Free Anomaly Detection for Big Da.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\L4MNZ5JY\\8919759.html}
}

@article{land1960,
  title = {An {{Automatic Method}} of {{Solving Discrete Programming Problems}}},
  author = {Land, A. H. and Doig, A. G.},
  year = {1960},
  journal = {Econometrica},
  volume = {28},
  number = {3},
  eprint = {1910129},
  eprinttype = {jstor},
  pages = {497--520},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {0012-9682},
  doi = {10.2307/1910129},
  urldate = {2021-02-23},
  abstract = {In the classical linear programming problem the behaviour of continuous, nonnegative variables subject to a system of linear inequalities is investigated. One possible generalization of this problem is to relax the continuity condition the variables. This paper presents a simple numerical algorithm for the solution of programming problems in which some or all of the variables can take only discrete values. The algorithm requires no special techniques beyond these used in ordinary linear programming, and lends itself to automatic computing. Its use is illustrated on two numerical examples.},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Land and Doig - 1960 - An Automatic Method of Solving Discrete Programmin.pdf}
}

@article{land1963,
  title = {A {{Problem}} of {{Assignment}} with {{Inter-Related Costs}}},
  author = {Land, A. H.},
  year = {1963},
  journal = {OR},
  volume = {14},
  number = {2},
  eprint = {3007372},
  eprinttype = {jstor},
  pages = {185--199},
  publisher = {{Operational Research Society}},
  issn = {1473-2858},
  doi = {10.2307/3007372},
  urldate = {2022-02-25},
  abstract = {There is a class of assignment problems where the cost function depends on the assignment of pairs of variables. A method of finding the optimum solution by a systematic exploration of a limited part of the solution space is presented by means of a numerical example. It is not possible to give a formula for the length of the computation since this will depend on the actual costs of the particular problem being solved.},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Land - 1963 - A Problem of Assignment with Inter-Related Costs.pdf}
}

@article{large2019,
  title = {A Probabilistic Classifier Ensemble Weighting Scheme Based on Cross-Validated Accuracy Estimates},
  author = {Large, James and Lines, Jason and Bagnall, Anthony},
  year = {2019},
  month = nov,
  journal = {Data Mining and Knowledge Discovery},
  volume = {33},
  number = {6},
  pages = {1674--1709},
  issn = {1573-756X},
  doi = {10.1007/s10618-019-00638-y},
  urldate = {2019-10-30},
  abstract = {Our hypothesis is that building ensembles of small sets of strong classifiers constructed with different learning algorithms is, on average, the best approach to classification for real-world problems. We propose a simple mechanism for building small heterogeneous ensembles based on exponentially weighting the probability estimates of the base classifiers with an estimate of the accuracy formed through cross-validation on the train data. We demonstrate through extensive experimentation that, given the same small set of base classifiers, this method has measurable benefits over commonly used alternative weighting, selection or meta-classifier approaches to heterogeneous ensembles. We also show how an ensemble of five well-known, fast classifiers can produce an ensemble that is not significantly worse than large homogeneous ensembles and tuned individual classifiers on datasets from the UCI archive. We provide evidence that the performance of the cross-validation accuracy weighted probabilistic ensemble (CAWPE) generalises to a completely separate set of datasets, the UCR time series classification archive, and we also demonstrate that our ensemble technique can significantly improve the state-of-the-art classifier for this problem domain. We investigate the performance in more detail, and find that the improvement is most marked in problems with smaller train sets. We perform a sensitivity analysis and an ablation study to demonstrate the robustness of the ensemble and the significant contribution of each design element of the classifier. We conclude that it is, on average, better to ensemble strong classifiers with a weighting scheme rather than perform extensive tuning and that CAWPE is a sensible starting point for combining classifiers.},
  langid = {english},
  keywords = {Classification,Ensemble,Heterogeneous,Weighted},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Large et al. - 2019 - A probabilistic classifier ensemble weighting sche.pdf}
}

@article{lauritzen1988,
  title = {Local {{Computations}} with {{Probabilities}} on {{Graphical Structures}} and {{Their Application}} to {{Expert Systems}}},
  author = {Lauritzen, Steffen L. and Spiegelhalter, David J.},
  year = {1988},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {50},
  number = {2},
  eprint = {2345762},
  eprinttype = {jstor},
  pages = {157--224},
  issn = {0035-9246},
  urldate = {2019-10-20},
  abstract = {A causal network is used in a number of areas as a depiction of patterns of `influence' among sets of variables. In expert systems it is common to perform `inference' by means of local computations on such large but sparse networks. In general, non-probabilistic methods are used to handle uncertainty when propagating the effects of evidence, and it has appeared that exact probabilistic methods are not computationally feasible. Motivated by an application in electromyography, we counter this claim by exploiting a range of local representations for the joint probability distribution, combined with topological changes to the original network termed `marrying' and `filling-in'. The resulting structure allows efficient algorithms for transfer between representations, providing rapid absorption and propagation of evidence. The scheme is first illustrated on a small, fictitious but challenging example, and the underlying theory and computational aspects are then discussed.},
  keywords = {sum-product-divide}
}

@book{lauritzen1996,
  title = {Graphical Models},
  author = {Lauritzen, Steffen L.},
  year = {1996},
  series = {Oxford Statistical Science Series},
  number = {17},
  publisher = {{Clarendon Press ; Oxford University Press}},
  address = {{Oxford : New York}},
  isbn = {978-0-19-852219-5},
  lccn = {QA279 .L35 1996},
  keywords = {Graphical modeling (Statistics)}
}

@misc{lecun1994,
  title = {The {{MNIST}} Database of Handwritten Digits},
  author = {LeCun, Yann and Cortes, Corinna and Burges, Christopher J. C.},
  year = {1994}
}

@misc{lee2022,
  title = {Computing {{Divergences}} between {{Discrete Decomposable Models}}},
  author = {Lee, Loong Kuan and Piatkowski, Nico and Petitjean, Fran{\c c}ois and Webb, Geoffrey I.},
  year = {2022},
  month = nov,
  number = {arXiv:2112.04583},
  eprint = {2112.04583},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.04583},
  urldate = {2022-12-10},
  abstract = {There are many applications that benefit from computing the exact divergence between 2 discrete probability measures, including machine learning. Unfortunately, in the absence of any assumptions on the structure or independencies within these distributions, computing the divergence between them is an intractable problem in high dimensions. We show that we are able to compute a wide family of functionals and divergences, such as the alpha-beta divergence, between two decomposable models, i.e. chordal Markov networks, in time exponential to the treewidth of these models. The alpha-beta divergence is a family of divergences that include popular divergences such as the Kullback-Leibler divergence, the Hellinger distance, and the chi-squared divergence. Thus, we can accurately compute the exact values of any of this broad class of divergences to the extent to which we can accurately model the two distributions using decomposable models.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\T4XEIPJW\\Lee et al_2022_Computing Divergences between Discrete Decomposable Models.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\K4U495BW\\2112.html}
}

@article{lee2023,
  title = {Computing {{Divergences}} between {{Discrete Decomposable Models}}},
  author = {Lee, Loong Kuan and Piatkowski, Nico and Petitjean, Fran{\c c}ois and Webb, Geoffrey I.},
  year = {2023},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {37},
  number = {10},
  pages = {12243--12251},
  issn = {2374-3468},
  doi = {10.1609/aaai.v37i10.26443},
  urldate = {2023-06-28},
  abstract = {There are many applications that benefit from computing the exact divergence between 2 discrete probability measures, including machine learning. Unfortunately, in the absence of any assumptions on the structure or independencies within these distributions, computing the divergence between them is an intractable problem in high dimensions. We show that we are able to compute a wide family of functionals and divergences, such as the alpha-beta divergence, between two decomposable models, i.e. chordal Markov networks, in time exponential to the treewidth of these models. The alpha-beta divergence is a family of divergences that include popular divergences such as the Kullback-Leibler divergence, the Hellinger distance, and the chi-squared divergence. Thus, we can accurately compute the exact values of any of this broad class of divergences to the extent to which we can accurately model the two distributions using decomposable models.},
  copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {RU: Stochastic Models \& Probabilistic Inference},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Lee et al. - 2023 - Computing Divergences between Discrete Decomposabl.pdf}
}

@article{lefort2013,
  title = {{{treeKL}}: {{A}} Distance between High Dimension Empirical Distributions},
  shorttitle = {{{treeKL}}},
  author = {Lefort, Riwal and Fleuret, Fran{\c c}ois},
  year = {2013},
  month = jan,
  journal = {Pattern Recognition Letters},
  volume = {34},
  number = {2},
  pages = {140--145},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2012.08.019},
  urldate = {2020-02-02},
  abstract = {This paper offers a methodological contribution for computing the distance between two empirical distributions in an Euclidean space of very large dimension. We propose to use decision trees instead of relying on standard quantification of the feature space. Our contribution is twofold: We first define a new distance between empirical distributions, based on the Kullback\textendash Leibler (KL) divergence between the distributions over the leaves of decision trees built for the two empirical distributions. Then, we propose a new procedure to build these unsupervised trees efficiently. The performance of this new metric is illustrated on image clustering and neuron classification. Results show that the tree-based method outperforms standard methods based on standard bag-of-features procedures.},
  langid = {english},
  keywords = {Distribution modeling,Kulback\textendash Leibler distance,Unsupervised trees},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Lefort and Fleuret - 2013 - treeKL A distance between high dimension empirica.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\ZKS94VM2\\S0167865512002838.html}
}

@article{lenguyen2019,
  title = {Interpretable Time Series Classification Using Linear Models and Multi-Resolution Multi-Domain Symbolic Representations},
  author = {Le Nguyen, Thach and Gsponer, Severin and Ilie, Iulia and O'Reilly, Martin and Ifrim, Georgiana},
  year = {2019},
  month = jul,
  journal = {Data Mining and Knowledge Discovery},
  volume = {33},
  number = {4},
  pages = {1183--1222},
  issn = {1573-756X},
  doi = {10.1007/s10618-019-00633-3},
  urldate = {2019-06-19},
  abstract = {The time series classification literature has expanded rapidly over the last decade, with many new classification approaches published each year. Prior research has mostly focused on improving the accuracy and efficiency of classifiers, with interpretability being somewhat neglected. This aspect of classifiers has become critical for many application domains and the introduction of the EU GDPR legislation in 2018 is likely to further emphasize the importance of interpretable learning algorithms. Currently, state-of-the-art classification accuracy is achieved with very complex models based on large ensembles (COTE) or deep neural networks (FCN). These approaches are not efficient with regard to either time or space, are difficult to interpret and cannot be applied to variable-length time series, requiring pre-processing of the original series to a set fixed-length. In this paper we propose new time series classification algorithms to address these gaps. Our approach is based on symbolic representations of time series, efficient sequence mining algorithms and linear classification models. Our linear models are as accurate as deep learning models but are more efficient regarding running time and memory, can work with variable-length time series and can be interpreted by highlighting the discriminative symbolic features on the original time series. We advance the state-of-the-art in time series classification by proposing new algorithms built using the following three key ideas: (1) Multiple resolutions of symbolic representations: we combine symbolic representations obtained using different parameters, rather than one fixed representation (e.g., multiple SAX representations); (2) Multiple domain representations: we combine symbolic representations in time (e.g., SAX) and frequency (e.g., SFA) domains, to be more robust across problem types; (3) Efficient navigation in a huge symbolic-words space: we extend a symbolic sequence classifier (SEQL) to work with multiple symbolic representations and use its greedy feature selection strategy to effectively filter the best features for each representation. We show that our multi-resolution multi-domain linear classifier (mtSS-SEQL+LR) achieves a similar accuracy to the state-of-the-art COTE ensemble, and to recent deep learning methods (FCN, ResNet), but uses a fraction of the time and memory required by either COTE or deep models. To further analyse the interpretability of our classifier, we present a case study on a human motion dataset collected by the authors. We discuss the accuracy, efficiency and interpretability of our proposed algorithms and release all the results, source code and data to encourage reproducibility.},
  langid = {english},
  keywords = {Interpretable classifier,Linear models,Multi-resolution multi-domain symbolic representations,SAX,SEQL,SFA,Time series classification},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Le Nguyen et al. - 2019 - Interpretable time series classification using lin.pdf}
}

@article{li2018,
  title = {Structural {{Capacitance}} in {{Protein Evolution}} and {{Human Diseases}}},
  author = {Li, Chen and Clark, Liah V.T. and Zhang, Rory and Porebski, Benjamin T. and McCoey, Julia M. and Borg, Natalie A. and Webb, Geoffrey I. and Kass, Itamar and Buckle, Malcolm and Song, Jiangning and Woolfson, Adrian and Buckle, Ashley M.},
  year = {2018},
  month = sep,
  journal = {Journal of Molecular Biology},
  volume = {430},
  number = {18},
  pages = {3200--3217},
  issn = {00222836},
  doi = {10.1016/j.jmb.2018.06.051},
  urldate = {2018-12-15},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Li et al. - 2018 - Structural Capacitance in Protein Evolution and Hu.pdf}
}

@article{lin1991,
  title = {Divergence Measures Based on the {{Shannon}} Entropy},
  author = {Lin, J.},
  year = {1991},
  month = jan,
  journal = {IEEE Transactions on Information Theory},
  volume = {37},
  number = {1},
  pages = {145--151},
  issn = {1557-9654},
  doi = {10.1109/18.61115},
  abstract = {A novel class of information-theoretic divergence measures based on the Shannon entropy is introduced. Unlike the well-known Kullback divergences, the new measures do not require the condition of absolute continuity to be satisfied by the probability distributions involved. More importantly, their close relationship with the variational distance and the probability of misclassification error are established in terms of bounds. These bounds are crucial in many applications of divergence measures. The measures are also well characterized by the properties of nonnegativity, finiteness, semiboundedness, and boundedness.{$<>$}},
  keywords = {Computer science,Entropy,Genetics,Pattern analysis,Pattern recognition,Probability distribution,Signal analysis,Signal processing,Taxonomy,Upper bound},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\B5VUAPPD\\Lin - 1991 - Divergence measures based on the Shannon entropy.pdf;D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Lin - 1991 - Divergence measures based on the Shannon entropy.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\TP2XYMA4\\61115.html}
}

@book{lincoff1981,
  title = {The {{Audubon Society Field Guide}} to {{North American Mushrooms}}},
  author = {Lincoff, Gary},
  year = {1981},
  publisher = {{Knopf}},
  abstract = {With more than 700 mushrooms detailed with color photographs and descriptive text, this is the most comprehensive photographic field guide to the mushrooms of North America. The 762 full-color identification photographs show the mushrooms as they appear in natural habitats. Organized visually, the book groups all mushrooms by color and shape to make identification simple and accurate in the field, while the text account for each species includes a detailed physical description, information on edibility, season, habitat, range, look-alikes, alternative names, and facts on edible and poisonous species, uses, and folklore. A supplementary section on cooking and eating wild mushrooms, and illustrations identifying the parts of a mushroom, round out this essential guide.},
  googlebooks = {bf8UAQAAIAAJ},
  isbn = {978-0-394-51992-0},
  langid = {english},
  keywords = {Nature / Plants / Mushrooms}
}

@article{lindstrom2008,
  title = {Autopilot: {{Simulating Changing Concepts}} in {{Real Data}}},
  shorttitle = {Autopilot},
  author = {Lindstrom, Patrick and Delany, Sarah Jane and Namee, Brian Mac},
  year = {2008},
  month = jan,
  journal = {Conference papers},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Lindstrom et al. - 2008 - Autopilot Simulating Changing Concepts in Real Da.pdf}
}

@inproceedings{lindstrom2011,
  title = {Drift {{Detection Using Uncertainty Distribution Divergence}}},
  booktitle = {2011 {{IEEE}} 11th {{International Conference}} on {{Data Mining Workshops}}},
  author = {Lindstrom, Patrick and Mac Namee, Brian and Delany, Sarah Jane},
  year = {2011},
  month = dec,
  pages = {604--608},
  issn = {2375-9259},
  doi = {10.1109/ICDMW.2011.70},
  abstract = {Concept drift is believed to be prevalent in most data gathered from naturally occurring processes and thus warrants research by the machine learning community. There are a myriad of approaches to concept drift handling which have been shown to handle concept drift with varying degrees of success. However, most approaches make the key assumption that the labelled data will be available at no labelling cost shortly after classification, an assumption which is often violated. The high labelling cost in many domains provides a strong motivation to reduce the number of labelled instances required to handle concept drift. Explicit detection approaches that do not require labelled instances to detect concept drift show great promise for achieving this. Our approach Confidence Distribution Batch Detection (CDBD) provides a signal correlated to changes in concept without using labelled data. We also show how this signal combined with a trigger and a rebuild policy can maintain classifier accuracy while using a limited amount of labelled data.},
  keywords = {Accuracy,classifier confidence,concept drift,Conferences,Data mining,explicit drift detection,Labeling,labelling cost,Machine learning,Training,Training data},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Lindstrom et al. - 2011 - Drift Detection Using Uncertainty Distribution Div.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\HA684KXF\\6137435.html}
}

@inproceedings{liu2017,
  title = {Regional Concept Drift Detection and Density Synchronized Drift Adaptation},
  booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, {{IJCAI-17}}},
  author = {Liu, Anjin and Song, Yiliao and Zhang, Guangquan and Lu, Jie},
  year = {2017},
  pages = {2280--2286},
  doi = {10.24963/ijcai.2017/317},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Liu et al. - 2017 - Regional Concept Drift Detection and Density Synch2.pdf}
}

@article{lokhov2018,
  title = {Optimal Structure and Parameter Learning of {{Ising}} Models},
  author = {Lokhov, Andrey Y. and Vuffray, Marc and Misra, Sidhant and Chertkov, Michael},
  year = {2018},
  month = mar,
  journal = {Science Advances},
  volume = {4},
  number = {3},
  pages = {e1700791},
  publisher = {{American Association for the Advancement of Science}},
  issn = {2375-2548},
  doi = {10.1126/sciadv.1700791},
  urldate = {2021-08-10},
  abstract = {Reconstruction of the structure and parameters of an Ising model from binary samples is a problem of practical importance in a variety of disciplines, ranging from statistical physics and computational biology to image processing and machine learning. The focus of the research community shifted toward developing universal reconstruction algorithms that are both computationally efficient and require the minimal amount of expensive data. We introduce a new method, interaction screening, which accurately estimates model parameters using local optimization problems. The algorithm provably achieves perfect graph structure recovery with an information-theoretically optimal number of samples, notably in the low-temperature regime, which is known to be the hardest for learning. The efficacy of interaction screening is assessed through extensive numerical tests on synthetic Ising models of various topologies with different types of interactions, as well as on real data produced by a D-Wave quantum computer. This study shows that the interaction screening method is an exact, tractable, and optimal technique that universally solves the inverse Ising problem. An arbitrary Ising model can be exactly recovered from observations using an information-theoretically optimal amount of data. An arbitrary Ising model can be exactly recovered from observations using an information-theoretically optimal amount of data.},
  chapter = {Research Article},
  copyright = {Copyright \textcopyright{} 2018 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution NonCommercial License 4.0 (CC BY-NC).. This is an open-access article distributed under the terms of the Creative Commons Attribution-NonCommercial license, which permits use, distribution, and reproduction in any medium, so long as the resultant use is not for commercial advantage and provided the original work is properly cited.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Lokhov et al. - 2018 - Optimal structure and parameter learning of Ising .pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\P33NPUKJ\\e1700791.html}
}

@article{lubba2019,
  title = {Catch22: {{CAnonical Time-series CHaracteristics}}},
  shorttitle = {Catch22},
  author = {Lubba, Carl H. and Sethi, Sarab S. and Knaute, Philip and Schultz, Simon R. and Fulcher, Ben D. and Jones, Nick S.},
  year = {2019},
  month = jan,
  urldate = {2019-10-23},
  abstract = {Capturing the dynamical properties of time series concisely as interpretable feature vectors can enable efficient clustering and classification for time-series applications across science and industry. Selecting an appropriate feature-based representation of time series for a given application can be achieved through systematic comparison across a comprehensive time-series feature library, such as those in the hctsa toolbox. However, this approach is computationally expensive and involves evaluating many similar features, limiting the widespread adoption of feature-based representations of time series for real-world applications. In this work, we introduce a method to infer small sets of time-series features that (i) exhibit strong classification performance across a given collection of time-series problems, and (ii) are minimally redundant. Applying our method to a set of 93 time-series classification datasets (containing over 147000 time series) and using a filtered version of the hctsa feature library (4791 features), we introduce a generically useful set of 22 CAnonical Time-series CHaracteristics, catch22. This dimensionality reduction, from 4791 to 22, is associated with an approximately 1000-fold reduction in computation time and near linear scaling with time-series length, despite an average reduction in classification accuracy of just 7\%. catch22 captures a diverse and interpretable signature of time series in terms of their properties, including linear and non-linear autocorrelation, successive differences, value distributions and outliers, and fluctuation scaling properties. We provide an efficient implementation of catch22, accessible from many programming environments, that facilitates feature-based time-series analysis for scientific, industrial, financial and medical applications using a common language of interpretable time-series properties.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Lubba et al. - 2019 - catch22 CAnonical Time-series CHaracteristics.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\6GWKYWMV\\1901.html}
}

@article{ma,
  title = {Joint {{Structural Estimation}} of {{Multiple Graphical Models}}},
  author = {Ma, Jing and Michailidis, George},
  pages = {48},
  abstract = {Gaussian graphical models capture dependence relationships between random variables through the pattern of nonzero elements in the corresponding inverse covariance matrices. To date, there has been a large body of literature on both computational methods and analytical results on the estimation of a single graphical model. However, in many application domains, one has to estimate several related graphical models, a problem that has also received attention in the literature. The available approaches usually assume that all graphical models are globally related. On the other hand, in many settings different relationships between subsets of the node sets exist between different graphical models. We develop methodology that jointly estimates multiple Gaussian graphical models, assuming that there exists prior information on how they are structurally related. For many applications, such information is available from external data sources. The proposed method consists of first applying neighborhood selection with a group lasso penalty to obtain edge sets of the graphs, and a maximum likelihood refit for estimating the nonzero entries in the inverse covariance matrices. We establish consistency of the proposed method for sparse high-dimensional Gaussian graphical models and examine its performance using simulation experiments. Applications to a climate data set and a breast cancer data set are also discussed.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Ma and Michailidis - Joint Structural Estimation of Multiple Graphical .pdf}
}

@inproceedings{ma2018,
  title = {Robust and {{Rapid Adaption}} for {{Concept Drift}} in {{Software System Anomaly Detection}}},
  booktitle = {2018 {{IEEE}} 29th {{International Symposium}} on {{Software Reliability Engineering}} ({{ISSRE}})},
  author = {Ma, Minghua and Zhang, Shenglin and Pei, Dan and Huang, Xin and Dai, Hongwei},
  year = {2018},
  month = oct,
  pages = {13--24},
  issn = {2332-6549},
  doi = {10.1109/ISSRE.2018.00013},
  abstract = {Anomaly detection is critical for web-based software systems. Anecdotal evidence suggests that in these systems, the accuracy of a static anomaly detection method that was previously ensured is bound to degrade over time. It is due to the significant change of data distribution, namely concept drift, which is caused by software change or personal preferences evolving. Even though dozens of anomaly detectors have been proposed over the years in the context of software system, they have not tackled the problem of concept drift. In this paper, we present a framework, StepWise, which can detect concept drift without tuning detection threshold or per-KPI (Key Performance Indicator) model parameters in a large scale KPI streams, take external factors into account to distinguish the concept drift which under operators' expectations, and help any kind of anomaly detection algorithm to handle it rapidly. For the prototype deployed in Sogou, our empirical evaluation shows StepWise improve the average F-score by 206\% for many widely-used anomaly detectors over a baseline without any concept drift detection.},
  keywords = {Anomaly detection,Concept drift,Detectors,Monitoring,Servers,Software service KPI,Software systems,Web-based software system},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Ma et al. - 2018 - Robust and Rapid Adaption for Concept Drift in Sof.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\HPLCZJY4\\8539065.html}
}

@article{ma2021,
  ids = {zotero-677},
  title = {Deep Graph Similarity Learning: A Survey},
  shorttitle = {Deep Graph Similarity Learning},
  author = {Ma, Guixiang and Ahmed, Nesreen K. and Willke, Theodore L. and Yu, Philip S.},
  year = {2021},
  month = may,
  journal = {Data Mining and Knowledge Discovery},
  volume = {35},
  number = {3},
  pages = {688--725},
  issn = {1573-756X},
  doi = {10.1007/s10618-020-00733-5},
  urldate = {2021-08-07},
  abstract = {In many domains where data are represented as graphs, learning a similarity metric among graphs is considered a key problem, which can further facilitate various learning tasks, such as classification, clustering, and similarity search. Recently, there has been an increasing interest in deep graph similarity learning, where the key idea is to learn a deep learning model that maps input graphs to a target space such that the distance in the target space approximates the structural distance in the input space. Here, we provide a comprehensive review of the existing literature of deep graph similarity learning. We propose a systematic taxonomy for the methods and applications. Finally, we discuss the challenges and future directions for this problem.},
  langid = {english},
  keywords = {Researcher App},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Ma et al. - 2021 - Deep graph similarity learning a survey.pdf}
}

@article{maaten2008,
  title = {Visualizing {{Data}} Using T-{{SNE}}},
  author = {van der Maaten, Laurens and Hinton, Geoffrey},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  number = {86},
  pages = {2579--2605},
  issn = {1533-7928},
  urldate = {2021-02-24},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Maaten and Hinton - 2008 - Visualizing Data using t-SNE.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\RUBVVBQX\\vandermaaten08a.html}
}

@book{mackay2003,
  title = {Information {{Theory}}, {{Inference}} and {{Learning Algorithms}}},
  author = {MacKay, David J. C.},
  year = {2003},
  month = sep,
  edition = {1st edition},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, UK ; New York}},
  isbn = {978-0-521-64298-9},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\MacKay - 2003 - Information Theory, Inference and Learning Algorit.pdf}
}

@article{mahmoud2015,
  title = {Modelling Self-Optimised Short Term Load Forecasting for Medium Voltage Loads Using Tunning Fuzzy Systems and {{Artificial Neural Networks}}},
  author = {Mahmoud, Thair S. and Habibi, Daryoush and Hassan, Mohammed Y. and Bass, Octavian},
  year = {2015},
  month = dec,
  journal = {Energy Conversion and Management},
  volume = {106},
  pages = {1396--1408},
  issn = {0196-8904},
  doi = {10.1016/j.enconman.2015.10.066},
  urldate = {2021-03-30},
  abstract = {This paper presents an intelligent mechanism for Short Term Load Forecasting (STLF) models, which allows self-adaptation with respect to the load operational conditions. Specifically, a knowledge-based FeedBack Tunning Fuzzy System (FBTFS) is proposed to instantaneously correlate the information about the demand profile and its operational conditions to make decisions for controlling the model's forecasting error rate. To maintain minimum forecasting error under various operational scenarios, the FBTFS adaptation was optimised using a Multi-Layer Perceptron Artificial Neural Network (MLPANN), which was trained using Backpropagation algorithm, based on the information about the amount of error and the operational conditions at time of forecasting. For the sake of comparison and performance testing, this mechanism was added to the conventional forecasting methods, i.e. Nonlinear AutoRegressive eXogenous-Artificial Neural Network (NARXANN), Fuzzy Subtractive Clustering Method-based Adaptive Neuro Fuzzy Inference System (FSCMANFIS) and Gaussian-kernel Support Vector Machine (GSVM), and the measured forecasting error reduction average in a 12month simulation period was 7.83\%, 8.5\% and 8.32\% respectively. The 3.5MW variable load profile of Edith Cowan University (ECU) in Joondalup, Australia, was used in the modelling and simulations of this model, and the data was provided by Western Power, the transmission and distribution company of the state of Western Australia.},
  langid = {english},
  keywords = {\_tablet,Adaptive model,Artificial Neural Networks,Fuzzy Logic,Generation scheduling,Optimum tuning,Short term forecasting},
  file = {C\:\\Users\\LoongKuan\\Dropbox\\academic papers\\paper library\\Mahmoud et al\\Mahmoud et al. - 2015 - Modelling self-optimised short term load forecasti.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\NPD73KQA\\S0196890415009887.html}
}

@article{malvestuto1991,
  title = {Approximating Discrete Probability Distributions with Decomposable Models},
  author = {Malvestuto, F. M.},
  year = {1991},
  month = sep,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  volume = {21},
  number = {5},
  pages = {1287--1294},
  issn = {0018-9472},
  doi = {10.1109/21.120082},
  abstract = {A heuristic procedure is presented for approximating an n-dimensional discrete probability distribution with a decomposable model of a given complexity. It is shown that, without loss of generality, the search space can be restricted to a suitable subclass of decomposable models, whose members are called elementary models. The selected elementary model is constructed in an incremental manner according to a local-optimality criterion that consists of minimizing a suitable cost function. It is shown by an example that the solution computed by the procedure is sometimes optimal.{$<>$}},
  keywords = {approximation theory,Artificial intelligence,computational complexity,Cost function,Cybernetics,decomposable models,discrete probability distribution approximation,elementary models,Feature extraction,heuristic,Information systems,local-optimality criterion,optimisation,Pattern recognition,probability,Probability distribution,Random variables,search space,set theory,statistical analysis,Stochastic processes,Stress},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Malvestuto - 1991 - Approximating discrete probability distributions w.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\ZAVIJ2QM\\120082.html}
}

@inproceedings{manapragada2018,
  title = {Extremely {{Fast Decision Tree}}},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}  - {{KDD}} '18},
  author = {Manapragada, Chaitanya and Webb, Geoffrey I. and Salehi, Mahsa},
  year = {2018},
  pages = {1953--1962},
  publisher = {{ACM Press}},
  address = {{London, United Kingdom}},
  doi = {10.1145/3219819.3220005},
  urldate = {2018-12-15},
  isbn = {978-1-4503-5552-0},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Manapragada et al. - 2018 - Extremely Fast Decision Tree.pdf}
}

@inproceedings{maniu2019,
  title = {An {{Experimental Study}} of the {{Treewidth}} of {{Real-World Graph Data}}},
  booktitle = {22nd {{International Conference}} on {{Database Theory}} ({{ICDT}} 2019)},
  author = {Maniu, Silviu and Senellart, Pierre and Jog, Suraj},
  editor = {Barcelo, Pablo and Calautti, Marco},
  year = {2019},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  volume = {127},
  pages = {12:1--12:18},
  publisher = {{Schloss Dagstuhl\textendash Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  issn = {1868-8969},
  doi = {10.4230/LIPIcs.ICDT.2019.12},
  urldate = {2023-02-02},
  isbn = {978-3-95977-101-6},
  keywords = {Experiments,Graph decompositions,Query processing,Treewidth},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Maniu et al. - 2019 - An Experimental Study of the Treewidth of Real-Wor2.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\4SAG4N8D\\10314.html}
}

@article{mezzini2010,
  title = {Simple Algorithms for Minimal Triangulation of a Graph and Backward Selection of a Decomposable {{Markov}} Network},
  author = {Mezzini, Mauro and Moscarini, Marina},
  year = {2010},
  month = feb,
  journal = {Theoretical Computer Science},
  volume = {411},
  number = {7},
  pages = {958--966},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2009.10.004},
  urldate = {2019-11-07},
  abstract = {In this paper we propose a simple algorithm called CliqueMinTriang for computing a minimal triangulation of a graph. If F is the set of edges that is added to G to make it a complete graph Kn then the asymptotic complexity of CliqueMinTriang is O(|F|({$\delta$}2+|F|)) where {$\delta$} is the degree of the subgraph of Kn induced by F. Therefore our algorithm performs well when G is a dense graph. We also show how to exploit the existing minimal triangulation techniques in conjunction with CliqueMinTriang to efficiently find a minimal triangulation of nondense graphs. Finally we show how the algorithm can be adapted to perform a backward stepwise selection of decomposable Markov networks; the resulting procedure has the same time complexity as that of existing similar algorithms.},
  langid = {english},
  keywords = {Chordal graphs,Learning decomposable models,Markov networks,Minimal triangulations},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Mezzini and Moscarini - 2010 - Simple algorithms for minimal triangulation of a g.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\G6MPTEI5\\S030439750900735X.html}
}

@article{mezzini2012,
  title = {Fully Dynamic Algorithm for Chordal Graphs with {{O}}(1) Query-Time and {{O}}(N2) Update-Time},
  author = {Mezzini, Mauro},
  year = {2012},
  month = aug,
  journal = {Theoretical Computer Science},
  volume = {445},
  pages = {82--92},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2012.05.002},
  urldate = {2019-11-05},
  abstract = {We propose dynamic algorithms and data structures for chordal graphs supporting the following operation: determine if an edge can be added or removed from the graph while preserving the chordality in O(1) time. We show that the complexity of the algorithms for updating the data structures when an edge is actually inserted or deleted is O(n2) where n is the number of vertices of the graph.},
  langid = {english},
  keywords = {Chordal graphs,Dynamic algorithms,Minimal Triangulations},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Mezzini - 2012 - Fully dynamic algorithm for chordal graphs with O(.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\MCGRE3P9\\S0304397512004239.html}
}

@mastersthesis{micevska2019,
  title = {A {{Statistical Drift Detection Method}}},
  author = {Micevska, Simona},
  year = {2019},
  month = may,
  address = {{Tartu, Estonia}},
  urldate = {2020-02-16},
  abstract = {Machine learning models assume that data is drawn from a stationary distribution. However, in practice, challenges are imposed on models that need to make sense of fast-evolving data streams, where the content of data is changing and evolving dynamically over time. This change between the underlying distributions of the training and test datasets is called concept drift. The presence of concept drift may compromise the accuracy and reliability of prospective computational predictions. Therefore, handling concept drift is of great importance in the direction of diminishing its negative effects on a model's performance. In order to handle concept drift, one has to detect it first. Concept drift detectors have been used to accomplish this - reactive concept drift detectors try to detect drift as soon as it occurs by monitoring the performance of the underlying machine learning model. However, the importance of interpretability in machine learning indicates that it may prove useful to not only detect that drift is occurring in the data, but to also identify and analyze the causes of the drift. In this thesis, the importance of interpretability in drift detection is highlighted and the Statistical Drift Detection Method (SDDM) is presented, which detects drifts in fast-evolving data streams with a smaller number of false positives and false negatives when compared to the state-of-the-art, and has the ability to interpret the cause of the concept drift. The effectiveness of the method is demonstrated by applying it on both synthetic and real-world datasets.},
  langid = {english},
  school = {University of Tartu},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Micevska - 2019 - A Statistical Drift Detection Method.pdf}
}

@article{mierswa2005,
  title = {Automatic {{Feature Extraction}} for {{Classifying Audio Data}}},
  author = {Mierswa, Ingo and Morik, Katharina},
  year = {2005},
  month = feb,
  journal = {Machine Learning},
  volume = {58},
  number = {2-3},
  pages = {127--149},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-005-5824-7},
  urldate = {2019-11-20},
  abstract = {Today, many private households as well as broadcasting or film companies own large collections of digital music plays. These are time series that differ from, e.g., weather reports or stocks market data. The task is normally that of classification, not prediction of the next value or recognizing a shape or motif. New methods for extracting features that allow to classify audio data have been developed. However, the development of appropriate feature extraction methods is a tedious effort, particularly because every new classification task requires tailoring the feature set anew.},
  langid = {english},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\NVDUFQCG\\Mierswa and Morik - 2005 - Automatic Feature Extraction for Classifying Audio.pdf}
}

@article{minka2005,
  title = {Divergence {{Measures}} and {{Message Passing}}},
  author = {Minka, Tom},
  year = {2005},
  month = jan,
  urldate = {2022-01-30},
  abstract = {This paper presents a unifying view of message-passing algorithms, as methods to approximate a complex Bayesian network by a simpler network with minimum information divergence. In this view, the difference between mean-field methods and belief propagation is not the amount of structure they model, but only the measure of loss they minimize (`exclusive' versus `inclusive' [\ldots ]},
  langid = {american},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Minka - 2005 - Divergence Measures and Message Passing.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\G2ST9PEC\\divergence-measures-and-message-passing.html}
}

@article{minku2010,
  title = {The {{Impact}} of {{Diversity}} on {{Online Ensemble Learning}} in the {{Presence}} of {{Concept Drift}}},
  author = {Minku, Leandro L. and White, Allan P. and Yao, Xin},
  year = {2010},
  month = may,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {22},
  number = {5},
  pages = {730--742},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2009.156},
  abstract = {Online learning algorithms often have to operate in the presence of concept drift (i.e., the concepts to be learned can change with time). This paper presents a new categorization for concept drift, separating drifts according to different criteria into mutually exclusive and nonheterogeneous categories. Moreover, although ensembles of learning machines have been used to learn in the presence of concept drift, there has been no deep study of why they can be helpful for that and which of their features can contribute or not for that. As diversity is one of these features, we present a diversity analysis in the presence of different types of drifts. We show that, before the drift, ensembles with less diversity obtain lower test errors. On the other hand, it is a good strategy to maintain highly diverse ensembles to obtain lower test errors shortly after the drift independent on the type of drift, even though high diversity is more important for more severe drifts. Longer after the drift, high diversity becomes less important. Diversity by itself can help to reduce the initial increase in error caused by a drift, but does not provide the faster recovery from drifts in long-term.},
  keywords = {concept drift,Concept drift,diversity analysis,diversity.,exclusive category,learning (artificial intelligence),learning machines,neural nets,neural network ensembles,nonheterogeneous category,online ensemble learning,online learning},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Minku et al. - 2010 - The Impact of Diversity on Online Ensemble Learnin.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\IV5ZU77A\\5156502.html}
}

@misc{moa2011,
  title = {{{MOA}} Dataset Repository},
  author = {{MOA}},
  year = {2011},
  month = aug,
  journal = {MOA dataset repository},
  urldate = {2018-05-30},
  abstract = {Forest Covertype Contains the forest cover type for 30 x 30 meter cells obtained from US Forest Service (USFS) Region 2 Resource Information System (RIS) data. It contains 581, 012 instances and 54\ldots},
  howpublished = {https://moa.cms.waikato.ac.nz/datasets/},
  langid = {american},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\TWJDZXRE\\datasets.html}
}

@article{moral2021,
  title = {Computation of {{Kullback}}\textendash{{Leibler Divergence}} in {{Bayesian Networks}}},
  author = {Moral, Seraf{\'i}n and Cano, Andr{\'e}s and {G{\'o}mez-Olmedo}, Manuel},
  year = {2021},
  month = sep,
  journal = {Entropy},
  volume = {23},
  number = {9},
  pages = {1122},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/e23091122},
  urldate = {2022-01-12},
  abstract = {Kullback\textendash Leibler divergence KL(p,q) is the standard measure of error when we have a true probability distribution p which is approximate with probability distribution q. Its efficient computation is essential in many tasks, as in approximate computation or as a measure of error when learning a probability. In high dimensional probabilities, as the ones associated with Bayesian networks, a direct computation can be unfeasible. This paper considers the case of efficiently computing the Kullback\textendash Leibler divergence of two probability distributions, each one of them coming from a different Bayesian network, which might have different structures. The paper is based on an auxiliary deletion algorithm to compute the necessary marginal distributions, but using a cache of operations with potentials in order to reuse past computations whenever they are necessary. The algorithms are tested with Bayesian networks from the bnlearn repository. Computer code in Python is provided taking as basis pgmpy, a library for working with probabilistic graphical models.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Kullback\textendash Leibler divergence,learning algorithms,probabilistic graphical models},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Moral et al. - 2021 - Computation of Kullback–Leibler Divergence in Baye.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\U42QF2KI\\1122.html}
}

@article{morimoto1963,
  title = {Markov {{Processes}} and the {{H-Theorem}}},
  author = {Morimoto, Tetsuzo},
  year = {1963},
  month = mar,
  journal = {Journal of the Physical Society of Japan},
  volume = {18},
  number = {3},
  pages = {328--331},
  publisher = {{The Physical Society of Japan}},
  issn = {0031-9015},
  doi = {10.1143/JPSJ.18.328},
  urldate = {2023-03-30},
  abstract = {The H -theorem is investigated in view of Markov processes. The proof is valid even in the fields other than physics, since none of physical relations, such as the principle of microscopic reversibility, the unitarity of S -matrix and so on, is utilized. The well-known ` Wiederkehreinwand ' is also re-examined from this standpoint of view and a reconciliation is regained between the H -theorem and the recurrence in the sense of probability theory.},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Morimoto - 1963 - Markov Processes and the H-Theorem.pdf}
}

@article{moro2014,
  title = {A Data-Driven Approach to Predict the Success of Bank Telemarketing},
  author = {Moro, S{\'e}rgio and Cortez, Paulo and Rita, Paulo},
  year = {2014},
  month = jun,
  journal = {Decision Support Systems},
  volume = {62},
  pages = {22--31},
  issn = {01679236},
  doi = {10.1016/j.dss.2014.03.001},
  urldate = {2018-11-08},
  abstract = {We propose a data mining (DM) approach to predict the success of telemarketing calls for selling bank long-term deposits. A Portuguese retail bank was addressed, with data collected from 2008 to 2013, thus including the effects of the recent financial crisis. We analyzed a large set of 150 features related with bank client, product and social-economic attributes. A semi-automatic feature selection was explored in the modeling phase, performed with the data prior to July 2012 and that allowed to select a reduced set of 22 features. We also compared four DM models: logistic regression, decision trees (DTs), neural network (NN) and support vector machine. Using two metrics, area of the receiver operating characteristic curve (AUC) and area of the LIFT cumulative curve (ALIFT), the four models were tested on an evaluation set, using the most recent data (after July 2012) and a rolling window scheme. The NN presented the best results (AUC = 0.8 and ALIFT = 0.7), allowing to reach 79\% of the subscribers by selecting the half better classified clients. Also, two knowledge extraction methods, a sensitivity analysis and a DT, were applied to the NN model and revealed several key attributes (e.g., Euribor rate, direction of the call and bank agent experience). Such knowledge extraction confirmed the obtained model as credible and valuable for telemarketing campaign managers.},
  langid = {english},
  keywords = {bank,dataset},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Moro et al. - 2014 - A data-driven approach to predict the success of b.pdf}
}

@article{munoz2018,
  title = {Instance Spaces for Machine Learning Classification},
  author = {Mu{\~n}oz, Mario A. and Villanova, Laura and Baatar, Davaatseren and {Smith-Miles}, Kate},
  year = {2018},
  month = jan,
  journal = {Machine Learning},
  volume = {107},
  number = {1},
  pages = {109--147},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-017-5629-5},
  urldate = {2018-06-25},
  abstract = {This paper tackles the issue of objective performance evaluation of machine learning classifiers, and the impact of the choice of test instances. Given that statistical properties or features of a dataset affect the difficulty of an instance for particular classification algorithms, we examine the diversity and quality of the UCI repository of test instances used by most machine learning researchers. We show how an instance space can be visualized, with each classification dataset represented as a point in the space. The instance space is constructed to reveal pockets of hard and easy instances, and enables the strengths and weaknesses of individual classifiers to be identified. Finally, we propose a methodology to generate new test instances with the aim of enriching the diversity of the instance space, enabling potentially greater insights than can be afforded by the current UCI repository.},
  langid = {english},
  file = {D\:\\work\\literature\\library\\Muñoz et al. - 2018 - Instance spaces for machine learning classificatio.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\PHF9JNB5\\s10994-017-5629-5.html}
}

@article{nachman2020,
  title = {Unfolding Quantum Computer Readout Noise},
  author = {Nachman, Benjamin and Urbanek, Miroslav and {de Jong}, Wibe A. and Bauer, Christian W.},
  year = {2020},
  month = sep,
  journal = {npj Quantum Information},
  volume = {6},
  number = {1},
  pages = {1--7},
  publisher = {{Nature Publishing Group}},
  issn = {2056-6387},
  doi = {10.1038/s41534-020-00309-7},
  urldate = {2023-07-09},
  abstract = {In the current era of noisy intermediate-scale quantum computers, noisy qubits can result in biased results for early quantum algorithm applications. This is a significant challenge for interpreting results from quantum computer simulations for quantum chemistry, nuclear physics, high energy physics (HEP), and other emerging scientific applications. An important class of qubit errors are readout errors. The most basic method to correct readout errors is matrix inversion, using a response matrix built from simple operations to probe the rate of transitions from known initial quantum states to readout outcomes. One challenge with inverting matrices with large off-diagonal components is that the results are sensitive to statistical fluctuations. This challenge is familiar to HEP, where prior-independent regularized matrix inversion techniques (``unfolding'') have been developed for years to correct for acceptance and detector effects, when performing differential cross section measurements. We study one such method, known as iterative Bayesian unfolding, as a potential tool for correcting readout errors from universal gate-based quantum computers. This method is shown to avoid pathologies from commonly used matrix inversion and least squares methods.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Quantum chemistry,Quantum information},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Nachman et al. - 2020 - Unfolding quantum computer readout noise.pdf}
}

@inproceedings{narasimhamurthy2007,
  title = {A {{Framework}} for {{Generating Data}} to {{Simulate Changing Environments}}},
  booktitle = {Proceedings of the 25th {{Conference}} on {{Proceedings}} of the 25th {{IASTED International Multi-Conference}}: {{Artificial Intelligence}} and {{Applications}}},
  author = {Narasimhamurthy, Anand and Kuncheva, Ludmila I.},
  year = {2007},
  series = {{{AIAP}}'07},
  pages = {384--389},
  publisher = {{ACTA Press}},
  address = {{Anaheim, CA, USA}},
  urldate = {2018-03-21},
  abstract = {A fundamental assumption often made in supervised classification is that the problem is static, i.e. the description of the classes does not change with time. However many practical classification tasks involve changing environments. Thus designing and testing classifiers for changing environments are of increasing interest and importance. A number of benchmark data sets are available for static classification tasks. For example, the UCI machine learning repository is extensively used by researchers to compare algorithms across various domains. No such benchmark datasets are available for changing environments. Also, while generating data for static environments is relatively straightforward, this is not so for changing environments. The reason is that an infinite amount of changes can be simulated, and it is difficult to define which ones will be realistic and hence useful. In this paper we propose a general framework for generating data to simulate changing environments. The paper gives illustrations of how the framework encompasses various types of changes observed in real data and also how the two most popular simulation models (STAGGER and moving hyperplane) are represented within.},
  keywords = {artificial data,changing environments,concept drift,machine learning,population drift,simulated data},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Narasimhamurthy and Kuncheva - 2007 - A Framework for Generating Data to Simulate Changi.pdf}
}

@article{nilsson1998,
  title = {An Efficient Algorithm for Finding the {{M}} Most Probable Configurationsin Probabilistic Expert Systems},
  author = {Nilsson, D.},
  year = {1998},
  month = jun,
  journal = {Statistics and Computing},
  volume = {8},
  number = {2},
  pages = {159--173},
  issn = {1573-1375},
  doi = {10.1023/A:1008990218483},
  urldate = {2021-09-26},
  abstract = {A probabilistic expert system provides a graphical representation of a joint probability distribution which enables local computations of probabilities. Dawid (1992) provided a `flow- propagation' algorithm for finding the most probable configuration of the joint distribution in such a system. This paper analyses that algorithm in detail, and shows how it can be combined with a clever partitioning scheme to formulate an efficient method for finding the M most probable configurations. The algorithm is a divide and conquer technique, that iteratively identifies the M most probable configurations.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Nilsson - 1998 - An efficient algorithm for finding the M most prob2.pdf}
}

@article{novak2009,
  title = {Supervised Descriptive Rule Discovery: {{A}} Unifying Survey of Contrast Set, Emerging Pattern and Subgroup Mining},
  shorttitle = {Supervised Descriptive Rule Discovery},
  author = {Novak, Petra Kralj and Lavra{\v c}, Nada and Webb, Geoffrey I.},
  year = {2009},
  journal = {Journal of Machine Learning Research},
  volume = {10},
  number = {Feb},
  pages = {377--403},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Novak et al. - 2009 - Supervised descriptive rule discovery A unifying .pdf}
}

@inproceedings{oliveira2017,
  title = {Time {{Series Forecasting}} in the {{Presence}} of {{Concept Drift}}: {{A PSO-based Approach}}},
  shorttitle = {Time {{Series Forecasting}} in the {{Presence}} of {{Concept Drift}}},
  booktitle = {2017 {{IEEE}} 29th {{International Conference}} on {{Tools}} with {{Artificial Intelligence}} ({{ICTAI}})},
  author = {Oliveira, G. H. F. M. and Cavalcante, R. C. and Cabral, G. G. and Minku, L. L. and Oliveira, A. L. I.},
  year = {2017},
  month = nov,
  pages = {239--246},
  issn = {2375-0197},
  doi = {10.1109/ICTAI.2017.00046},
  abstract = {Time series forecasting is a problem with many applications. However, in many domains, such as stock market, the underlying generating process of the time series observations may change, making forecasting models obsolete. This problem is known as Concept Drift. Approaches for time series forecasting should be able to detect and react to concept drift in a timely manner, so that the forecasting model can be updated as soon as possible. Despite the fact that the concept drift problem is well investigated in the literature, little effort has been made to solve this problem for time series forecasting so far. This work proposes two novel methods for dealing with the time series forecasting problem in the presence of concept drift. The proposed methods benefit from the Particle Swarm Optimization (PSO) technique to detect and react to concept drifts in the time series data stream. It is expected that the use of collective intelligence of PSO makes the proposed method more robust to false positive drift detections while maintaining a low error rate on the forecasting task. Experiments show that the methods achieved competitive results in comparison to state-of-the-art methods.},
  keywords = {\_tablet,Adaptation models,Concept drift,Data models,Data streams,Forecasting,Monitoring,Particle swarm optimization,Predictive models,Time series,Time series analysis,Training},
  file = {C\:\\Users\\LoongKuan\\Dropbox\\academic papers\\paper library\\Oliveira et al\\Oliveira et al. - 2017 - Time Series Forecasting in the Presence of Concept.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\MU22HDES\\8371949.html}
}

@inproceedings{orfanides2020,
  title = {Learning Decomposable Models by Coarsening},
  booktitle = {Proceedings of the 10th {{International Conference}} on {{Probabilistic Graphical Models}}},
  author = {Orfanides, George and P{\'e}rez, Aritz},
  year = {2020},
  month = feb,
  pages = {317--328},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-09-19},
  abstract = {During the last decade, some exact algorithms have been proposed for learning decomposable models by maximizing additively decomposable score functions, such as Log-likelihood, BDeu, and BIC. However, up to the date, the proposed exact approaches are practical for learning models up to 202020 variables. In this work, we present an approximated procedure that can learn decomposable models over hundreds of variables with a remarkable trade-off between the quality of the obtained solution and the amount of the computational resources required. The proposed learning procedure iteratively constructs a sequence of coarser decomposable (chordal) graphs. At each step, given a decomposable graph, the algorithm adds the subset of edges due to the actual minimal separators that maximizes the score function while maintaining the chordality. The proposed procedure has shown competitive results for learning decomposable models over hundred of variables using a reasonable amount of computational resources. Finally, we empirically show that it can be used to reduce the search space of exact procedures, which would allow them to address the learning of high-dimensional decomposable models.},
  langid = {english},
  file = {D\:\\work\\literature\\library\\Orfanides and Pérez - 2020 - Learning decomposable models by coarsening.pdf}
}

@inproceedings{orlitsky2004,
  title = {On Modeling Profiles Instead of Values},
  booktitle = {Proceedings of the 20th Conference on {{Uncertainty}} in Artificial Intelligence},
  author = {Orlitsky, Alon and Santhanam, Narayana P. and Viswanathan, Krishnamurthy and Zhang, Junan},
  year = {2004},
  month = jul,
  series = {{{UAI}} '04},
  pages = {426--435},
  publisher = {{AUAI Press}},
  address = {{Arlington, Virginia, USA}},
  urldate = {2021-02-08},
  abstract = {We consider the problem of estimating the distribution underlying an observed sample of data. Instead of maximum likelihood, which maximizes the probability of the observed values, we propose a different estimate, the \emph{high-profile} distribution, which maximizes the probability of the observed \emph{profile}---the number of symbols appearing any given number of times. We determine the high-profile distribution of several data samples, establish some of its general properties, and show that when the number of distinct symbols observed is small compared to the data size, the high-profile and maximum-likelihood distributions are roughly the same, but when the number of symbols is large, the distributions differ, and high-profile better explains the data.},
  isbn = {978-0-9749039-0-3},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Orlitsky et al. - 2004 - On modeling profiles instead of values.pdf}
}

@inproceedings{oza2005,
  title = {Online Bagging and Boosting},
  booktitle = {2005 {{IEEE International Conference}} on {{Systems}}, {{Man}} and {{Cybernetics}}},
  author = {Oza, N. C.},
  year = {2005},
  month = oct,
  volume = {3},
  pages = {2340-2345 Vol. 3},
  doi = {10.1109/ICSMC.2005.1571498},
  abstract = {Bagging and boosting are two of the most well-known ensemble learning methods due to their theoretical performance guarantees and strong experimental results. However, these algorithms have been used mainly in batch mode, i.e., they require the entire training set to be available at once and, in some cases, require random access to the data. In this paper, we present online versions of bagging and boosting that require only one pass through the training data. We build on previously presented work by describing some theoretical results. We also compare the online and batch algorithms experimentally in terms of accuracy and running time.},
  keywords = {Backpropagation algorithms,Bagging,batch mode,boosting,Boosting,ensemble learning,Intelligent systems,learning (artificial intelligence),Learning systems,NASA,online bagging learning method,online boosting learning method,online learning,Postal services,Predictive models,Supervised learning,training data,Training data},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Oza - 2005 - Online bagging and boosting.pdf}
}

@article{p?rez-g?llego2017,
  title = {Using Ensembles for Problems with Characterizable Changes in Data Distribution: {{A}} Case Study on Quantification},
  shorttitle = {Using Ensembles for Problems with Characterizable Changes in Data Distribution},
  author = {{P?rez-G?llego}, Pablo and Quevedo, Jos? Ram?n and {del Coz}, Juan Jos?},
  year = {2017},
  month = mar,
  journal = {Information Fusion},
  volume = {34},
  pages = {87--100},
  issn = {15662535},
  doi = {10.1016/j.inffus.2016.07.001},
  urldate = {2017-06-15},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Prez-Gllego et al. - 2017 - Using ensembles for problems with characterizable .pdf}
}

@article{pan2020,
  title = {Contracting {{Arbitrary Tensor Networks}}: {{General Approximate Algorithm}} and {{Applications}} in {{Graphical Models}} and {{Quantum Circuit Simulations}}},
  shorttitle = {Contracting {{Arbitrary Tensor Networks}}},
  author = {Pan, Feng and Zhou, Pengfei and Li, Sujie and Zhang, Pan},
  year = {2020},
  month = aug,
  journal = {Physical Review Letters},
  volume = {125},
  number = {6},
  pages = {060503},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevLett.125.060503},
  urldate = {2023-07-09},
  abstract = {We present a general method for approximately contracting tensor networks with an arbitrary connectivity. This enables us to release the computational power of tensor networks to wide use in inference and learning problems defined on general graphs. We show applications of our algorithm in graphical models, specifically on estimating free energy of spin glasses defined on various of graphs, where our method largely outperforms existing algorithms, including the mean-field methods and the recently proposed neural-network-based methods. We further apply our method to the simulation of random quantum circuits and demonstrate that, with a trade-off of negligible truncation errors, our method is able to simulate large quantum circuits that are out of reach of the state-of-the-art simulation methods.},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Pan et al. - 2020 - Contracting Arbitrary Tensor Networks General App.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\793PQ2JM\\PhysRevLett.125.html}
}

@article{pang2021,
  title = {Homophily Outlier Detection in Non-{{IID}} Categorical Data},
  author = {Pang, Guansong and Cao, Longbing and Chen, Ling},
  year = {2021},
  month = jul,
  journal = {Data Mining and Knowledge Discovery},
  volume = {35},
  number = {4},
  pages = {1163--1224},
  issn = {1573-756X},
  doi = {10.1007/s10618-021-00750-y},
  urldate = {2021-08-07},
  abstract = {Most of existing outlier detection methods assume that the outlier factors (i.e., outlierness scoring measures) of data entities (e.g., feature values and data objects) are Independent and Identically Distributed (IID). This assumption does not hold in real-world applications where the outlierness of different entities is dependent on each other and/or taken from different probability distributions (non-IID). This may lead to the failure of detecting important outliers that are too subtle to be identified without considering the non-IID nature. The issue is even intensified in more challenging contexts, e.g., high-dimensional data with many noisy features. This work introduces a novel outlier detection framework and its two instances to identify outliers in categorical data by capturing non-IID outlier factors. Our approach first defines and incorporates distribution-sensitive outlier factors and their interdependence into a value-value graph-based representation. It then models an outlierness propagation process in the value graph to learn the outlierness of feature values. The learned value outlierness allows for either direct outlier detection or outlying feature selection. The graph representation and mining approach is employed here to well capture the rich non-IID characteristics. Our empirical results on 15 real-world data sets with different levels of data complexities show that (i) the proposed outlier detection methods significantly outperform five state-of-the-art methods at the 95\%/99\% confidence level, achieving 10\textendash 28\% AUC improvement on the 10 most complex data sets; and (ii) the proposed feature selection methods significantly outperform three competing methods in enabling subsequent outlier detection of two different existing detectors.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Pang et al. - 2021 - Homophily outlier detection in non-IID categorical.pdf}
}

@article{papamakarios2019,
  title = {Normalizing {{Flows}} for {{Probabilistic Modeling}} and {{Inference}}},
  author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  year = {2019},
  month = dec,
  journal = {arXiv:1912.02762 [cs, stat]},
  eprint = {1912.02762},
  primaryclass = {cs, stat},
  urldate = {2020-07-05},
  abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,probalistic models,Statistics - Machine Learning},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Papamakarios et al. - 2019 - Normalizing Flows for Probabilistic Modeling and I.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\SK5LBYWG\\1912.html}
}

@article{paparrizos2019,
  title = {{{GRAIL}}: Efficient Time-Series Representation Learning},
  shorttitle = {{{GRAIL}}},
  author = {Paparrizos, John and Franklin, Michael J.},
  year = {2019},
  month = jul,
  journal = {Proceedings of the VLDB Endowment},
  volume = {12},
  number = {11},
  pages = {1762--1777},
  issn = {21508097},
  doi = {10.14778/3342263.3342648},
  urldate = {2019-10-16},
  abstract = {The analysis of time series is becoming increasingly prevalent across scientific disciplines and industrial applications. The effectiveness and the scalability of time-series mining techniques critically depend on design choices for three components responsible for (i) representing; (ii) comparing; and (iii) indexing time series. Unfortunately, these components have to date been investigated and developed independently, often resulting in mutually incompatible methods. The lack of a unified approach has hindered progress towards fast and accurate analytics over massive time-series collections. To address this major drawback, we present GRAIL, a generic framework to learn compact time-series representations that preserve the properties of a user-specified comparison function. Given the comparison function, GRAIL (i) extracts landmark time series using clustering; (ii) optimizes necessary parameters; and (iii) exploits approximations for kernel methods to construct representations in linear time and space by expressing each time series as a combination of the landmark time series. We extensively evaluate GRAIL for querying, classification, clustering, sampling, and visualization of time series. For these tasks, methods leveraging GRAIL's representations are significantly faster and at least as accurate as state-of-the-art methods operating over the raw time series. GRAIL shows promise as a new primitive for highly accurate, yet scalable, time-series analysis.},
  langid = {english},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\5B7HDNK3\\Paparrizos and Franklin - 2019 - GRAIL efficient time-series representation learni.pdf}
}

@article{pardo1997,
  title = {About Distances of Discrete Distributions Satisfying the Data Processing Theorem of Information Theory},
  author = {Pardo, M.C. and Vajda, I.},
  year = {1997},
  month = jul,
  journal = {IEEE Transactions on Information Theory},
  volume = {43},
  number = {4},
  pages = {1288--1293},
  issn = {1557-9654},
  doi = {10.1109/18.605597},
  abstract = {The distances of discrete probability distributions are considered. Necessary and sufficient conditions for validity of the data processing theorem of information theory are established. These conditions are applied to the Burbea-Rao (1982) divergences and Bregman (1967) distances.},
  keywords = {Automation,Chromium,Data processing,Extraterrestrial measurements,Information processing,Information theory,Probability distribution,Statistical distributions,Statistics,Sufficient conditions},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Pardo and Vajda - 1997 - About distances of discrete distributions satisfyi.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\H4BPMW67\\605597.html}
}

@article{parkash2015,
  title = {An {{Algorithm}} to {{Generate Probabilities}} with {{Specified Entropy}}},
  author = {Parkash, Om and Kakkar, Priyanka},
  year = {2015},
  journal = {Applied Mathematics},
  volume = {06},
  number = {12},
  pages = {1968},
  publisher = {{Scientific Research Publishing}},
  doi = {10.4236/am.2015.612174},
  urldate = {2022-08-07},
  abstract = {The present communication offers a method to determine an unknown discrete probability distribution with specified Tsallis entropy close to uniform distribution. The relative error of the distribution obtained has been compared with the distribution obtained with the help of mathematica software. The applications of the proposed algorithm with respect to Tsallis source coding, Huffman coding and cross entropy optimization principles have been provided.},
  langid = {english},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\UL97NCDM\\Parkash and Kakkar - 2015 - An Algorithm to Generate Probabilities with Specif.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\CXKTNUSJ\\2-7402894_61023.html}
}

@article{pavlichin2019,
  ids = {JMLR:v20:18-075},
  title = {Approximate {{Profile Maximum Likelihood}}},
  author = {Pavlichin, Dmitri S. and Jiao, Jiantao and Weissman, Tsachy},
  year = {2019},
  journal = {Journal of Machine Learning Research},
  volume = {20},
  number = {122},
  pages = {1--55},
  issn = {1533-7928},
  urldate = {2021-01-24},
  langid = {english},
  file = {/home/lklee/pCloudDrive/literature/library/Pavlichin et al. - 2019 - Approximate Profile Maximum Likelihood2.pdf;D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Pavlichin et al. - 2019 - Approximate Profile Maximum Likelihood.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\4DNSKGNR\\18-075.html}
}

@techreport{pearl1987,
  type = {Technical {{Report}}},
  title = {{{GRAPHOIDS}}: A {{Graph-Based Logic}} for {{Reasoning}} about {{Relevance Relations}}},
  author = {Pearl, Judea and Paz, Azaria},
  year = {1987},
  number = {R-53-L},
  pages = {28},
  institution = {{UCLA Computer Science Department}},
  abstract = {We consider 3-place relations I(x, z, y) where, x, y, and z are three non-intersecting sets of elements (e.g., propositions), and I(x, z, y) stands for the statement: ``Know ing z renders x irrelevant to y.'' We give sufficient conditions on I for the existence of a (minimal) graph G such that I(x, z, y) can be validated by testing whether z separates x from y in G. These conditions define a GRAPHOID. The theory of graphoids uncovers the axiomatic basis of information relevance (e.g., probabilis tic dependencies) and ties it to vertex-separation conditions in graphs. The defin ing axioms can also be viewed as inference rules for deducing which propositions are relevant to each other, given a certain state of knowledge},
  annotation = {pdf from chapter 13 of: Probabilistic and Causal Inference: The Works of Judea Pearl},
  file = {/home/lklee/pCloudDrive/literature/library/Pearl and Paz - 1987 - GRAPHOIDS a Graph-Based Logic for Reasoning about2.pdf;D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Pearl and Paz - 1987 - GRAPHOIDS a Graph-Based Logic for Reasoning about.pdf}
}

@book{pearl1988,
  title = {Probabilistic {{Reasoning}} in {{Intelligent Systems}}: {{Networks}} of {{Plausible Inference}}},
  shorttitle = {Probabilistic {{Reasoning}} in {{Intelligent Systems}}},
  author = {Pearl, Judea},
  year = {1988},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  address = {{San Francisco, CA, USA}},
  abstract = {From the Publisher: Probabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertaintyand offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognitionin short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information. Probabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability.},
  isbn = {978-1-55860-479-7},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Pearl - 1988 - Probabilistic Reasoning in Intelligent Systems Ne.pdf}
}

@article{pearl1989,
  title = {Conditional Independence and Its Representations},
  author = {Pearl, Judea and Geiger, Dan and Verma, Thomas},
  year = {1989},
  journal = {Kybernetika},
  volume = {25},
  number = {7},
  pages = {33--44},
  urldate = {2022-06-22},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Pearl et al. - 1989 - Conditional independence and its representations.pdf}
}

@book{pearl2000,
  title = {Causality: Models, Reasoning, and Inference},
  shorttitle = {Causality},
  author = {Pearl, Judea},
  year = {2000},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, U.K. ; New York}},
  isbn = {978-0-521-89560-6 978-0-521-77362-1},
  lccn = {BD541 .P43 2000},
  keywords = {Causation,Probabilities},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Pearl - 2000 - Causality models, reasoning, and inference.pdf}
}

@article{pearson1900,
  title = {X. {{On}} the Criterion That a given System of Deviations from the Probable in the Case of a Correlated System of Variables Is Such That It Can Be Reasonably Supposed to Have Arisen from Random Sampling},
  author = {Pearson, Karl},
  year = {1900},
  month = jul,
  journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume = {50},
  number = {302},
  pages = {157--175},
  publisher = {{Taylor \& Francis}},
  issn = {1941-5982},
  doi = {10.1080/14786440009463897},
  urldate = {2023-01-31},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Pearson_1900_X.pdf}
}

@incollection{pechenizkiy2005,
  title = {Knowledge {{Discovery}} from {{Microbiology Data}}: {{Many-Sided Analysis}} of {{Antibiotic Resistance}} in {{Nosocomial Infections}}},
  shorttitle = {Knowledge {{Discovery}} from {{Microbiology Data}}},
  booktitle = {Professional {{Knowledge Management}}},
  author = {Pechenizkiy, Mykola and Tsymbal, Alexey and Puuronen, Seppo and Shifrin, Michael and Alexandrova, Irina},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Althoff, Klaus-Dieter and Dengel, Andreas and Bergmann, Ralph and Nick, Markus and {Roth-Berghofer}, Thomas},
  year = {2005},
  volume = {3782},
  pages = {360--372},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11590019\_41},
  urldate = {2020-03-04},
  isbn = {978-3-540-30465-4 978-3-540-31620-6}
}

@misc{perrin2023,
  title = {Mitigating Crosstalk Errors by Randomized Compiling: {{Simulation}} of the {{BCS}} Model on a Superconducting Quantum Computer},
  shorttitle = {Mitigating Crosstalk Errors by Randomized Compiling},
  author = {Perrin, Hugo and Scoquart, Thibault and Shnirman, Alexander and Schmalian, J{\"o}rg and Snizhko, Kyrylo},
  year = {2023},
  month = may,
  number = {arXiv:2305.02345},
  eprint = {2305.02345},
  primaryclass = {cond-mat, physics:quant-ph},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.02345},
  urldate = {2023-06-26},
  abstract = {We develop and apply an extension of the randomized compiling (RC) protocol that includes neighbouring qubits and dramatically reduces crosstalk between superconducting qubits on IBMQ quantum computers (\textbackslash texttt\{ibm\textbackslash\_lagos\} and \textbackslash texttt\{ibmq\textbackslash\_ehningen\}). Crosstalk errors, stemming from CNOT two-qubit gates, are a crucial source of errors on numerous quantum computing platforms. For the IBMQ machines their magnitude is usually underestimated by the benchmark protocols provided by the manufacturer. Our RC protocol turns coherent noise due to crosstalk into a depolarising noise channel that can then be treated using established error mitigation schemes such as noise estimation circuits. We apply our approach to the quantum simulation of the non-equilibrium dynamics of the Bardeen-Cooper-Schrieffer (BCS) Hamiltonian for superconductivity, a model that is particularly challenging because of the long-range interaction of Cooper pairs. With 135 CNOT gates, we work in a regime where crosstalk, as opposed to either trotterization or qubit decoherence, dominates the error. Our twirling of neighbouring qubits is shown to dramatically improve the noise estimation protocol without the need to add new qubits or circuits and allows for a quantitative simulation of the BCS-model.},
  archiveprefix = {arxiv},
  keywords = {Condensed Matter - Superconductivity,Quantum Physics},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Perrin et al. - 2023 - Mitigating crosstalk errors by randomized compilin.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\4YG3H6BB\\2305.html}
}

@inproceedings{petitjean2013,
  title = {Scaling {{Log-Linear Analysis}} to {{High-Dimensional Data}}},
  booktitle = {2013 {{IEEE}} 13th {{International Conference}} on {{Data Mining}}},
  author = {Petitjean, Francois and Webb, Geoffrey I. and Nicholson, Ann E.},
  year = {2013},
  month = dec,
  pages = {597--606},
  publisher = {{IEEE}},
  address = {{Dallas, TX, USA}},
  doi = {10.1109/ICDM.2013.17},
  urldate = {2019-06-26},
  abstract = {Association discovery is a fundamental data mining task. The primary statistical approach to association discovery between variables is log-linear analysis. Classical approaches to log-linear analysis do not scale beyond about ten variables. We develop an efficient approach to log-linear analysis that scales to hundreds of variables by melding the classical statistical machinery of log-linear analysis with advanced data mining techniques from association discovery and graphical modeling.},
  isbn = {978-0-7695-5108-1},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Petitjean et al. - 2013 - Scaling Log-Linear Analysis to High-Dimensional Da.pdf}
}

@inproceedings{petitjean2014,
  title = {A {{Statistically Efficient}} and {{Scalable Method}} for {{Log-Linear Analysis}} of {{High-Dimensional Data}}},
  booktitle = {2014 {{IEEE International Conference}} on {{Data Mining}}},
  author = {Petitjean, Francois and Allison, Lloyd and Webb, Geoffrey I.},
  year = {2014},
  month = dec,
  pages = {480--489},
  publisher = {{IEEE}},
  address = {{Shenzhen, China}},
  doi = {10.1109/ICDM.2014.23},
  urldate = {2019-02-26},
  abstract = {Log-linear analysis is the primary statistical approach to discovering conditional dependencies between the variables of a dataset. A good log-linear analysis method requires both high precision and statistical efficiency. High precision means that the risk of false discoveries should be kept very low. Statistical efficiency means that the method should discover actual associations with as few samples as possible. Classical approaches to log-linear analysis make use of {$\chi$}2 tests to control this balance between quality and complexity. We present an information-theoretic approach to loglinear analysis. We show that our approach 1) requires significantly fewer samples to discover the true associations than statistical approaches \textendash{} statistical efficiency \textendash{} 2) controls for the risk of false discoveries as well as statistical approaches \textendash{} high precision \textendash and 3) can perform the discovery on datasets with hundreds of variables on a standard desktop computer \textendash{} computational efficiency.},
  isbn = {978-1-4799-4302-9 978-1-4799-4303-6},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Petitjean et al. - 2014 - A Statistically Efficient and Scalable Method for .pdf}
}

@inproceedings{petitjean2015,
  title = {Scaling Log-Linear Analysis to Datasets with Thousands of Variables},
  booktitle = {Proceedings of the 2015 {{SIAM International Conference}} on {{Data Mining}}},
  author = {Petitjean, Fran{\c c}ois and Webb, Geoffrey I.},
  year = {2015},
  month = jun,
  pages = {469--477},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611974010.53},
  urldate = {2019-02-26},
  abstract = {Association discovery is a fundamental data mining task. The primary statistical approach to association discovery between variables is log-linear analysis. Classical approaches to log-linear analysis do not scale beyond about ten variables. We have recently shown that, if we ensure that the graph supporting the log-linear model is chordal, log-linear analysis can be applied to datasets with hundreds of variables without sacrificing the statistical soundness [21]. However, further scalability remained limited, because state-of-the-art techniques have to examine every edge at every step of the search. This paper makes the following contributions: 1) we prove that only a very small subset of edges has to be considered at each step of the search; 2) we demonstrate how to efficiently find this subset of edges and 3) we show how to efficiently keep track of the best edges to be subsequently added to the initial model. Our experiments, carried out on real datasets with up to 2000 variables, show that our contributions make it possible to gain about 4 orders of magnitude, making log-linear analysis of datasets with thousands of variables possible in seconds instead of days.},
  isbn = {978-1-61197-401-0},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Petitjean and Webb - 2015 - Scaling log-linear analysis to datasets with thous.pdf}
}

@article{petitjean2018,
  title = {Accurate Parameter Estimation for {{Bayesian}} Network Classifiers Using Hierarchical {{Dirichlet}} Processes},
  author = {Petitjean, Fran{\c c}ois and Buntine, Wray and Webb, Geoffrey I. and Zaidi, Nayyar},
  year = {2018},
  month = sep,
  journal = {Machine Learning},
  volume = {107},
  number = {8-10},
  pages = {1303--1331},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-018-5718-0},
  urldate = {2018-12-15},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Petitjean et al. - 2018 - Accurate parameter estimation for Bayesian network.pdf}
}

@article{piatkowski2013,
  title = {Spatio-Temporal Random Fields: Compressible Representation and Distributed Estimation},
  shorttitle = {Spatio-Temporal Random Fields},
  author = {Piatkowski, Nico and Lee, Sangkyun and Morik, Katharina},
  year = {2013},
  month = oct,
  journal = {Machine Learning},
  volume = {93},
  number = {1},
  pages = {115--139},
  issn = {1573-0565},
  doi = {10.1007/s10994-013-5399-7},
  urldate = {2023-06-21},
  abstract = {Modern sensing technology allows us enhanced monitoring of dynamic activities in business, traffic, and home, just to name a few. The increasing amount of sensor measurements, however, brings us the challenge for efficient data analysis. This is especially true when sensing targets can interoperate\textemdash in such cases we need learning models that can capture the relations of sensors, possibly without collecting or exchanging all data. Generative graphical models namely the Markov random fields (MRF) fit this purpose, which can represent complex spatial and temporal relations among sensors, producing interpretable answers in terms of probability. The only drawback will be the cost for inference, storing and optimizing a very large number of parameters\textemdash not uncommon when we apply them for real-world applications.},
  langid = {english},
  keywords = {Graphical models,Regularization,Spatio-temporal},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\63CVDUIK\\Piatkowski et al. - 2013 - Spatio-temporal random fields compressible repres.pdf;D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Piatkowski et al. - 2013 - Spatio-temporal random fields compressible repres.pdf}
}

@inproceedings{piatkowski2020,
  title = {Hyper-{{Parameter-Free Generative Modelling}} with {{Deep Boltzmann Trees}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Piatkowski, Nico},
  editor = {Brefeld, Ulf and Fromont, Elisa and Hotho, Andreas and Knobbe, Arno and Maathuis, Marloes and Robardet, C{\'e}line},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {415--431},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-46147-8\_25},
  abstract = {Deep neural networks achieve state-of-the-art results in various classification and synthetic data generation tasks. However, only little is known about why depth improves a model. We investigate the structure of stochastic deep neural works, also known as Deep Boltzmann Machines, to shed some light on this issue. While the best known results postulate an exponential dependence between the number of visible units and the depth of the model, we show that the required depth is upper bounded by the longest path in the underlying junction tree, which is at most linear in the number of visible units. Moreover, we show that the conditional independence structure of any categorical Deep Boltzmann Machine contains a sub-tree that allows the consistent estimation of the full joint probability mass function of all visible units. We connect our results to l1l1l\_1-regularized maximum-likelihood estimation and Chow-Liu trees. Based on our theoretical findings, we present a new tractable version of Deep Boltzmann Machines, namely the Deep Boltzmann Tree (DBT). We provide a hyper-parameter-free algorithm for learning the DBT from data, and propose a new initialization method to enforce convergence to good solutions. Our findings provide some theoretical evidence for why a deep model might be beneficial. Experimental results on benchmark data show, that the DBT is a theoretical sound alternative to likelihood-free generative models.},
  isbn = {978-3-030-46147-8},
  langid = {english},
  keywords = {Deep Boltzmann Machine,Generative model,Structure learning},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Piatkowski - 2020 - Hyper-Parameter-Free Generative Modelling with Dee.pdf}
}

@article{plasse2021,
  ids = {StreamingChangepointDetection},
  title = {Streaming Changepoint Detection for Transition Matrices},
  author = {Plasse, Joshua and Hoeltgebaum, Henrique and Adams, Niall M.},
  year = {2021},
  month = jul,
  journal = {Data Mining and Knowledge Discovery},
  volume = {35},
  number = {4},
  pages = {1287--1316},
  issn = {1573-756X},
  doi = {10.1007/s10618-021-00747-7},
  urldate = {2021-08-07},
  abstract = {Sequentially detecting multiple changepoints in a data stream is a challenging task. Difficulties relate to both computational and statistical aspects, and in the latter, specifying control parameters is a particular problem. Choosing control parameters typically relies on unrealistic assumptions, such as the distributions generating the data, and their parameters, being known. This is implausible in the streaming paradigm, where several changepoints will exist. Further, current literature is mostly concerned with streams of continuous-valued observations, and focuses on detecting a single changepoint. There is a dearth of literature dedicated to detecting multiple changepoints in transition matrices, which arise from a sequence of discrete states. This paper makes the following contributions: a complete framework is developed for adaptively and sequentially estimating a Markov transition matrix in the streaming data setting. A change detection method is then developed, using a novel moment matching technique, which can effectively monitor for multiple changepoints in a transition matrix. This adaptive detection and estimation procedure for transition matrices, referred to as ADEPT-M, is compared to several change detectors on synthetic data streams, and is implemented on two real-world data streams \textendash{} one consisting of over nine million HTTP web requests, and the other being a well-studied electricity market data set.},
  langid = {english},
  keywords = {Researcher App},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Plasse et al. - 2021 - Streaming changepoint detection for transition mat.pdf}
}

@inproceedings{poczos2012,
  title = {Nonparametric {{Estimation}} of {{Conditional Information}} and {{Divergences}}},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Poczos, Barnabas and Schneider, Jeff},
  year = {2012},
  month = mar,
  pages = {914--923},
  publisher = {{PMLR}},
  issn = {1938-7228},
  urldate = {2021-01-24},
  abstract = {In this paper we propose new nonparametric estimators for a family of conditional mutual information and divergences. Our estimators are easy to compute; they only use simple k nearest neighbor bas...},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Poczos and Schneider - 2012 - Nonparametric Estimation of Conditional Informatio.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\RZCZC9QE\\poczos12.html}
}

@article{polikar2001,
  title = {Learn++: An Incremental Learning Algorithm for Supervised Neural Networks},
  shorttitle = {Learn++},
  author = {Polikar, Robi and Upda, Lalita and Upda, Satish S. and Honavar, Vasant},
  year = {2001},
  month = nov,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume = {31},
  number = {4},
  pages = {497--508},
  issn = {1094-6977},
  doi = {10.1109/5326.983933},
  abstract = {We introduce Learn++, an algorithm for incremental training of neural network (NN) pattern classifiers. The proposed algorithm enables supervised NN paradigms, such as the multilayer perceptron (MLP), to accommodate new data, including examples that correspond to previously unseen classes. Furthermore, the algorithm does not require access to previously used data during subsequent incremental learning sessions, yet at the same time, it does not forget previously acquired knowledge. Learn++ utilizes ensemble of classifiers by generating multiple hypotheses using training data sampled according to carefully tailored distributions. The outputs of the resulting classifiers are combined using a weighted majority voting procedure. We present simulation results on several benchmark datasets as well as a real-world classification task. Initial results indicate that the proposed algorithm works rather well in practice. A theoretical upper bound on the error of the classifiers constructed by Learn++ is also provided},
  keywords = {benchmark datasets,catastrophic forgetting,classification algorithms,Classification algorithms,Costs,data analysis,incremental learning,incremental learning algorithm,incremental learning sessions,incremental training,knowledge acquisition,Knowledge acquisition,Learn++,learning (artificial intelligence),MLP,multilayer perceptron,Multilayer perceptrons,multiple hypotheses,neural nets,neural network pattern classifiers,Neural networks,pattern classification,pattern recognition,Pattern recognition,previously acquired knowledge,real-world classification task,Stability,supervised neural networks,supervised NN paradigms,training data,Training data,unseen classes,Upper bound,Voting,weighted majority voting procedure},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Polikar et al. - 2001 - Learn++ an incremental learning algorithm for sup.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\BYFGD78G\\983933.html}
}

@misc{polo2022,
  title = {A Unified Framework for Dataset Shift Diagnostics},
  author = {Polo, Felipe Maia and Izbicki, Rafael and Lacerda, Evanildo Gomes and {Ibieta-Jimenez}, Juan Pablo and Vicente, Renato},
  year = {2022},
  number = {arXiv:2205.08340},
  eprint = {2205.08340},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2205.08340},
  urldate = {2023-04-26},
  abstract = {Most machine learning (ML) methods assume that the data used in the training phase comes from the target population. However, in practice one often faces dataset shift, which, if not properly taken into account, may decrease the predictive performance of the ML models. In general, if the practitioner knows which type of shift is taking place -- e.g., covariate shift or label shift -- they may apply transfer learning methods to obtain better predictions. Unfortunately, current methods for detecting shift are only designed to detect specific types of shift or cannot formally test their presence. We introduce a general and unified framework that gives insights on how to improve prediction methods by detecting the presence of different types of shift and quantifying how strong they are. Our approach can be used for any data type (tabular/image/text) and both for classification and regression tasks. Moreover, it uses formal hypotheses tests that controls false alarms. We illustrate how our framework is useful in practice using both artificial and real datasets, including an example of how our framework leads to insights that indeed improve the predictive power of a supervised model. Our package for dataset shift detection can be found in https://github.com/felipemaiapolo/detectshift.},
  archiveprefix = {arxiv},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML),Methodology (stat.ME)},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Polo et al. - 2022 - A unified framework for dataset shift diagnostics3.pdf}
}

@inproceedings{poon2011,
  title = {Sum-Product Networks: {{A}} New Deep Architecture},
  shorttitle = {Sum-Product Networks},
  booktitle = {2011 {{IEEE International Conference}} on {{Computer Vision Workshops}} ({{ICCV Workshops}})},
  author = {Poon, H. and Domingos, P.},
  year = {2011},
  month = nov,
  pages = {689--690},
  doi = {10.1109/ICCVW.2011.6130310},
  abstract = {The key limiting factor in graphical model inference and learning is the complexity of the partition function. We thus ask the question: what are the most general conditions under which the partition function is tractable? The answer leads to a new kind of deep architecture, which we call sum product networks (SPNs) and will present in this abstract. The key idea of SPNs is to compactly represent the partition function by introducing multiple layers of hidden variables. An SPN is a rooted directed acyclic graph with variables as leaves, sums and products as internal nodes, and weighted edges.},
  keywords = {Backpropagation,Computational modeling,Computer architecture,Decision trees,directed graphs,graphical model inference,Graphical models,hidden variables,internal nodes,Junctions,learning,leaves,partition function,rooted directed acyclic graph,sum-product networks,weighted edges},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Poon and Domingos - 2011 - Sum-product networks A new deep architecture.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\EFP3Y3NS\\6130310.html}
}

@inproceedings{pratt2003,
  title = {Visualizing {{Concept Drift}}},
  booktitle = {Proceedings of the {{Ninth ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Pratt, Kevin B. and Tschapek, Gleb},
  year = {2003},
  series = {{{KDD}} '03},
  pages = {735--740},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/956750.956849},
  urldate = {2018-08-22},
  abstract = {We describe a visualization technique that uses brushed, parallel histograms to aid in understanding concept drift in multidimensional problem spaces. This technique illustrates the relationship between changes in distributions of multiple antecedent feature values and the outcome distribution. We can also observe effects on the relative utilization of predictive rules. Our parallel histogram technique solves the over-plotting difficulty of parallel coordinate graphs and the difficulty of comparing distributions of brushed and original data. We demonstrate our technique's usefulness in understanding concept drifts in power demand and stock investment returns.},
  isbn = {978-1-58113-737-8},
  keywords = {brushing,concept drift,parallel coordinate graph,parallel histogram,visualization},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Pratt and Tschapek - 2003 - Visualizing Concept Drift.pdf}
}

@book{pressman2010,
  title = {Software Engineering: A Practitioner's Approach},
  shorttitle = {Software Engineering},
  author = {Pressman, Roger S.},
  year = {2010},
  edition = {7th ed},
  publisher = {{McGraw-Hill Higher Education}},
  address = {{New York}},
  isbn = {978-0-07-337597-7},
  lccn = {QA76.758 .P75 2010},
  keywords = {Software engineering},
  annotation = {OCLC: ocn271105592},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Pressman - 2010 - Software engineering a practitioner's approach.pdf}
}

@article{rahman2020,
  title = {A {{Statistically Efficient}} and {{Scalable Method}} for {{Exploratory Analysis}} of {{High-Dimensional Data}}},
  author = {Rahman, Mohammad S. and Haffari, Gholamreza},
  year = {2020},
  month = feb,
  journal = {SN Computer Science},
  volume = {1},
  number = {2},
  pages = {64},
  issn = {2661-8907},
  doi = {10.1007/s42979-020-0064-2},
  urldate = {2021-02-08},
  abstract = {Discovering associations among variables is an important data mining task. The associations can be considered as statistical dependencies among random variables, expressed as the structure of an underlying probabilistic graphical model. Current methods for graphical model structure discovery either do not scale well to datasets with large sample sizes, or suffer from high false discovery rates when the number of dimensions is much larger than the sample size. In this paper, we propose a scalable and statistically efficient approach for graphical model structure discovery for multivariate data involving continuous variables. Our approach uses a minimum message length (MML)-based objective, for which we design a greedy algorithm where the best edges maximising improvements to the MML-based score are added incrementally to the graphical model. We present extensive empirical results on synthetic data with different sample, variable, clique and inverse correlation coefficient and show that our method outperforms strong baselines in terms of both speed and the accuracy of the predicted associations among the random variables in the graphical model. We also report that our method performs significantly very well in AML, BRCA cancer data and other real-life datasets.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Rahman and Haffari - 2020 - A Statistically Efficient and Scalable Method for .pdf}
}

@misc{rajkovicvladislav1997,
  title = {{{UCI Machine Learning Repository}}: {{Nursery Data Set}}},
  author = {{Rajkovic, Vladislav} and {Bohanec, Marko} and {Zupan, Blaz}},
  year = {1997},
  month = jun,
  urldate = {2018-11-08},
  howpublished = {https://archive.ics.uci.edu/ml/datasets/Nursery},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\7AVIJDI7\\Nursery.html}
}

@article{rao1957,
  title = {Maximum {{Likelihood Estimation}} for the {{Multinomial Distribution}}},
  author = {Rao, C. Radhakrishna},
  year = {1957},
  journal = {Sankhy\=a: The Indian Journal of Statistics (1933-1960)},
  volume = {18},
  number = {1/2},
  eprint = {25048341},
  eprinttype = {jstor},
  pages = {139--148},
  issn = {0036-4452},
  urldate = {2020-01-10},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Rao - 1957 - Maximum Likelihood Estimation for the Multinomial .pdf}
}

@article{raza2019,
  title = {Covariate Shift Estimation Based Adaptive Ensemble Learning for Handling Non-Stationarity in Motor Imagery Related {{EEG-based}} Brain-Computer Interface},
  author = {Raza, Haider and Rathee, Dheeraj and Zhou, Shang-Ming and Cecotti, Hubert and Prasad, Girijesh},
  year = {2019},
  month = may,
  journal = {Neurocomputing},
  series = {Learning in the {{Presence}} of {{Class Imbalance}} and {{Concept Drift}}},
  volume = {343},
  pages = {154--166},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2018.04.087},
  urldate = {2019-06-12},
  abstract = {The non-stationary nature of electroencephalography (EEG) signals makes an EEG-based brain-computer interface (BCI) a dynamic system, thus improving its performance is a challenging task. In addition, it is well-known that due to non-stationarity based covariate shifts, the input data distributions of EEG-based BCI systems change during inter- and intra-session transitions, which poses great difficulty for developments of online adaptive data-driven systems. Ensemble learning approaches have been used previously to tackle this challenge. However, passive scheme based implementation leads to poor efficiency while increasing high computational cost. This paper presents a novel integration of covariate shift estimation and unsupervised adaptive ensemble learning (CSE-UAEL) to tackle non-stationarity in motor-imagery (MI) related EEG classification. The proposed method first employs an exponentially weighted moving average model to detect the covariate shifts in the common spatial pattern features extracted from MI related brain responses. Then, a classifier ensemble was created and updated over time to account for changes in streaming input data distribution wherein new classifiers are added to the ensemble in accordance with estimated shifts. Furthermore, using two publicly available BCI-related EEG datasets, the proposed method was extensively compared with the state-of-the-art single-classifier based passive scheme, single-classifier based active scheme and ensemble based passive schemes. The experimental results show that the proposed active scheme based ensemble learning algorithm significantly enhances the BCI performance in MI classifications.},
  keywords = {Brain-computer interface (BCI),Covariate shift,Electroencephalogram (EEG),Ensemble learning,Non-stationary learning},
  file = {/home/lklee/google-drive/Academic Papers/Zotero/pdf/2019/Raza et al_2019_Covariate shift estimation based adaptive ensemble learning for handling.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\92G9J827\\S0925231219301560.html}
}

@article{read2018,
  title = {Concept-Drifting {{Data Streams}} Are {{Time Series}}; {{The Case}} for {{Continuous Adaptation}}},
  author = {Read, Jesse},
  year = {2018},
  month = oct,
  journal = {arXiv:1810.02266 [cs, stat]},
  eprint = {1810.02266},
  primaryclass = {cs, stat},
  urldate = {2021-03-30},
  abstract = {Learning from data streams is an increasingly important topic in data mining, machine learning, and artificial intelligence in general. A major focus in the data stream literature is on designing methods that can deal with concept drift, a challenge where the generating distribution changes over time. A general assumption in most of this literature is that instances are independently distributed in the stream. In this work we show that, in the context of concept drift, this assumption is contradictory, and that the presence of concept drift necessarily implies temporal dependence; and thus some form of time series. This has important implications on model design and deployment. We explore and highlight the these implications, and show that Hoeffding-tree based ensembles, which are very popular for learning in streams, are not naturally suited to learning \textbackslash emph\{within\} drift; and can perform in this scenario only at significant computational cost of destructive adaptation. On the other hand, we develop and parameterize gradient-descent methods and demonstrate how they can perform \textbackslash emph\{continuous\} adaptation with no explicit drift-detection mechanism, offering major advantages in terms of accuracy and efficiency. As a consequence of our theoretical discussion and empirical observations, we outline a number of recommendations for deploying methods in concept-drifting streams.},
  archiveprefix = {arxiv},
  keywords = {\_tablet,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\DUG3FQIY\\Read - 2018 - Concept-drifting Data Streams are Time Series\; The.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\MUH4238P\\1810.html}
}

@article{renyi1961,
  title = {On {{Measures}} of {{Entropy}} and {{Information}}},
  author = {R{\'e}nyi, Alfr{\'e}d},
  year = {1961},
  month = jan,
  journal = {Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics},
  volume = {4.1},
  pages = {547--562},
  publisher = {{University of California Press}},
  urldate = {2022-10-11},
  abstract = {Berkeley Symposium on Mathematical Statistics and Probability},
  file = {D\:\\work\\literature\\library\\Rényi - 1961 - On Measures of Entropy and Information.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\GDVMJ5YQ\\1200512181.html}
}

@article{riso2022,
  title = {Concept {{Drift Estimation}} with {{Graphical Models}}},
  author = {Riso, Luigi and Guerzoni, Marco},
  year = {2022},
  month = may,
  journal = {Information Sciences},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2022.05.056},
  urldate = {2022-05-18},
  abstract = {This paper deals with the issue of concept-drift in machine learning in the context of high dimensional problems. In contrast to previous concept drift detection methods, this application does not depend on the machine learning model in use for a specific target variable, but rather, it attempts to assess the concept drift as an independent characteristic of the evolution of a dataset. This major achievement enables data to be tested for the presence of drift, independently of the specific problem at hand. This is extremely useful when the same dataset is utilized for different classifications simultaneously, as it is often the case in a business environment. Moreover, unlike previous approaches, this method does not require the re-testing of each new model; a strategy which could prove expensive in computational terms. The fundamental intention of this work is to make use of graphical models to elicit the visible structure of data and represent it as a network. Specifically, we investigate how a graphical model evolves by looking at the creation of new links, and the disappearance of existing ones, in different time periods. We perform this task in four steps. We compute the adjacency matrix of a graph in each period, we apply a function that maps each possible state of the adjacency matrix over time into a transition matrix. We use the information in the transition matrix to produce a metric to estimate the presence of a drift in the data. Eventually, we evaluate this method with both three real-world datasets and a synthetic one.},
  langid = {english},
  keywords = {Bayesian Logistic Regression,Drift Estimation,Graphical Models,Unsupervised Learning},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\D2B4XKQ7\\S0020025522004881.html}
}

@article{roberts2017,
  title = {Cross-validation Strategies for Data with Temporal, Spatial, Hierarchical, or Phylogenetic Structure},
  author = {Roberts, David R. and Bahn, Volker and Ciuti, Simone and Boyce, Mark S. and Elith, Jane and Guillera-Arroita, Gurutzeta and Hauenstein, Severin and Lahoz-Monfort, Jos{\'e} J. and Schr{\"o}der, Boris and Thuiller, Wilfried and Warton, David I. and Wintle, Brendan A. and Hartig, Florian and Dormann, Carsten F.},
  year = {2017},
  month = aug,
  journal = {Ecography},
  volume = {40},
  number = {8},
  pages = {913--929},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {1600-0587},
  doi = {10.1111/ecog.02881},
  urldate = {2020-03-13},
  abstract = {Ecological data often show temporal, spatial, hierarchical (random effects), or phylogenetic structure. Modern statistical approaches are increasingly accounting for such dependencies. However, when ...},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Roberts et al. - 2017 - Cross‐validation strategies for data with temporal.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\KSTF7KNF\\ecog.html}
}

@incollection{rodriguez2005,
  title = {First Principles Model Based Control},
  booktitle = {Computer {{Aided Chemical Engineering}}},
  author = {Rodr{\'i}guez, Manuel and P{\'e}rez, David},
  year = {2005},
  volume = {20},
  pages = {1285--1290},
  publisher = {{Elsevier}},
  doi = {10.1016/S1570-7946(05)80056-2},
  urldate = {2018-05-30},
  abstract = {Model Based Control is an important and widely used (mainly MPC) technique. This paper provides a new control architecture based on the use of physical models. In this preliminary work the architecture is applied for unconstrained multivariable control. It has been applied on several standard process units obtaining encouraging results. It has some advantages over MPC as it can use non linear rigorous models and it doesn't need any identification step.},
  isbn = {978-0-444-51987-0},
  langid = {english},
  file = {D\:\\work\\literature\\library\\Rodríguez and Pérez - 2005 - First principles model based control.pdf}
}

@article{roseberry2021,
  title = {Self-Adjusting k Nearest Neighbors for Continual Learning from Multi-Label Drifting Data Streams},
  author = {Roseberry, Martha and Krawczyk, Bartosz and Djenouri, Youcef and Cano, Alberto},
  year = {2021},
  month = jun,
  journal = {Neurocomputing},
  volume = {442},
  pages = {10--25},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2021.02.032},
  urldate = {2023-03-22},
  abstract = {Drifting data streams and multi-label data are both challenging problems. Multi-label instances may simultaneously be associated with many labels and classifiers must predict the complete set of labels. Learning from data streams requires algorithms able to learn from potentially unbounded data that is constantly changing. When multi-label data arrives as a stream, the challenges of both problems must be addressed, but additional challenges unique to the combined problem also arise. Each label may experience different concept drifts, simultaneously or distinctly, and parameter optimizations may be different for each label. In this paper we present a self-adapting algorithm for drifting, multi-label data streams, that can adapt to a variety of concepts drifts, is robust to data-level difficulties, and mitigates the necessity to tune multiple parameters. The window of retained instances self-adjusts in size to retain only the current concept, enabling efficient response to abrupt concept drift. The value k is self-adapting for each label, relieving the necessity to tune and allowing it to change, over time, for each label individually. A novel, label-based mechanism disables individual labels that contribute to error, while another punitive measure removes erroneous instances entirely, increasing robustness to noise, concept drift and label differences. Extensive experiments on 35 multi-label streams and generators demonstrate the superiority and advantages of the self-adapting mechanisms proposed compared to existing state-of-the-art methods.},
  langid = {english},
  keywords = {Concept drift,Continual learning,Data streams,Multi-label classification},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Roseberry et al. - 2021 - Self-adjusting k nearest neighbors for continual l.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\AJ7SATBE\\S0925231221002782.html}
}

@incollection{rubenstein2019,
  title = {Practical and {{Consistent Estimation}} of F-{{Divergences}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Rubenstein, Paul and Bousquet, Olivier and Djolonga, Josip and Riquelme, Carlos and Tolstikhin, Ilya O},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {4070--4080},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2020-03-22},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Rubenstein et al. - 2019 - Practical and Consistent Estimation of f-Divergenc.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\Q39BALI2\\8661-practical-and-consistent-estimation-of-f-divergences.html}
}

@article{russwurm2018,
  title = {Multi-{{Temporal Land Cover Classification}} with {{Sequential Recurrent Encoders}}},
  author = {Ru{\ss}wurm, Marc and K{\"o}rner, Marco},
  year = {2018},
  month = mar,
  journal = {ISPRS International Journal of Geo-Information},
  volume = {7},
  number = {4},
  pages = {129},
  issn = {2220-9964},
  doi = {10.3390/ijgi7040129},
  urldate = {2019-07-03},
  abstract = {Earth observation (EO) sensors deliver data at daily or weekly intervals. Most land use and land cover classification (LULC) approaches, however, are designed for cloud-free and mono-temporal observations. The increasing temporal capabilities of today's sensors enable the use of temporal, along with spectral and spatial features.Domains such as speech recognition or neural machine translation, work with inherently temporal data and, today, achieve impressive results by using sequential encoder-decoder structures. Inspired by these sequence-to-sequence models, we adapt an encoder structure with convolutional recurrent layers in order to approximate a phenological model for vegetation classes based on a temporal sequence of Sentinel 2 (S2) images. In our experiments, we visualize internal activations over a sequence of cloudy and non-cloudy images and find several recurrent cells that reduce the input activity for cloudy observations. Hence, we assume that our network has learned cloud-filtering schemes solely from input data, which could alleviate the need for tedious cloud-filtering as a preprocessing step for many EO approaches. Moreover, using unfiltered temporal series of top-of-atmosphere (TOA) reflectance data, our experiments achieved state-of-the-art classification accuracies on a large number of crop classes with minimal preprocessing, compared to other classification approaches.},
  langid = {english},
  file = {D\:\\work\\literature\\library\\Rußwurm and Körner - 2018 - Multi-Temporal Land Cover Classification with Sequ.pdf}
}

@article{saez2015,
  title = {Probabilistic Change Detection and Visualization Methods for the Assessment of Temporal Stability in Biomedical Data Quality},
  author = {S{\'a}ez, Carlos and Rodrigues, Pedro Pereira and Gama, Jo{\~a}o and Robles, Montserrat and {Garc{\'i}a-G{\'o}mez}, Juan M.},
  year = {2015},
  month = jul,
  journal = {Data Mining and Knowledge Discovery},
  volume = {29},
  number = {4},
  pages = {950--975},
  issn = {1573-756X},
  doi = {10.1007/s10618-014-0378-6},
  urldate = {2022-08-02},
  abstract = {Knowledge discovery on biomedical data can be based on on-line, data-stream analyses, or using retrospective, timestamped, off-line datasets. In both cases, changes in the processes that generate data or in their quality features through time may hinder either the knowledge discovery process or the generalization of past knowledge. These problems can be seen as a lack of data temporal stability. This work establishes the temporal stability as a data quality dimension and proposes new methods for its assessment based on a probabilistic framework. Concretely, methods are proposed for (1) monitoring changes, and (2) characterizing changes, trends and detecting temporal subgroups. First, a probabilistic change detection algorithm is proposed based on the Statistical Process Control of the posterior Beta distribution of the Jensen\textendash Shannon distance, with a memoryless forgetting mechanism. This algorithm (PDF-SPC) classifies the degree of current change in three states: In-Control, Warning, and Out-of-Control. Second, a novel method is proposed to visualize and characterize the temporal changes of data based on the projection of a non-parametric information-geometric statistical manifold of time windows. This projection facilitates the exploration of temporal trends using the proposed IGT-plot and, by means of unsupervised learning methods, discovering conceptually-related temporal subgroups. Methods are evaluated using real and simulated data based on the National Hospital Discharge Survey (NHDS) dataset.},
  langid = {english},
  keywords = {Biomedical data,Change detection,Data quality,Information geometry,Information theory,Visual analytics},
  file = {D\:\\work\\literature\\library\\Sáez et al. - 2015 - Probabilistic change detection and visualization m.pdf}
}

@article{schafer2019,
  title = {{{TEASER}}: {{Early}} and {{Accurate Time Series Classification}}},
  shorttitle = {{{TEASER}}},
  author = {Sch{\"a}fer, P. and Leser, U.},
  year = {2019},
  month = aug,
  journal = {arXiv:1908.03405 [cs, stat]},
  eprint = {1908.03405},
  primaryclass = {cs, stat},
  urldate = {2019-09-18},
  abstract = {Early time series classification (eTSC) is the problem of classifying a time series after as few measurements as possible with the highest possible accuracy. The most critical issue of any eTSC method is to decide when enough data of a time series has been seen to take a decision: Waiting for more data points usually makes the classification problem easier but delays the time in which a classification is made; in contrast, earlier classification has to cope with less input data, often leading to inferior accuracy. The state-of-the-art eTSC methods compute a fixed optimal decision time assuming that every times series has the same defined start time (like turning on a machine). However, in many real-life applications measurements start at arbitrary times (like measuring heartbeats of a patient), implying that the best time for taking a decision varies heavily between time series. We present TEASER, a novel algorithm that models eTSC as a two two-tier classification problem: In the first tier, a classifier periodically assesses the incoming time series to compute class probabilities. However, these class probabilities are only used as output label if a second-tier classifier decides that the predicted label is reliable enough, which can happen after a different number of measurements. In an evaluation using 45 benchmark datasets, TEASER is two to three times earlier at predictions than its competitors while reaching the same or an even higher classification accuracy. We further show TEASER's superior performance using real-life use cases, namely energy monitoring, and gait detection.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\work\\literature\\library\\Schäfer and Leser - 2019 - TEASER Early and Accurate Time Series Classificat.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\SZYEZ6ZD\\1908.html}
}

@article{schlimmer1986,
  title = {Incremental Learning from Noisy Data},
  author = {Schlimmer, Jeffrey C. and Granger, Richard H.},
  year = {1986},
  journal = {Machine Learning},
  volume = {1},
  number = {3},
  pages = {317--354},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00116895},
  urldate = {2018-03-21},
  langid = {english},
  keywords = {Drift Generator,STAGGER},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Schlimmer and Granger - 1986 - Incremental learning from noisy data.pdf}
}

@misc{schlimmerjeff1987,
  title = {{{UCI Machine Learning Repository}}: {{Mushroom Data Set}}},
  author = {{Schlimmer,Jeff}},
  year = {1987},
  month = apr,
  urldate = {2018-11-08},
  howpublished = {https://archive.ics.uci.edu/ml/datasets/Mushroom},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\3ABZINF5\\Mushroom.html}
}

@inproceedings{scholz2005,
  title = {An {{Ensemble Classifier}} for {{Drifting Concepts}}},
  booktitle = {In {{Proceedings}} of the {{Second International Workshop}} on {{Knowledge Discovery}} in {{Data Streams}}},
  author = {Scholz, Martin and Klinkenberg, Ralf},
  year = {2005},
  pages = {53--64},
  abstract = {This paper proposes a boosting-like method to train a classifier  ensemble from data streams. It naturally adapts to concept drift and  allows to quantify the drift in terms of its base learners. The algorithm  is empirically shown to outperform learning algorithms that ignore concept  drift. It performs no worse than advanced adaptive time window  and example selection strategies that store all the data and are thus not  suited for mining massive streams.},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Scholz and Klinkenberg - 2005 - An Ensemble Classifier for Drifting Concepts.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\4DWJYJTM\\summary.html}
}

@inproceedings{schwaighofer2007,
  title = {Structure {{Learning}} with {{Nonparametric Decomposable Models}}},
  booktitle = {Artificial {{Neural Networks}} \textendash{} {{ICANN}} 2007},
  author = {Schwaighofer, Anton and Dejori, Math{\"a}us and Tresp, Volker and Stetter, Martin},
  editor = {{de S{\'a}}, Joaquim Marques and Alexandre, Lu{\'i}s A. and Duch, W{\l}odzis{\l}aw and Mandic, Danilo},
  year = {2007},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {119--128},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-74690-4\_13},
  abstract = {We present a novel approach to structure learning for graphical models. By using nonparametric estimates to model clique densities in decomposable models, both discrete and continuous distributions can be handled in a unified framework. Also, consistency of the underlying probabilistic model is guaranteed. Model selection is based on predictive assessment, with efficient algorithms that allow fast greedy forward and backward selection within the class of decomposable models. We show the validity of this structure learning approach on toy data, and on two large sets of gene expression data.},
  isbn = {978-3-540-74690-4},
  langid = {english},
  keywords = {Chordal Graph,Clique Tree,Model Score,Nonparametric Density Estimate,Structure Learning},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Schwaighofer et al. - 2007 - Structure Learning with Nonparametric Decomposable.pdf}
}

@article{scutari2010,
  title = {Learning {{Bayesian Networks}} with the Bnlearn {{R Package}}},
  author = {Scutari, Marco},
  year = {2010},
  month = jul,
  journal = {Journal of Statistical Software},
  volume = {35},
  pages = {1--22},
  issn = {1548-7660},
  doi = {10.18637/jss.v035.i03},
  urldate = {2022-06-22},
  abstract = {bnlearn is an R package (R Development Core Team 2010) which includes several algorithms for learning the structure of Bayesian networks with either discrete or continuous variables. Both constraint-based and score-based algorithms are implemented, and can use the functionality provided by the snow package (Tierney et al. 2008) to improve their performance via parallel computing. Several network scores and conditional independence algorithms are available for both the learning algorithms and independent use. Advanced plotting options are provided by the Rgraphviz package (Gentry et al. 2010).},
  copyright = {Copyright (c) 2009 Marco Scutari},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Scutari - 2010 - Learning Bayesian Networks with the bnlearn R Pack.pdf}
}

@inproceedings{sebastiao2007,
  title = {Change {{Detection}} in {{Learning Histograms}} from {{Data Streams}}},
  booktitle = {Progress in {{Artificial Intelligence}}},
  author = {Sebasti{\~a}o, Raquel and Gama, Jo{\~a}o},
  editor = {Neves, Jos{\'e} and Santos, Manuel Filipe and Machado, Jos{\'e} Manuel},
  year = {2007},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {112--123},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-77002-2\_10},
  abstract = {In this paper we study the problem of constructing histograms from high-speed time-changing data streams. Learning in this context requires the ability to process examples once at the rate they arrive, maintaining a histogram consistent with the most recent data, and forgetting out-date data whenever a change in the distribution is detected. To construct histogram from high-speed data streams we use the two layer structure used in the Partition Incremental Discretization (PiD) algorithm. Our contribution is a new method to detect whenever a change in the distribution generating examples occurs. The base idea consists of monitoring distributions from two different time windows: the reference time window, that reflects the distribution observed in the past; and the current time window reflecting the distribution observed in the most recent data. We compare both distributions and signal a change whenever they are greater than a threshold value, using three different methods: the Entropy Absolute Difference, the Kullback-Leibler divergence and the Cosine Distance. The experimental results suggest that Kullback-Leibler divergence exhibit high probability in change detection, faster detection rates, with few false positives alarms.},
  isbn = {978-3-540-77002-2},
  langid = {english},
  keywords = {Change Detection,Concept Drift,Current Window,Data Stream,Exploratory Data Analysis},
  file = {D\:\\work\\literature\\library\\Sebastião and Gama - 2007 - Change Detection in Learning Histograms from Data .pdf}
}

@inproceedings{sebastiao2010,
  title = {Monitoring {{Incremental Histogram Distribution}} for {{Change Detection}} in {{Data Streams}}},
  booktitle = {Knowledge {{Discovery}} from {{Sensor Data}}},
  author = {Sebasti{\~a}o, Raquel and Gama, Jo{\~a}o and Rodrigues, Pedro Pereira and Bernardes, Jo{\~a}o},
  editor = {Gaber, Mohamed Medhat and Vatsavai, Ranga Raju and Omitaomu, Olufemi A. and Gama, Jo{\~a}o and Chawla, Nitesh V. and Ganguly, Auroop R.},
  year = {2010},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {25--42},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-12519-5\_2},
  abstract = {Histograms are a common technique for density estimation and they have been widely used as a tool in exploratory data analysis. Learning histograms from static and stationary data is a well known topic. Nevertheless, very few works discuss this problem when we have a continuous flow of data generated from dynamic environments.},
  isbn = {978-3-642-12519-5},
  langid = {english},
  keywords = {Adaptive Cumulative Windows,Change detection,Data streams,Learning histograms,Machine learning,Monitoring data distribution},
  file = {D\:\\work\\literature\\library\\Sebastião et al. - 2010 - Monitoring Incremental Histogram Distribution for .pdf}
}

@inproceedings{seker2017,
  title = {Linear-{{Time Generation}} of {{Random Chordal Graphs}}},
  booktitle = {Algorithms and {{Complexity}}},
  author = {{\c S}eker, Oylum and Heggernes, Pinar and Ekim, T{\i}naz and Ta{\c s}k{\i}n, Z. Caner},
  editor = {Fotakis, Dimitris and Pagourtzis, Aris and Paschos, Vangelis Th.},
  year = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {442--453},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-57586-5\_37},
  abstract = {Chordal graphs form one of the most well studied graph classes. Several graph problems that are NP-hard in general become solvable in polynomial time on chordal graphs, whereas many others remain NP-hard. For a large group of problems among the latter, approximation algorithms, parameterized algorithms, and algorithms with moderately exponential or sub-exponential running time have been designed. Chordal graphs have also gained increasing interest during the recent years in the area of enumeration algorithms. Being able to test these algorithms on instances of chordal graphs is crucial for understanding the concepts of tractability of hard problems on graph classes. Unfortunately, only few published papers give algorithms for generating chordal graphs. Even in these papers, only very few methods aim for generating a large variety of chordal graphs. Surprisingly, none of these methods is based on the ``intersection of subtrees of a tree'' characterization of chordal graphs. In this paper, we give an algorithm for generating chordal graphs, based on the characterization that a graph is chordal if and only if it is the intersection graph of subtrees of a tree. The complexity of our algorithm is linear in the size of the produced graph. We give test results to show the variety of chordal graphs that are produced, and we compare these results to existing results.},
  isbn = {978-3-319-57586-5},
  langid = {english},
  file = {D\:\\work\\literature\\library\\Şeker et al. - 2017 - Linear-Time Generation of Random Chordal Graphs.pdf}
}

@article{seker2018,
  title = {Generation of Random Chordal Graphs Using Subtrees of a Tree},
  author = {{\c S}eker, Oylum and Heggernes, Pinar and Ekim, T{\i}naz and Ta{\c s}k{\i}n, Z. Caner},
  year = {2018},
  month = oct,
  urldate = {2021-12-07},
  abstract = {Chordal graphs form one of the most studied graph classes. Several graph problems that are NP-hard in general become solvable in polynomial time on chordal graphs, whereas many others remain NP-hard. For a large group of problems among the latter, approximation algorithms, parameterized algorithms, and algorithms with moderately exponential or sub-exponential running time have been designed. Chordal graphs have also gained increasing interest during the recent years in the area of enumeration algorithms. Being able to test these algorithms on instances of chordal graphs is crucial for understanding the concepts of tractability of hard problems on graph classes. Unfortunately, only few studies give algorithms for generating chordal graphs. Even in these papers, only very few methods aim for generating a large variety of chordal graphs. Surprisingly, none of these methods is directly based on the "intersection of subtrees of a tree" characterization of chordal graphs. In this paper, we give an algorithm for generating chordal graphs, based on the characterization that a graph is chordal if and only if it is the intersection graph of subtrees of a tree. Upon generating a random host tree, we give and test various methods that generate subtrees of the host tree. We compare our methods to one another and to existing ones for generating chordal graphs. Our experiments show that one of our methods is able to generate the largest variety of chordal graphs in terms of maximal clique sizes. Moreover, two of our subtree generation methods result in an overall complexity of our generation algorithm that is the best possible time complexity for a method generating the entire node set of subtrees in a "intersection of subtrees of a tree" representation. The instances corresponding to the results presented in this paper, and also a set of relatively small-sized instances are made available online.},
  langid = {english},
  file = {D\:\\work\\literature\\library\\Şeker et al. - 2018 - Generation of random chordal graphs using subtrees.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\ZMN86I25\\1810.html}
}

@article{seroussi1994,
  title = {An Algorithm Directly Finding the {{K}} Most Probable Configurations in {{Bayesian}} Networks},
  author = {Seroussi, B. and Golmard, J. L.},
  year = {1994},
  month = oct,
  journal = {International Journal of Approximate Reasoning},
  volume = {11},
  number = {3},
  pages = {205--233},
  issn = {0888-613X},
  doi = {10.1016/0888-613X(94)90031-0},
  urldate = {2021-09-26},
  abstract = {This article describes an algorithm that solves the problem of finding the K most probable configurations of a Bayesian network, given certain evidence, for any K, and for any type of network, including multiply connected networks. This algorithm is based on the compilation of the initial network into a junction tree. After a description of the preliminary steps needed to get a junction tree, namely, the moralization, the triangulation, and the ordering of cliques, we explain how the incorporation of evidence is processed. The principle of the algorithm is to visit in a bottom-up way each clique of the junction tree, and to store, at each level, the K most probable configurations of the deeper levels. The complexity of the algorithm is computed and shown to be mainly dependent of the maximum clique size, as it is for Bayesian updating algorithms using the junction tree internal representation. The classic example ASIA is used to illustrate the detailed execution of the algorithm with K = 3. Finally, our method is compared with related work.},
  langid = {english},
  keywords = {Bayesian network,junction tree,multiple diagnosis,probabilistic inference,uncertain reasoning},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Seroussi and Golmard - 1994 - An algorithm directly finding the K most probable .pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\ULMIVZNE\\0888613X94900310.html}
}

@article{sethi2017,
  ids = {sethiReliableDetectionConcept2017},
  title = {On the Reliable Detection of Concept Drift from Streaming Unlabeled Data},
  author = {Sethi, Tegjyot Singh and Kantardzic, Mehmed},
  year = {2017},
  month = oct,
  journal = {Expert Systems with Applications: An International Journal},
  volume = {82},
  number = {C},
  pages = {77--99},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2017.04.008},
  urldate = {2020-02-03},
  abstract = {New classifier-independent, dynamic, unsupervised approach for detecting concept drift.Reduced number of false alarms and increased relevance of drift detection.Results comparable to supervised approaches, which require fully labeled streams.Our approach generalizes the notion of margin density, as a signal to detect drifts.Experiments on cybersecurity datasets, show efficacy for detecting adversarial drifts. Classifiers deployed in the real world operate in a dynamic environment, where the data distribution can change over time. These changes, referred to as concept drift, can cause the predictive performance of the classifier to drop over time, thereby making it obsolete. To be of any real use, these classifiers need to detect drifts and be able to adapt to them, over time. Detecting drifts has traditionally been approached as a supervised task, with labeled data constantly being used for validating the learned model. Although effective in detecting drifts, these techniques are impractical, as labeling is a difficult, costly and time consuming activity. On the other hand, unsupervised change detection techniques are unreliable, as they produce a large number of false alarms. The inefficacy of the unsupervised techniques stems from the exclusion of the characteristics of the learned classifier, from the detection process. In this paper, we propose the Margin Density Drift Detection (MD3) algorithm, which tracks the number of samples in the uncertainty region of a classifier, as a metric to detect drift. The MD3 algorithm is a distribution independent, application independent, model independent, unsupervised and incremental algorithm for reliably detecting drifts from data streams. Experimental evaluation on 6 drift induced datasets and 4 additional datasets from the cybersecurity domain demonstrates that the MD3 approach can reliably detect drifts, with significantly fewer false alarms compared to unsupervised feature based drift detectors. At the same time, it produces performance comparable to that of a fully labeled drift detector. The reduced false alarms enables the signaling of drifts only when they are most likely to affect classification performance. As such, the MD3 approach leads to a detection scheme which is credible, label efficient and general in its applicability.},
  keywords = {Concept drift,Cybersecurity,Ensemble,Margin density,Streaming data,Unlabeled},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Sethi and Kantardzic - 2017 - On the reliable detection of concept drift from st.pdf;D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Sethi and Kantardzic - 2017 - On the reliable detection of concept drift from st2.pdf}
}

@article{shafer1990,
  title = {Probability Propagation},
  author = {Shafer, Glenn R. and Shenoy, Prakash P.},
  year = {1990},
  month = mar,
  journal = {Annals of Mathematics and Artificial Intelligence},
  volume = {2},
  number = {1},
  pages = {327--351},
  issn = {1573-7470},
  doi = {10.1007/BF01531015},
  urldate = {2019-10-20},
  abstract = {In this paper we give a simple account of local computation of marginal probabilities when the joint probability distribution is given in factored form and the sets of variables involved in the factors form a hypertree. Previous expositions of such local computation have emphasized conditional probability. We believe this emphasis is misplaced. What is essential to local computation is a factorization. It is not essential that this factorization be interpreted in terms of conditional probabilities. The account given here avoids the divisions required by conditional probabilities and generalizes readily to alternative measures of subjective probability, such as Dempster-Shafer or Spohnian belief functions.},
  langid = {english},
  keywords = {array,construction sequence,hypertree,hypertree cover,local computation,Markov tree,parallel processing,potential,Probability propagation,sum-product belief propagation},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Shafer and Shenoy - 1990 - Probability propagation.pdf}
}

@article{shah2021,
  title = {A {{Computationally Efficient Method}} for {{Learning Exponential Family Distributions}}},
  author = {Shah, Abhin and Shah, Devavrat and Wornell, Gregory W.},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.15397 [cs]},
  eprint = {2110.15397},
  primaryclass = {cs},
  urldate = {2022-01-26},
  abstract = {We consider the question of learning the natural parameters of a \$k\$ parameter minimal exponential family from i.i.d. samples in a computationally and statistically efficient manner. We focus on the setting where the support as well as the natural parameters are appropriately bounded. While the traditional maximum likelihood estimator for this class of exponential family is consistent, asymptotically normal, and asymptotically efficient, evaluating it is computationally hard. In this work, we propose a computationally efficient estimator that is consistent as well as asymptotically normal under mild conditions. We provide finite sample guarantees to achieve an (\$\textbackslash ell\_2\$) error of \$\textbackslash alpha\$ in the parameter estimation with sample complexity \$O(\textbackslash mathrm\{poly\}(k/\textbackslash alpha))\$ and computational complexity \$\{O\}(\textbackslash mathrm\{poly\}(k/\textbackslash alpha))\$. To establish these results, we show that, at the population level, our method can be viewed as the maximum likelihood estimation of a re-parameterized distribution belonging to the same class of exponential family.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Shah et al. - 2021 - A Computationally Efficient Method for Learning Ex.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\2N2H88FA\\2110.html}
}

@article{shaker2015,
  title = {Recovery Analysis for Adaptive Learning from Non-Stationary Data Streams: {{Experimental}} Design and Case Study},
  shorttitle = {Recovery Analysis for Adaptive Learning from Non-Stationary Data Streams},
  author = {Shaker, Ammar and H{\"u}llermeier, Eyke},
  year = {2015},
  month = feb,
  journal = {Neurocomputing},
  volume = {150},
  pages = {250--264},
  issn = {09252312},
  doi = {10.1016/j.neucom.2014.09.076},
  urldate = {2020-11-10},
  langid = {english},
  file = {D\:\\work\\literature\\library\\Shaker and Hüllermeier - 2015 - Recovery analysis for adaptive learning from non-s.pdf}
}

@article{shaker2022,
  title = {Learning to {{Transfer}} with von {{Neumann Conditional Divergence}}},
  author = {Shaker, Ammar and Yu, Shujian and {O{\~n}oro-Rubio}, Daniel},
  year = {2022},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {8},
  pages = {8231--8239},
  issn = {2374-3468},
  doi = {10.1609/aaai.v36i8.20797},
  urldate = {2022-07-31},
  abstract = {The similarity of feature representations plays a pivotal role in the success of problems related to domain adaptation. Feature similarity includes both the invariance of marginal distributions and the closeness of conditional distributions given the desired response y (e.g., class labels). Unfortunately, traditional methods always learn such features without fully taking into consideration the information in y, which in turn may lead to a mismatch of the conditional distributions or the mixup of discriminative structures underlying data distributions. In this work, we introduce the recently proposed von Neumann conditional divergence to improve the transferability across multiple domains. We show that this new divergence is differentiable and eligible to easily quantify the functional dependence between features and y. Given multiple source tasks, we integrate this divergence to capture discriminative information in y and design novel learning objectives assuming those source tasks are observed either simultaneously or sequentially. In both scenarios, we obtain favorable performance against state-of-the-art methods in terms of smaller generalization error on new tasks and less catastrophic forgetting on source tasks (in the sequential setup).},
  copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {Machine Learning (ML)},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Shaker et al. - 2022 - Learning to Transfer with von Neumann Conditional .pdf}
}

@article{shannon1948,
  title = {A Mathematical Theory of Communication},
  author = {Shannon, C. E.},
  year = {1948},
  month = jul,
  journal = {The Bell System Technical Journal},
  volume = {27},
  number = {3},
  pages = {379--423},
  issn = {0005-8580},
  doi = {10.1002/j.1538-7305.1948.tb01338.x},
  abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist1 and Hartley2 on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Shannon - 1948 - A mathematical theory of communication.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\G7ZD6NH8\\6773024.html}
}

@incollection{shenoy1990,
  title = {Axioms for {{Probability}} and {{Belief-Function Propagation}}},
  booktitle = {Machine {{Intelligence}} and {{Pattern Recognition}}},
  author = {Shenoy, Prakash P. and Shafer, Glenn},
  editor = {Shachter, Ross D. and Levitt, Tod S. and Kanal, Laveen N. and Lemmer, John F.},
  year = {1990},
  month = jan,
  series = {Uncertainty in {{Artificial Intelligence}}},
  volume = {9},
  pages = {169--198},
  publisher = {{North-Holland}},
  doi = {10.1016/B978-0-444-88650-7.50019-6},
  urldate = {2019-10-20},
  abstract = {In this paper, we describe an abstract framework and axioms under which exact local computation of marginals is possible. The primitive objects of the framework are variables and valuations. The primitive operators of the framework are combination and marginalization. These operate on valuations. We state three axioms for these operators and we derive the possibility of local computation from the axioms. Next, we describe a propagation scheme for computing marginals of a valuation when we have a factorization of the valuation on a hypertree. Finally we show how the problem of computing marginals of joint probability distributions and joint belief functions fits the general framework.},
  langid = {english},
  keywords = {sum-product belief propagation},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Shenoy and Shafer - 1990 - Axioms for Probability and Belief-Function Propaga.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\ASKF3ADY\\B9780444886507500196.html}
}

@article{sibson1969,
  title = {Information Radius},
  author = {Sibson, Robin},
  year = {1969},
  month = jun,
  journal = {Zeitschrift f\"ur Wahrscheinlichkeitstheorie und Verwandte Gebiete},
  volume = {14},
  number = {2},
  pages = {149--160},
  issn = {1432-2064},
  doi = {10.1007/BF00537520},
  urldate = {2022-08-14},
  abstract = {This paper is an account of a new method of constructing measures of divergence between probability measures; the new divergence measures so constructed are called information radius measures. They are information-theoretic in character, and are based on the work of R\'enyi [8] and Csisz\'ar [2, 3]. The divergence measure K1 can be used for the measurement of dissimilarity in numerical taxonomy, and its application to this field is discussed in Jardine and Sibson [5]; it was this application which originally motivated the study of information radius. Other forms of information radius are related to the variation distance, and the normal information radius discussed in \textsection{} 3 is related to Mahalanobis' D2 Statistic. This paper is in part intended to lay the mathematical foundations for [5], but because information radius appears to be of some general interest, the investigation of its properties is here carried further than is needed for the applications discussed in [5].},
  langid = {english},
  keywords = {Divergence Measure,Mathematical Biology,Probability Measure,Probability Theory,Stochastic Process},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Sibson - 1969 - Information radius.pdf}
}

@book{silander2008,
  title = {Factorized {{Normalized Maximum Likelihood Criterion}} for {{Learning Bayesian Network Structures}}},
  author = {Silander, Tomi and Roos, Teemu and Kontkanen, Petri and Myllym{\"a}ki, Petri},
  year = {2008},
  abstract = {This paper introduces a new scoring criterion, factorized normalized maximum likelihood, for learning Bayesian network structures. The proposed scoring criterion requires no parameter tuning, and it is decomposable and asymptotically consistent. We compare the new scoring criterion to other scoring criteria and describe its practical implementation. Empirical tests confirm its good performance.},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Silander et al. - 2008 - Factorized Normalized Maximum Likelihood Criterion.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\246W3LJM\\summary.html}
}

@inproceedings{silander2018,
  title = {Quotient {{Normalized Maximum Likelihood Criterion}} for {{Learning Bayesian Network Structures}}},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Silander, Tomi and {Lepp{\"a}-aho}, Janne and J{\"a}{\"a}saari, Elias and Roos, Teemu},
  year = {2018},
  month = mar,
  pages = {948--957},
  urldate = {2019-08-28},
  abstract = {We introduce an information theoretic criterion for Bayesian network structure learning which we call quotient normalized maximum likelihood (qNML). In contrast to the closely related factorized no...},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Silander et al. - 2018 - Quotient Normalized Maximum Likelihood Criterion f.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\ITQ2S4GX\\silander18a.html}
}

@misc{slatedavidj.1991,
  title = {{{UCI Machine Learning Repository}}: {{Letter Recognition Data Set}}},
  author = {{Slate, David J.}},
  year = {1991},
  month = jan,
  urldate = {2018-11-08},
  howpublished = {https://archive.ics.uci.edu/ml/datasets/Letter+Recognition},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\82Q2ZI4C\\Letter+Recognition.html}
}

@article{smith1989,
  title = {Influence {{Diagrams}} for {{Statistical Modelling}}},
  author = {Smith, J. Q.},
  year = {1989},
  journal = {The Annals of Statistics},
  volume = {17},
  number = {2},
  eprint = {2241577},
  eprinttype = {jstor},
  pages = {654--672},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364},
  urldate = {2022-06-23},
  abstract = {A directed graph with identified nodes is defined to represent a set of conditional independence (c.i.) statements. It is shown how new c.i. statements can be read from the graph of an influence diagram and results of Howard and Matheson are rigorised and generalized. A new decomposition theorem, analogous to Kiiveri, Speed and Carlin and requiring no positivity condition, is proved. Connections between influence diagrams and Markov field networks are made explicit. Because all results depend on only three properties of c.i., the theorems proved here can be restated as theorems about other structures like second order processes.},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\55GRAHZ3\\Smith - 1989 - Influence Diagrams for Statistical Modelling.pdf}
}

@article{song2018,
  title = {{{iProt-Sub}}: A Comprehensive Package for Accurately Mapping and Predicting Protease-Specific Substrates and Cleavage Sites},
  shorttitle = {{{iProt-Sub}}},
  author = {Song, Jiangning and Wang, Yanan and Li, Fuyi and Akutsu, Tatsuya and Rawlings, Neil D and Webb, Geoffrey I and Chou, Kuo-Chen},
  year = {2018},
  month = apr,
  journal = {Briefings in Bioinformatics},
  issn = {1467-5463, 1477-4054},
  doi = {10.1093/bib/bby028},
  urldate = {2018-12-15},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Song et al. - 2018 - iProt-Sub a comprehensive package for accurately .pdf}
}

@article{song2018a,
  title = {{{PREvaIL}}, an Integrative Approach for Inferring Catalytic Residues Using Sequence, Structural, and Network Features in a Machine-Learning Framework},
  author = {Song, Jiangning and Li, Fuyi and Takemoto, Kazuhiro and Haffari, Gholamreza and Akutsu, Tatsuya and Chou, Kuo-Chen and Webb, Geoffrey I.},
  year = {2018},
  month = apr,
  journal = {Journal of Theoretical Biology},
  volume = {443},
  pages = {125--137},
  issn = {00225193},
  doi = {10.1016/j.jtbi.2018.01.023},
  urldate = {2018-12-15},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Song et al. - 2018 - PREvaIL, an integrative approach for inferring cat.pdf}
}

@inproceedings{sousa2016,
  title = {First {{Principle Models Based Dataset Generation}} for {{Multi-Target Regression}} and {{Multi-Label Classification Evaluation}}},
  booktitle = {{{STREAMEVOLV-2016}}},
  author = {Sousa, Ricardo and Gama, Joao},
  year = {2016},
  month = sep,
  abstract = {Machine Learning and Data Mining research strongly depend on the quality and quantity of the real world datasets for the evaluation stages of the developing methods. In the context of the emerging Online Multi-Target Regression and Multi-Label Classification methodologies, datasets present new characteristics that require specific testing and represent new challenges. The first difficulty found in evaluation is the reduced amount of examples caused by data damage, privacy preservation or high cost of acquirement. Secondly, few data events of interest such as data changes are difficult to find in the datasets of specific domains, since these events naturally scarce.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Sousa and Gama - 2016 - First Principle Models Based Dataset Generation fo.pdf}
}

@incollection{souza2015,
  title = {Data {{Stream Classification Guided}} by {{Clustering}} on {{Nonstationary Environments}} and {{Extreme Verification Latency}}},
  booktitle = {Proceedings of the 2015 {{SIAM International Conference}} on {{Data Mining}}},
  author = {Souza, V. and Silva, D. and Gama, J. and Batista, G.},
  year = {2015},
  month = jun,
  series = {Proceedings},
  pages = {873--881},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611974010.98},
  urldate = {2018-05-15},
  abstract = {Data stream classification algorithms for nonstationary environments frequently assume the availability of class labels, instantly or with some lag after the classification. However, certain applications, mainly those related to sensors and robotics, involve high costs to obtain new labels during the classification phase. Such a scenario in which the actual labels of processed data are never available is called extreme verification latency. Extreme verification latency requires new classification methods capable of adapting to possible changes over time without external supervision. This paper presents a fast, simple, intuitive and accurate algorithm to classify nonstationary data streams in an extreme verification latency scenario, namely Stream Classification Algorithm Guided by Clustering \textendash{} SCARGC. Our method consists of a clustering followed by a classification step applied repeatedly in a closed loop fashion. We show in several classification tasks evaluated in synthetic and real data that our method is faster and more accurate than the state-of-the-art.},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Souza et al. - 2015 - Data Stream Classification Guided by Clustering on.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\4KRFJBFP\\1.9781611974010.html}
}

@article{souza2021,
  title = {Efficient Unsupervised Drift Detector for Fast and High-Dimensional Data Streams},
  author = {Souza, Vinicius M. A. and Parmezan, Antonio R. S. and Chowdhury, Farhan A. and Mueen, Abdullah},
  year = {2021},
  month = jun,
  journal = {Knowledge and Information Systems},
  volume = {63},
  number = {6},
  pages = {1497--1527},
  issn = {0219-3116},
  doi = {10.1007/s10115-021-01564-6},
  urldate = {2022-08-02},
  abstract = {Stream mining considers the online arrival of examples at high speed and the possibility of changes in its descriptive features or class definitions compared with past knowledge (i.e., concept drifts). The fast detection of drifts is essential to keep the predictive model updated and stable in changing environments. For many applications, such as those related to smart sensors, the high number of features is an additional challenge in terms of memory and time for stream processing. This paper presents an unsupervised and model-independent concept drift detector suitable for high-speed and high-dimensional data streams. We propose a straightforward two-dimensional data representation that allows the faster processing of datasets with a large number of examples and dimensions. We developed an adaptive drift detector on this visual representation that is efficient for fast streams with thousands of features and is accurate as existing costly methods that perform various statistical tests considering each feature individually. Our method achieves better performance measured by execution time and accuracy in classification problems for different types of drifts. The experimental evaluation considering synthetic and real data demonstrates the method's versatility in several domains, including entomology, medicine, and transportation systems.},
  langid = {english},
  keywords = {Concept drift,Data stream,Unsupervised drift detector},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Souza et al. - 2021 - Efficient unsupervised drift detector for fast and.pdf}
}

@article{spitzer1971,
  title = {Markov {{Random Fields}} and {{Gibbs Ensembles}}},
  author = {Spitzer, Frank},
  year = {1971},
  journal = {The American Mathematical Monthly},
  volume = {78},
  number = {2},
  eprint = {2317621},
  eprinttype = {jstor},
  pages = {142--154},
  publisher = {{Mathematical Association of America}},
  issn = {0002-9890},
  doi = {10.2307/2317621},
  urldate = {2022-06-28},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Spitzer - 1971 - Markov Random Fields and Gibbs Ensembles.pdf}
}

@techreport{stanley2003,
  title = {Learning {{Concept Drift}} with a {{Committee}} of {{Decision Trees}}},
  author = {Stanley, Kenneth O.},
  year = {2003},
  urldate = {2018-05-23},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Stanley - 2003 - Learning Concept Drift with a Committee of Decisio.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\SARDA7MU\\pub-view.html}
}

@inproceedings{stiglic2011,
  title = {Interpretability of {{Sudden Concept Drift}} in {{Medical Informatics Domain}}},
  booktitle = {2011 {{IEEE}} 11th {{International Conference}} on {{Data Mining Workshops}}},
  author = {Stiglic, Gregor and Kokol, Peter},
  year = {2011},
  month = dec,
  pages = {609--613},
  issn = {2375-9259},
  doi = {10.1109/ICDMW.2011.104},
  abstract = {Concept drift is usually met in rapidly changing environments, especially in sequential data classification, where different types of concept drift occur on regular basis. This paper presents an approach to dynamic visualization of sequential data characteristics aiming to improve the comprehensibility of concept drifts that result in significant change of classification performance. The proposed approach is applied to sequential multi-label hospital discharge dataset containing diagnosis information for more than two million patients. Our experimental results demonstrate visualization of the anomalies in diagnosis coding through time that can explain the differences in sudden changes of class distribution or classification performance.},
  keywords = {concept drift,Data visualization,Discharges,Diseases,Hospitals,Humans,interpretability of classifiers,Kidney,Medical diagnostic imaging,multi-label classification},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Stiglic and Kokol - 2011 - Interpretability of Sudden Concept Drift in Medica.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\XJCB6ANB\\6137436.html}
}

@inproceedings{street2001,
  title = {A {{Streaming Ensemble Algorithm}} ({{SEA}}) for {{Large-scale Classification}}},
  booktitle = {Proceedings of the {{Seventh ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Street, W. Nick and Kim, YongSeog},
  year = {2001},
  series = {{{KDD}} '01},
  pages = {377--382},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/502512.502568},
  urldate = {2018-03-21},
  abstract = {Ensemble methods have recently garnered a great deal of attention in the machine learning community. Techniques such as Boosting and Bagging have proven to be highly effective but require repeated resampling of the training data, making them inappropriate in a data mining context. The methods presented in this paper take advantage of plentiful data, building separate classifiers on sequential chunks of training points. These classifiers are combined into a fixed-size ensemble using a heuristic replacement strategy. The result is a fast algorithm for large-scale or streaming data that classifies as well as a single decision tree built on all the data, requires approximately constant memory, and adjusts quickly to concept drift.},
  isbn = {978-1-58113-391-2},
  keywords = {ensemble classification,streaming data},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Street and Kim - 2001 - A Streaming Ensemble Algorithm (SEA) for Large-sca.pdf}
}

@article{sun2018,
  title = {Concept {{Drift Adaptation}} by {{Exploiting Historical Knowledge}}},
  author = {Sun, Yu and Tang, Ke and Zhu, Zexuan and Yao, Xin},
  year = {2018},
  month = oct,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {29},
  number = {10},
  pages = {4822--4832},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2017.2775225},
  urldate = {2020-04-02},
  keywords = {concept drift},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Sun et al. - 2018 - Concept Drift Adaptation by Exploiting Historical .pdf}
}

@article{sundberg1975,
  title = {Some {{Results}} about {{Decomposable}} ({{Or Markov-Type}}) {{Models}} for {{Multidimensional Contingency Tables}}: {{Distribution}} of {{Marginals}} and {{Partitioning}} of {{Tests}}},
  shorttitle = {Some {{Results}} about {{Decomposable}} ({{Or Markov-Type}}) {{Models}} for {{Multidimensional Contingency Tables}}},
  author = {Sundberg, Rolf},
  year = {1975},
  journal = {Scandinavian Journal of Statistics},
  volume = {2},
  number = {2},
  eprint = {4615579},
  eprinttype = {jstor},
  pages = {71--79},
  issn = {0303-6898},
  urldate = {2020-01-18},
  abstract = {For the so-called decomposable or Markov-type models for contingency tables of arbitrary dimension it is shown that the probability of a minimal set of fitted marginals may be expressed in a closed form, analogous to that of the maximum likelihood estimate. As a consequence a closed form expression can be given for the so-called exact test statistic in a test of a decomposable model. Finally a conjecture by A. H. Andersen is proved, stating that when the models are decomposable the exact test statistic and the likelihood ratio may both be completely factorized into exact test statistics and likelihood ratios, respectively, for testing homogeneity in one-dimensional or independence in two-dimensional contingency tables.},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Sundberg - 1975 - Some Results about Decomposable (Or Markov-Type) M.pdf}
}

@article{sur2019,
  title = {A Modern Maximum-Likelihood Theory for High-Dimensional Logistic Regression},
  author = {Sur, Pragya and Cand{\`e}s, Emmanuel J.},
  year = {2019},
  month = jul,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {29},
  pages = {14516--14525},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1810420116},
  urldate = {2020-01-10},
  abstract = {Students in statistics or data science usually learn early on that when the sample size n is large relative to the number of variables p, fitting a logistic model by the method of maximum likelihood produces estimates that are consistent and that there are well-known formulas that quantify the variability of these estimates which are used for the purpose of statistical inference. We are often told that these calculations are approximately valid if we have 5 to 10 observations per unknown parameter. This paper shows that this is far from the case, and consequently, inferences produced by common software packages are often unreliable. Consider a logistic model with independent features in which n and p become increasingly large in a fixed ratio. We prove that (i) the maximum-likelihood estimate (MLE) is biased, (ii) the variability of the MLE is far greater than classically estimated, and (iii) the likelihood-ratio test (LRT) is not distributed as a {$\chi$}2. The bias of the MLE yields wrong predictions for the probability of a case based on observed values of the covariates. We present a theory, which provides explicit expressions for the asymptotic bias and variance of the MLE and the asymptotic distribution of the LRT. We empirically demonstrate that these results are accurate in finite samples. Our results depend only on a single measure of signal strength, which leads to concrete proposals for obtaining accurate inference in finite samples through the estimate of this measure.},
  copyright = {Copyright \textcopyright{} 2019 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
  langid = {english},
  pmid = {31262828},
  keywords = {high-dimensional inference,likelihood-ratio test,logistic regression,maximum-likelihood estimate},
  file = {D\:\\work\\literature\\library\\Sur and Candès - 2019 - A modern maximum-likelihood theory for high-dimens.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\GJ5CB6LH\\14516.html}
}

@article{swaszek2000,
  title = {Generating Probabilities with a Specified Entropy},
  author = {Swaszek, Peter F and Wali, Sidharth},
  year = {2000},
  month = mar,
  journal = {Journal of the Franklin Institute},
  volume = {337},
  number = {2},
  pages = {97--103},
  issn = {0016-0032},
  doi = {10.1016/S0016-0032(00)00010-7},
  urldate = {2022-08-07},
  abstract = {This paper describes a method to randomly generate vectors of symbol probabilities so that the corresponding discrete memoryless source has a prescribed entropy. One application is to Monte Carlo simulation of the performance of noiseless variable length source coding.},
  langid = {english},
  keywords = {Entropy coding,Monte Carlo simulation},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\RRCGKU8A\\S0016003200000107.html}
}

@article{talagala2018,
  title = {Meta-Learning How to Forecast Time Series},
  author = {Talagala, Thiyanga S and Hyndman, Rob J and Athanasopoulos, George},
  year = {2018},
  pages = {30},
  abstract = {A crucial task in time series forecasting is the identification of the most suitable forecasting method. We present a general framework for forecast-model selection using meta-learning. A random forest is used to identify the best forecasting method using only time series features. The framework is evaluated using time series from the M1 and M3 competitions and is shown to yield accurate forecasts comparable to several benchmarks and other commonly used automated approaches of time series forecasting. A key advantage of our proposed framework is that the time-consuming process of building a classifier is handled in advance of the forecasting task at hand.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Talagala et al. - 2018 - Meta-learning how to forecast time series.pdf}
}

@article{tansey,
  title = {Vector-{{Space Markov Random Fields}} via {{Exponential Families}}},
  author = {Tansey, Wesley and Padilla, Oscar Hernan Madrid and Suggala, Arun Sai and Ravikumar, Pradeep},
  pages = {9},
  abstract = {We present Vector-Space Markov Random Fields (VS-MRFs), a novel class of undirected graphical models where each variable can belong to an arbitrary vector space. VS-MRFs generalize a recent line of work on scalar-valued, uni-parameter exponential family and mixed graphical models, thereby greatly broadening the class of exponential families available (e.g., allowing multinomial and Dirichlet distributions). Specifically, VSMRFs are the joint graphical model distributions where the node-conditional distributions belong to generic exponential families with general vector space domains. We also present a sparsistent M -estimator for learning our class of MRFs that recovers the correct set of edges with high probability. We validate our approach via a set of synthetic data experiments as well as a realworld case study of over four million foods from the popular diet tracking app MyFitnessPal. Our results demonstrate that our algorithm performs well empirically and that VS-MRFs are capable of capturing and highlighting interesting structure in complex, real-world data. All code for our algorithm is open source and publicly available.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Tansey et al. - Vector-Space Markov Random Fields via Exponential .pdf}
}

@article{tarjan1984,
  title = {Simple {{Linear-Time Algorithms}} to {{Test Chordality}} of {{Graphs}}, {{Test Acyclicity}} of {{Hypergraphs}}, and {{Selectively Reduce Acyclic Hypergraphs}}},
  author = {Tarjan, Robert E. and Yannakakis, Mihalis},
  year = {1984},
  month = aug,
  journal = {SIAM Journal on Computing},
  volume = {13},
  number = {3},
  pages = {566--579},
  issn = {0097-5397, 1095-7111},
  doi = {10.1137/0213035},
  urldate = {2021-08-07},
  abstract = {Chordal graphs arise naturally in the study of Gaussian elimination on sparse symmetric matrices; acyclic hypergraphs arise in the study of relational data bases. Rose, Tarjan and Lueker [SIAM J. Comput., 5 (1976), pp. 266-283] have given a linear-time algorithm to test whether a graph is chordal, which Yannakakis has modified to test whether a hypergraph is acyclic. Here we develop a simplified linear-time test for graph chordality and hypergraph acyclicity. The test uses a new kind of graph (and hypergraph) search, which we call maximum cardinality search. A variant of the method gives a way to selectively reduce acyclic hypergraphs, which is needed for evaluating queries in acyclic relational data bases.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Tarjan and Yannakakis - 1984 - Simple Linear-Time Algorithms to Test Chordality o.pdf}
}

@article{teixeira2012,
  title = {Conditional {{R\'enyi Entropies}}},
  author = {Teixeira, Andreia and Matos, Armando and Antunes, Lu{\'i}s},
  year = {2012},
  month = jul,
  journal = {IEEE Transactions on Information Theory},
  volume = {58},
  number = {7},
  pages = {4273--4277},
  issn = {1557-9654},
  doi = {10.1109/TIT.2012.2192713},
  abstract = {There is no generally accepted definition of conditional R\'enyi entropy. The (unconditional) R\'enyi entropy depends on a parameter {$\alpha$}, which for the case of min-entropy takes the value {$\infty$}. Even for this particular case, there are several proposals for the definition of conditional entropy. This paper describes three general definitions of conditional R\'enyi entropy that were found or suggested in the literature. Their properties are studied and their values, as a function of {$\alpha$}, are compared. The particular case of min-entropy is widely used in cryptography as a security parameter; this case is studied in some detail.},
  keywords = {Biomedical measurements,Computer science,Conditional entropies,Cryptography,Educational institutions,Entropy,Joints,Probability distribution,R\'enyi entropy},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\62LZQYTJ\\Teixeira et al. - 2012 - Conditional Rényi Entropies.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\HQRTMUVU\\6191351.html}
}

@article{thompson1987,
  title = {Sample {{Size}} for {{Estimating Multinomial Proportions}}},
  author = {Thompson, Steven K.},
  year = {1987},
  journal = {The American Statistician},
  volume = {41},
  number = {1},
  eprint = {2684318},
  eprinttype = {jstor},
  pages = {42--46},
  issn = {0003-1305},
  doi = {10.2307/2684318},
  urldate = {2020-01-10},
  abstract = {This article presents a procedure and a table for selecting sample size for simultaneously estimating the parameters of a multinomial distribution. The results are obtained by examining the "worst" possible value of a multinomial parameter vector, analogous to the case in which a binomial parameter equals one-half.}
}

@article{thudumu2020,
  title = {A Comprehensive Survey of Anomaly Detection Techniques for High Dimensional Big Data},
  author = {Thudumu, Srikanth and Branch, Philip and Jin, Jiong and Singh, Jugdutt (Jack)},
  year = {2020},
  month = jul,
  journal = {Journal of Big Data},
  volume = {7},
  number = {1},
  pages = {42},
  issn = {2196-1115},
  doi = {10.1186/s40537-020-00320-x},
  urldate = {2023-03-23},
  abstract = {Anomaly detection in high dimensional data is becoming a fundamental research problem that has various applications in the real world. However, many existing anomaly detection techniques fail to retain sufficient accuracy due to so-called ``big data'' characterised by high-volume, and high-velocity data generated by variety of sources. This phenomenon of having both problems together can be referred to the ``curse of big dimensionality,'' that affect existing techniques in terms of both performance and accuracy. To address this gap and to understand the core problem, it is necessary to identify the unique challenges brought by the anomaly detection with both high dimensionality and big data problems. Hence, this survey aims to document the state of anomaly detection in high dimensional big data by representing the unique challenges using a triangular model of vertices: the problem (big dimensionality), techniques/algorithms (anomaly detection), and tools (big data applications/frameworks). Authors' work that fall directly into any of the vertices or closely related to them are taken into consideration for review. Furthermore, the limitations of traditional approaches and current strategies of high dimensional data are discussed along with recent techniques and applications on big data required for the optimization of anomaly detection.},
  keywords = {Anomaly detection,Big data,Big dimensionality,Big dimensionality tools,High dimensionality,The curse of big dimensionality,The curse of dimensionality},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Thudumu et al. - 2020 - A comprehensive survey of anomaly detection techni.pdf}
}

@article{toller2019,
  title = {{{SAZED}}: Parameter-Free Domain-Agnostic Season Length Estimation in Time Series Data},
  shorttitle = {{{SAZED}}},
  author = {Toller, Maximilian and Santos, Tiago and Kern, Roman},
  year = {2019},
  month = nov,
  journal = {Data Mining and Knowledge Discovery},
  volume = {33},
  number = {6},
  pages = {1775--1798},
  issn = {1384-5810, 1573-756X},
  doi = {10.1007/s10618-019-00645-z},
  urldate = {2019-11-06},
  abstract = {Season length estimation is the task of identifying the number of observations in the dominant repeating pattern of seasonal time series data. As such, it is a common preprocessing task crucial for various downstream applications. Inferring season length from a real-world time series is often challenging due to phenomena such as slightly varying period lengths and noise. These issues may, in turn, lead practitioners to dedicate considerable effort to preprocessing of time series data since existing approaches either require dedicated parameter-tuning or their performance is heavily domaindependent. Hence, to address these challenges, we propose SAZED: spectral and average autocorrelation zero distance density. SAZED is a versatile ensemble of multiple, specialized time series season length estimation approaches. The combination of various base methods selected with respect to domain-agnostic criteria and a novel seasonality isolation technique, allow a broad applicability to real-world time series of varied properties. Further, SAZED is theoretically grounded and parameter-free, with a computational complexity of O(n log n), which makes it applicable in practice. In our experiments, SAZED was statistically significantly better than every other method on at least one dataset. The datasets we used for the evaluation consist of time series data from various real-world domains, sterile synthetic test cases and synthetic data that were designed to be seasonal and yet have no finite statistical moments of any order.},
  langid = {english},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\HNXNIXL4\\Toller et al_2019_SAZED.pdf}
}

@article{trandafir2003,
  title = {Determination of a Discrete Distribution with given Entropy},
  author = {Trandafir, Romica},
  year = {2003},
  month = jan,
  journal = {Operational Research},
  volume = {3},
  number = {1},
  pages = {41--46},
  issn = {1866-1505},
  doi = {10.1007/BF02940277},
  urldate = {2022-08-07},
  abstract = {This paper gives an algorithm to determine a discrete distribution with specified entropy. Givenn andHn, the entropy, the distribution found is close to a discrete finite uniform distribution. Finally, the relative errors of the distribution found using this algorithm are compared to the ones of the discrete distribution obtained from the Lagrange multipliers method.},
  langid = {english},
  keywords = {maximum entropy principle,Monte Carlo simulation,uniform distribution}
}

@techreport{tsymbal2004,
  type = {Technical {{Report}}},
  title = {The {{Problem}} of {{Concept Drift}}: {{Definitions}} and {{Related Work}}},
  shorttitle = {The {{Problem}} of {{Concept Drift}}},
  author = {Tsymbal, Alexey},
  year = {2004},
  month = may,
  number = {TCD-CS-2004-15},
  address = {{Computer Science Department}},
  institution = {{Trinity College Dublin}},
  abstract = {In the real world concepts are often not stable but change with time. Typical examples of this are weather prediction rules and customers' preferences. The underlying data distribution may change as well. Often these changes make the model built on old data inconsistent with the new data, and regular updating of the model is necessary. This problem, known as concept drift, complicates the task of learning a model from data and requires special approaches, different from commonly used techniques, which treat arriving instances as equally important contributors to the final concept. This paper considers different types of concept drift, peculiarities of the problem, and gives a critical review of existing approaches to the problem.},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Tsymbal - 2004 - The Problem of Concept Drift Definitions and Rela.pdf}
}

@article{tuia2016,
  title = {Domain {{Adaptation}} for the {{Classification}} of {{Remote Sensing Data}}: {{An Overview}} of {{Recent Advances}}},
  shorttitle = {Domain {{Adaptation}} for the {{Classification}} of {{Remote Sensing Data}}},
  author = {Tuia, Devis and Persello, Claudio and Bruzzone, Lorenzo},
  year = {2016},
  month = jun,
  journal = {IEEE Geoscience and Remote Sensing Magazine},
  volume = {4},
  number = {2},
  pages = {41--57},
  issn = {2168-6831},
  doi = {10.1109/MGRS.2016.2548504},
  urldate = {2019-03-06},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Tuia et al. - 2016 - Domain Adaptation for the Classification of Remote.pdf}
}

@inproceedings{tzimpragos2019,
  title = {Boosted {{Race Trees}} for {{Low Energy Classification}}},
  booktitle = {Proceedings of the {{Twenty-Fourth International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}  - {{ASPLOS}} '19},
  author = {Tzimpragos, Georgios and Madhavan, Advait and Vasudevan, Dilip and Strukov, Dmitri and Sherwood, Timothy},
  year = {2019},
  pages = {215--228},
  publisher = {{ACM Press}},
  address = {{Providence, RI, USA}},
  doi = {10.1145/3297858.3304036},
  urldate = {2019-06-12},
  abstract = {When extremely low-energy processing is required, the choice of data representation makes a tremendous difference. Each representation (e.g. frequency domain, residue coded, logscale) comes with a unique set of trade-offs \textemdash{} some operations are easier in that domain while others are harder. We demonstrate that race logic, in which temporally coded signals are getting processed in a dataflow fashion, provides interesting new capabilities for in-sensor processing applications. Specifically, with an extended set of race logic operations, we show that tree-based classifiers can be naturally encoded, and that common classification tasks can be implemented efficiently as a programmable accelerator in this class of logic. To verify this hypothesis, we design several race logic implementations of ensemble learners, compare them against state-of-the-art classifiers, and conduct an architectural design space exploration. Our proof-of-concept architecture, consisting of 1,000 reconfigurable Race Trees of depth 6, will process 15.2M frames/s, dissipating 613mW in 14nm CMOS.},
  isbn = {978-1-4503-6240-5},
  langid = {english},
  file = {/home/lklee/google-drive/Academic Papers/Zotero/pdf/2019/Tzimpragos et al_2019_Boosted Race Trees for Low Energy Classification.pdf}
}

@book{vaart1998,
  title = {Asymptotic {{Statistics}}},
  author = {van der Vaart, A. W.},
  year = {1998},
  series = {Cambridge {{Series}} in {{Statistical}} and {{Probabilistic Mathematics}}},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511802256},
  urldate = {2021-02-26},
  abstract = {This book is an introduction to the field of asymptotic statistics. The treatment is both practical and mathematically rigorous. In addition to most of the standard topics of an asymptotics course, including likelihood inference, M-estimation, the theory of asymptotic efficiency, U-statistics, and rank procedures, the book also presents recent research topics such as semiparametric models, the bootstrap, and empirical processes and their applications. The topics are organized from the central idea of approximation by limit experiments, which gives the book one of its unifying themes. This entails mainly the local approximation of the classical i.i.d. set up with smooth parameters by location experiments involving a single, normally distributed observation. Thus, even the standard subjects of asymptotic statistics are presented in a novel way. Suitable as a graduate or Master's level statistics text, this book will also give researchers an overview of research in asymptotic statistics.},
  isbn = {978-0-521-78450-4},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\7X4GUBDZ\\A3C7DAD3F7E66A1FA60E9C8FE132EE1D.html}
}

@incollection{verma1990,
  title = {Causal {{Networks}}: {{Semantics}} and {{Expressiveness}}},
  shorttitle = {Causal {{Networks}}},
  booktitle = {Machine {{Intelligence}} and {{Pattern Recognition}}},
  author = {Verma, Thomas and Pearl, Judea},
  editor = {Shachter, Ross D. and Levitt, Tod S. and Kanal, Laveen N. and Lemmer, John F.},
  year = {1990},
  month = jan,
  series = {Uncertainty in {{Artificial Intelligence}}},
  volume = {9},
  pages = {69--76},
  publisher = {{North-Holland}},
  doi = {10.1016/B978-0-444-88650-7.50011-1},
  urldate = {2022-06-22},
  abstract = {Dependency knowledge of the form ``x is independent of y once z is known'' invariably obeys the four graphoid axioms, examples include probabilistic and database dependencies. Often, such knowledge can be represented efficiently with graphical structures such as undirected graphs and directed acyclic graphs (DAGs). In this paper we show that the graphical criterion called d-separation is a sound rule for reading independencies from any DAG based on a causal input list drawn from a graphoid. The rule may be extended to cover DAGs that represent functional dependencies as well as conditional dependencies.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Verma and Pearl - 1990 - Causal Networks Semantics and Expressiveness Th.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\KYNL226P\\B9780444886507500111.html}
}

@inproceedings{vuffray2016,
  title = {Interaction {{Screening}}: {{Efficient}} and {{Sample-Optimal Learning}} of {{Ising Models}}},
  shorttitle = {Interaction {{Screening}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vuffray, Marc and Misra, Sidhant and Lokhov, Andrey and Chertkov, Michael},
  year = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2021-08-10},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Vuffray et al. - 2016 - Interaction Screening Efficient and Sample-Optima.pdf}
}

@article{vuffray2020,
  title = {Efficient {{Learning}} of {{Discrete Graphical Models}}},
  author = {Vuffray, Marc and Misra, Sidhant and Lokhov, Andrey Y.},
  year = {2020},
  month = jul,
  journal = {arXiv:1902.00600 [cs, math, stat]},
  eprint = {1902.00600},
  primaryclass = {cs, math, stat},
  urldate = {2021-08-05},
  abstract = {Graphical models are useful tools for describing structured high-dimensional probability distributions. Development of efficient algorithms for learning graphical models with least amount of data remains an active research topic. Reconstruction of graphical models that describe the statistics of discrete variables is a particularly challenging problem, for which the maximum likelihood approach is intractable. In this work, we provide the first sample-efficient method based on the Interaction Screening framework that allows one to provably learn fully general discrete factor models with node-specific discrete alphabets and multi-body interactions, specified in an arbitrary basis. We identify a single condition related to model parametrization that leads to rigorous guarantees on the recovery of model structure and parameters in any error norm, and is readily verifiable for a large class of models. Importantly, our bounds make explicit distinction between parameters that are proper to the model and priors used as an input to the algorithm. Finally, we show that the Interaction Screening framework includes all models previously considered in the literature as special cases, and for which our analysis shows a systematic improvement in sample complexity.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {\_tablet,Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Vuffray et al. - 2020 - Efficient Learning of Discrete Graphical Models.pdf}
}

@article{vuffray2021,
  title = {Efficient Learning of Discrete Graphical Models},
  author = {Vuffray, Marc and Misra, Sidhant and Lokhov, Andrey Y.},
  year = {2021},
  month = dec,
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2021},
  number = {12},
  pages = {124017},
  publisher = {{IOP Publishing}},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/ac3aea},
  urldate = {2022-05-18},
  abstract = {Graphical models are useful tools for describing structured high-dimensional probability distributions. Development of efficient algorithms for learning graphical models with least amount of data remains an active research topic. Reconstruction of graphical models that describe the statistics of discrete variables is a particularly challenging problem, for which the maximum likelihood approach is intractable. In this work, we provide the first sample-efficient method based on the interaction screening framework that allows one to provably learn fully general discrete factor models with node-specific discrete alphabets and multi-body interactions, specified in an arbitrary basis. We identify a single condition related to model parametrization that leads to rigorous guarantees on the recovery of model structure and parameters in any error norm, and is readily verifiable for a large class of models. Importantly, our bounds make explicit distinction between parameters that are proper to the model and priors used as an input to the algorithm. Finally, we show that the interaction screening framework includes all models previously considered in the literature as special cases, and for which our analysis shows a systematic improvement in sample complexity.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Vuffray et al. - 2021 - Efficient learning of discrete graphical modelsas.pdf}
}

@article{wainwright2008,
  title = {Graphical {{Models}}, {{Exponential Families}}, and {{Variational Inference}}},
  author = {Wainwright, Martin J. and Jordan, Michael I.},
  year = {2008},
  month = jan,
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {1},
  number = {1-2},
  pages = {1--305},
  issn = {1935-8237},
  doi = {10.1561/2200000001},
  urldate = {2021-10-15},
  abstract = {The formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. Graphical models have become a focus of research in many statistical, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimization, signal and image processing, information retrieval and statistical machine learning. Many problems that arise in specific instances \textemdash{} including the key problems of computing marginals and modes of probability distributions \textemdash{} are best studied in the general setting. Working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, we develop general variational representations of the problems of computing likelihoods, marginal probabilities and most probable configurations. We describe how a wide variety of algorithms \textemdash{} among them sum-product, cluster variational methods, expectation-propagation, mean field methods, max-product and linear programming relaxation, as well as conic programming relaxations \textemdash{} can all be understood in terms of exact or approximate forms of these variational representations. The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models.},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Wainwright and Jordan - 2008 - Graphical Models, Exponential Families, and Variat.pdf}
}

@book{wainwright2019,
  title = {High-Dimensional Statistics: A Non-Asymptotic Viewpoint},
  shorttitle = {High-Dimensional Statistics},
  author = {Wainwright, Martin},
  year = {2019},
  series = {Cambridge Series in Statistical and Probabilistic Mathematics},
  number = {48},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge ; New York, NY}},
  isbn = {978-1-108-49802-9},
  lccn = {QA276.18 .W35 2019},
  keywords = {Big data,Mathematical statistics,Textbooks},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Wainwright - 2019 - High-dimensional statistics a non-asymptotic view.pdf}
}

@inproceedings{wang2003,
  title = {Mining {{Concept-drifting Data Streams Using Ensemble Classifiers}}},
  booktitle = {Proceedings of the {{Ninth ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Wang, Haixun and Fan, Wei and Yu, Philip S. and Han, Jiawei},
  year = {2003},
  series = {{{KDD}} '03},
  pages = {226--235},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/956750.956778},
  urldate = {2019-03-19},
  abstract = {Recently, mining data streams with concept drifts for actionable insights has become an important and challenging task for a wide range of applications including credit card fraud protection, target marketing, network intrusion detection, etc. Conventional knowledge discovery tools are facing two challenges, the overwhelming volume of the streaming data, and the concept drifts. In this paper, we propose a general framework for mining concept-drifting data streams using weighted ensemble classifiers. We train an ensemble of classification models, such as C4.5, RIPPER, naive Beyesian, etc., from sequential chunks of the data stream. The classifiers in the ensemble are judiciously weighted based on their expected classification accuracy on the test data under the time-evolving environment. Thus, the ensemble approach improves both the efficiency in learning the model and the accuracy in performing classification. Our empirical study shows that the proposed methods have substantial advantage over single-classifier approaches in prediction accuracy, and the ensemble framework is effective for a variety of classification models.},
  isbn = {978-1-58113-737-8},
  keywords = {Accuracy Weighted Ensemble,AWE,classifier,classifier ensemble,concept drift,data streams},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Wang et al. - 2003 - Mining Concept-drifting Data Streams Using Ensembl.pdf}
}

@inproceedings{wang2010,
  title = {What {{Is Concept Drift}} and {{How}} to {{Measure It}}?},
  booktitle = {Knowledge {{Engineering}} and {{Management}} by the {{Masses}}},
  author = {Wang, Shenghui and Schlobach, Stefan and Klein, Michel},
  editor = {Cimiano, Philipp and Pinto, H. Sofia},
  year = {2010},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {241--256},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-16438-5\_17},
  abstract = {This paper studies concept drift over time. We first define the meaning of a concept in terms of intension, extension and label. We then introduce concept drift over time and two derived notions: (in)stability over a time period and concept shift between two time points. We apply our framework in three case-studies, one from communication science, on DBPedia, and one in the legal domain. We describe ways of identifying interesting changes in the meaning of concept within given application contexts. These case-studies illustrate the feasibility of our framework in analysing concept drift in knowledge organisation schemas of varying expressiveness.},
  isbn = {978-3-642-16438-5},
  langid = {english},
  keywords = {Concept Drift,Concept Shift,Domain Expert,Identical Concept,Knowledge Organisation System},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Wang et al. - 2010 - What Is Concept Drift and How to Measure It.pdf}
}

@article{wang2019,
  title = {Drifted {{Twitter Spam Classification Using Multiscale Detection Test}} on {{K-L Divergence}}},
  author = {Wang, Xuesong and Kang, Qi and An, Jing and Zhou, Mengchu},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {108384--108394},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2932018},
  abstract = {Twitter spam classification is a tough challenge for social media platforms and cyber security companies. Twitter spam with illegal links may evolve over time in order to deceive filtering models, causing disastrous loss to both users and the whole network. We define this distributional evolution as a concept drift scenario. To build an effective model, we adopt K-L divergence to represent spam distribution and use a multiscale drift detection test (MDDT) to localize possible drifts therein. A base classifier is then retrained based on the detection result to gain performance improvement. Comprehensive experiments show that K-L divergence has highly consistent change patterns between features when a drift occurs. Also, the MDDT is proved to be effective in improving final classification result in both accuracy, recall, and f-measure.},
  keywords = {Companies,Concept drift,Data mining,Data models,drift detection test,Feature extraction,K-L divergence,Microsoft Windows,Twitter,twitter spam classification},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Wang et al. - 2019 - Drifted Twitter Spam Classification Using Multisca.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\42ZALAKB\\8781937.html}
}

@article{wang2022,
  title = {Generalizing to {{Unseen Domains}}: {{A Survey}} on {{Domain Generalization}}},
  shorttitle = {Generalizing to {{Unseen Domains}}},
  author = {Wang, Jindong and Lan, Cuiling and Liu, Chang and Ouyang, Yidong and Qin, Tao and Lu, Wang and Chen, Yiqiang and Zeng, Wenjun and Yu, Philip},
  year = {2022},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  pages = {1--1},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2022.3178128},
  abstract = {Machine learning systems generally assume that the training and testing distributions are the same. To this end, a key requirement is to develop models that can generalize to unseen distributions. Domain generalization (DG), i.e., out-of-distribution generalization, has attracted increasing interests in recent years. Domain generalization deals with a challenging setting where one or several different but related domain(s) are given, and the goal is to learn a model that can generalize to an unseen test domain. Great progress has been made in the area of domain generalization for years. This paper presents the first review of recent advances in this area. First, we provide a formal definition of domain generalization and discuss several related fields. We then thoroughly review the theories related to domain generalization and carefully analyze the theory behind generalization. We categorize recent algorithms into three classes: data manipulation, representation learning, and learning strategy, and present several popular algorithms in detail for each category. Third, we introduce the commonly used datasets, applications, and our open-sourced codebase for fair evaluation. Finally, we summarize existing literature and present some potential research topics for the future.},
  keywords = {Adaptation models,Computational modeling,Data models,Domain adaptation,Domain generalization,Multitasking,Out-of-distribution generalization Training set Test set Sketch Cartoon Art painting Photo,Predictive models,Task analysis,Training,Transfer learning},
  file = {D\:\\work\\literature\\library\\Wang et al. - 2022 - Generalizing to Unseen Domains A Survey on Domain.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\MG2FGUG5\\9782500.html}
}

@inproceedings{webb2016,
  title = {A {{Multiple Test Correction}} for {{Streams}} and {{Cascades}} of {{Statistical Hypothesis Tests}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}} - {{KDD}} '16},
  author = {Webb, Geoffrey I. and Petitjean, Fran{\c c}ois},
  year = {2016},
  pages = {1255--1264},
  publisher = {{ACM Press}},
  address = {{San Francisco, California, USA}},
  doi = {10.1145/2939672.2939775},
  urldate = {2019-11-08},
  abstract = {Statistical hypothesis testing is a popular and powerful tool for inferring knowledge from data. For every such test performed, there is always a non-zero probability of making a false discovery, i.e. rejecting a null hypothesis in error. Familywise error rate (FWER) is the probability of making at least one false discovery during an inference process. The expected FWER grows exponentially with the number of hypothesis tests that are performed, almost guaranteeing that an error will be committed if the number of tests is big enough and the risk is not managed; a problem known as the multiple testing problem. State-of-the-art methods for controlling FWER in multiple comparison settings require that the set of hypotheses be pre-determined. This greatly hinders statistical testing for many modern applications of statistical inference, such as model selection, because neither the set of hypotheses that will be tested, nor even the number of hypotheses, can be known in advance.},
  isbn = {978-1-4503-4232-2},
  langid = {english}
}

@article{webb2016a,
  title = {Characterizing {{Concept Drift}}},
  author = {Webb, Geoffrey I. and Hyde, Roy and Cao, Hong and Nguyen, Hai Long and Petitjean, Francois},
  year = {2016},
  month = jul,
  journal = {Data Mining and Knowledge Discovery},
  volume = {30},
  number = {4},
  eprint = {1511.03816},
  pages = {964--994},
  issn = {1384-5810, 1573-756X},
  doi = {10.1007/s10618-015-0448-4},
  urldate = {2019-03-03},
  abstract = {Most machine learning models are static, but the world is dynamic, and increasing online deployment of learned models gives increasing urgency to the development of efficient and effective mechanisms to address learning in the context of non-stationary distributions, or as it is commonly called concept drift. However, the key issue of characterizing the different types of drift that can occur has not previously been subjected to rigorous definition and analysis. In particular, while some qualitative drift categorizations have been proposed, few have been formally defined, and the quantitative descriptions required for precise and objective understanding of learner performance have not existed. We present the first comprehensive framework for quantitative analysis of drift. This supports the development of the first comprehensive set of formal definitions of types of concept drift. The formal definitions clarify ambiguities and identify gaps in previous definitions, giving rise to a new comprehensive taxonomy of concept drift types and a solid foundation for research into mechanisms to detect and address concept drift.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Webb et al. - 2016 - Characterizing Concept Drift.pdf}
}

@article{webb2018,
  title = {Analyzing Concept Drift and Shift from Sample Data},
  author = {Webb, Geoffrey I. and Lee, Loong Kuan and Goethals, Bart and Petitjean, Fran{\c c}ois},
  year = {2018},
  month = sep,
  journal = {Data Mining and Knowledge Discovery},
  volume = {32},
  number = {5},
  pages = {1179--1199},
  issn = {1573-756X},
  doi = {10.1007/s10618-018-0554-1},
  urldate = {2018-09-16},
  abstract = {Concept drift and shift are major issues that greatly affect the accuracy and reliability of many real-world applications of machine learning. We propose a new data mining task, concept drift mapping\textemdash the description and analysis of instances of concept drift or shift. We argue that concept drift mapping is an essential prerequisite for tackling concept drift and shift. We propose tools for this purpose, arguing for the importance of quantitative descriptions of drift and shift in marginal distributions. We present quantitative concept drift mapping techniques, along with methods for visualizing their results. We illustrate their effectiveness for real-world applications across energy-pricing, vegetation monitoring and airline scheduling.},
  copyright = {All rights reserved},
  langid = {english},
  keywords = {Concept drift,Concept shift,Mapping,Non-stationary distribution,Visualisation},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Webb et al. - 2018 - Analyzing concept drift and shift from sample data.pdf}
}

@inproceedings{widmer1993,
  title = {Effective Learning in Dynamic Environments by Explicit Context Tracking},
  booktitle = {Machine {{Learning}}: {{ECML-93}}},
  author = {Widmer, Gerhard and Kubat, Miroslav},
  year = {1993},
  month = apr,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {227--243},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/3-540-56602-3\_139},
  urldate = {2018-05-23},
  abstract = {Daily experience shows that in the real world, the meaning of many concepts heavily depends on some implicit context, and changes in that context can cause radical changes in the concepts. This paper introduces a method for incremental concept learning in dynamic environments where the target concepts may be context-dependent and may change drastically over time. The method has been implemented in a system called FLORA3. FLORA3 is very flexible in adapting to changes in the target concepts and tracking concept drift. Moreover, by explicitly storing old hypotheses and re-using them to bias learning in new contexts, it possesses the ability to utilize experience from previous learning. This greatly increases the system's effectiveness in environments where contexts can reoccur periodically. The paper describes the various algorithms that constitute the method and reports on several experiments that demonstrate the flexibility of FLORA3 in dynamic environments.},
  isbn = {978-3-540-56602-1 978-3-540-47597-2},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Widmer and Kubat - 1993 - Effective learning in dynamic environments by expl.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\LTKYMNWD\\3-540-56602-3_139.html}
}

@article{widmer1996,
  title = {Learning in the Presence of Concept Drift and Hidden Contexts},
  author = {Widmer, Gerhard and Kubat, Miroslav},
  year = {1996},
  month = apr,
  journal = {Machine Learning},
  volume = {23},
  number = {1},
  pages = {69--101},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00116900},
  urldate = {2018-05-15},
  abstract = {On-line learning in domains where the target concept depends on some hidden context poses serious problems. A changing context can induce changes in the target concepts, producing what is known as concept drift. We describe a family of learning algorithms that flexibly react to concept drift and can take advantage of situations where contexts reappear. The general approach underlying all these algorithms consists of (1) keeping only a window of currently trusted examples and hypotheses; (2) storing concept descriptions and reusing them when a previous context re-appears; and (3) controlling both of these functions by a heuristic that constantly monitors the system's behavior. The paper reports on experiments that test the systems' perfomance under various conditions such as different levels of noise and different extent and rate of concept drift.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Widmer and Kubat - 1996 - Learning in the presence of concept drift and hidd.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\NJIQU8XR\\BF00116900.html}
}

@article{wright1921,
  title = {Correlation and Causation},
  author = {Wright, Sewall},
  year = {1921},
  month = jan,
  journal = {Journal of Agricultural Research},
  volume = {20},
  number = {7},
  pages = {557--585},
  publisher = {{Dept of Agriculture}},
  address = {{Washington, D.C. :}},
  issn = {0095-9758},
  lccn = {agr13001837},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Wright - 1921 - Correlation and causation.pdf}
}

@article{wright1934,
  title = {The {{Method}} of {{Path Coefficients}}},
  author = {Wright, Sewall},
  year = {1934},
  journal = {The Annals of Mathematical Statistics},
  volume = {5},
  number = {3},
  pages = {161--215},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {00034851},
  urldate = {2022-06-23},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Wright - 1934 - The Method of Path Coefficients.pdf}
}

@article{wu2022,
  title = {Probabilistic Exact Adaptive Random Forest for Recurrent Concepts in Data Streams},
  author = {Wu, Ocean and Koh, Yun Sing and Dobbie, Gillian and Lacombe, Thomas},
  year = {2022},
  month = jan,
  journal = {International Journal of Data Science and Analytics},
  volume = {13},
  number = {1},
  pages = {17--32},
  issn = {2364-4168},
  doi = {10.1007/s41060-021-00273-1},
  urldate = {2023-03-24},
  abstract = {In order to adapt random forests to the dynamic nature of data streams, the state-of-the-art technique discards trained trees and grows new trees when concept drifts are detected. This is particularly wasteful when recurrent patterns exist. In this work, we introduce a novel framework called PEARL, which uses both an exact technique and a probabilistic graphical model with Lossy Counting, to replace drifted trees with relevant trees built in the past. The exact technique utilizes pattern matching to find the set of drifted trees that co-occurred in predictions in the past. Meanwhile, a probabilistic graphical model is being built to capture the tree replacements among recurrent concept drifts. Once the graphical model becomes stable, it replaces the exact technique and finds relevant trees in a probabilistic fashion. Further, Lossy Counting is applied to the graphical model which brings an added theoretical guarantee for both error rate and space complexity. We empirically show our technique outperforms baselines in terms of accuracy and kappa on both synthetic and real-world datasets.},
  langid = {english},
  keywords = {Concept drift,Data stream,Random forest,Recurring concepts},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Wu et al. - 2022 - Probabilistic exact adaptive random forest for rec.pdf}
}

@inproceedings{xu1995,
  title = {Computing Marginals from the Marginal Representation in {{Markov}} Trees},
  booktitle = {Advances in {{Intelligent Computing}} \textemdash{} {{IPMU}} '94},
  author = {Xu, Hong},
  editor = {{Bouchon-Meunier}, Bernadette and Yager, Ronald R. and Zadeh, Lotfi A.},
  year = {1995},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {108--116},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/BFb0035942},
  abstract = {Local computational techniques have been proposed to compute marginals for the variables in belief networks or valuation networks, based on the secondary structures called clique trees or Markov trees. However, these techniques only compute the marginal on the subset of variables contained in one node of the secondary structure. This paper presents a method for computing the marginal on the subset that may not be contained in one node. The proposed method allows us to change the structure of the Markov tree without changing any information contained in the nodes, thus avoids the possible repeated computations. Moreover, it can compute the marginal on any subset from the marginal representation already obtained. An efficient implementation of this method is also proposed.},
  isbn = {978-3-540-49443-0},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Xu - 1995 - Computing marginals from the marginal representati.pdf}
}

@article{xu2018,
  title = {Enriching the Random Subspace Method with Margin Theory \textendash{} a Solution for the High-Dimensional Classification Task},
  author = {Xu, Hongyan and Lin, Tao and Xie, Yingtao and Chen, Zhi},
  year = {2018},
  month = oct,
  journal = {Connection Science},
  volume = {30},
  number = {4},
  pages = {409--424},
  issn = {0954-0091, 1360-0494},
  doi = {10.1080/09540091.2018.1512556},
  urldate = {2023-03-22},
  abstract = {ABSTRACT The random subspace method (RSM) has proved its excellence in numbers of pattern recognition tasks. However, the standard RSM is limited owing to the randomness in its feature selection procedure that is likely to lead to feature subset having poor class separability. In this paper, a proposal for a margin-based criterion has been presented for the evaluation of the true significance of the features, together with the true classification ability of base classifiers, so that both the training phase and integration phase of standard RSM could be enhanced. In the training phase, the random feature selection procedure is enhanced using a weighted random feature selection procedure, in order to improve the classification ability of the base classifier. In the integration phase, the simple majority voting strategy is enhanced using a weighted majority voting strategy for the purpose of assigning the base classifiers with poor classification ability to the lower voting weights. Experimental results on 30 benchmark datasets, together with 6 high-dimensional datasets prove that the recommended approach is capable of better providing classification ability to the usual classification task, in addition to the high-dimensional classification task.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Xu et al. - 2018 - Enriching the random subspace method with margin t.pdf}
}

@inproceedings{yadav2019,
  title = {Cold {{Case}}: {{The Lost MNIST Digits}}},
  shorttitle = {Cold {{Case}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yadav, Chhavi and Bottou, Leon},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-04-27},
  abstract = {Although the popular MNIST dataset \textbackslash citep\{mnist\} is derived from the NIST database \textbackslash citep\{nist-sd19\}, precise processing steps of this derivation have been lost to time. We propose a reconstruction that is accurate enough to serve as a replacement for the MNIST dataset, with insignificant changes in accuracy.  We trace each MNIST digit to its NIST source and its rich metadata such as writer identifier, partition identifier, etc. We also reconstruct the complete MNIST test set with 60,000 samples instead of the usual 10,000. Since the balance 50,000 were never distributed, they enable us to investigate the impact of twenty-five years of MNIST experiments on the reported testing performances. Our results unambiguously confirm the trends observed by \textbackslash citet\{recht2018cifar,recht2019imagenet\}: although the misclassification rates are slightly off, classifier ordering and model selection remain broadly reliable. We attribute this phenomenon to the pairing benefits of comparing classifiers on the same digits.},
  file = {C\:\\Users\\Loong Kuan\\Zotero\\storage\\DVEPLVXF\\Yadav and Bottou - 2019 - Cold Case The Lost MNIST Digits.pdf}
}

@article{yannakakis1981,
  title = {Computing the {{Minimum Fill-In}} Is {{NP-Complete}}},
  author = {Yannakakis, Mihalis},
  year = {1981},
  month = mar,
  journal = {SIAM Journal on Algebraic Discrete Methods},
  volume = {2},
  number = {1},
  pages = {77--79},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0196-5212},
  doi = {10.1137/0602010},
  urldate = {2022-10-10},
  abstract = {We show that the following problem is NP-complete. Given a graph, find the minimum number of edges (fill-in) whose addition makes the graph chordal. This problem arises in the solution of sparse symmetric positive definite systems of linear equations by Gaussian elimination.}
}

@article{yao2013,
  title = {Concept {{Drift Visualization}}},
  author = {Yao, Yuan and {Feng, Lin} and {Feng, Chen}},
  year = {2013},
  month = jul,
  journal = {Journal of Information and Computational Science},
  volume = {10},
  number = {10},
  pages = {3021--3029},
  issn = {15487741},
  doi = {10.12733/jics20101915},
  urldate = {2018-08-22},
  abstract = {Mining data stream are facing many challenges now, one of them is concept drift problem. In many practical applications, concept drift usually affects the classification performance for data stream, or even make the classifier failed. However, most of the proposed methods are mainly focusing on solving concept drift from the data value point of view, and very little attention has been focused on mining the knowledge in the data concept level. Motivated by this, in this paper, we use Kullback-Leibler divergence (KL-divergence) algorithm to detect concept drift dynamically. Meanwhile, we also construct a concept pool to reserve distinct concepts in data stream and analyze the concept transformation information. Experimental studies on two real-world data sets demonstrate that the proposed concept visualization method and concept transformation map could effectively and efficiently mine concept drifts relationship from the noisy streaming data.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Yao et al. - 2013 - Concept Drift Visualization.pdf}
}

@article{yeh2019,
  title = {Online {{Amnestic Dynamic Time Warping}} to {{Allow Real-Time Golden Batch Monitoring}}},
  author = {Yeh, Chin-Chia Michael and Zhu, Yan and Dau, Hoang Anh and Darvishzadeh, Amirali and Noskov, Mikhail and Keogh, Eamonn},
  year = {2019},
  pages = {12},
  abstract = {In manufacturing, there is the Golden Batch concept. A golden batch is an idealized realization of the perfect process to produce the desired item, typically represented as a multidimensional time series of temperatures, pressures, flow-rates and so forth. The golden batch is sometimes produced from first-principle models, but it is typically created by recording a batch produced by the most experienced engineers on carefully cleaned and calibrated machines. In most cases, the golden batch is only used in postmortem analysis of an unexpectedly inferior quality product as plant managers attempt to understand where and when the last production attempt went wrong. In this work, we make two contributions to golden batch processing. We introduce an online algorithm that allows practitioners to understand if the process is currently deviating from the golden batch in real-time, allowing engineers to intervene and potentially save the batch. This may be done, for example, by cooling a boiler that is running unexpectedly hot. In addition, we show that our ideas can greatly expand the purview of golden batch monitoring beyond industrial manufacturing. In particular, we show that golden batch monitoring can be used for anomaly detection, attention focusing, and personalized training/skill assessment in a host of novel domains.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Yeh et al. - 2019 - Online Amnestic Dynamic Time Warping to Allow Real.pdf}
}

@inproceedings{yu2020,
  title = {Measuring the {{Discrepancy}} between {{Conditional Distributions}}: {{Methods}}, {{Properties}} and {{Applications}}},
  shorttitle = {Measuring the {{Discrepancy}} between {{Conditional Distributions}}},
  booktitle = {Twenty-{{Ninth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Yu, Shujian and Shaker, Ammar and Alesiani, Francesco and Principe, Jose},
  year = {2020},
  month = jul,
  volume = {3},
  pages = {2777--2784},
  issn = {1045-0823},
  doi = {10.24963/ijcai.2020/385},
  urldate = {2022-07-31},
  abstract = {Electronic proceedings of IJCAI 2020},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Yu et al. - 2020 - Measuring the Discrepancy between Conditional Dist.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\KEG8GADC\\385.html}
}

@article{zaidi2017,
  title = {Efficient Parameter Learning of {{Bayesian}} Network Classifiers},
  author = {Zaidi, Nayyar A. and Webb, Geoffrey I. and Carman, Mark J. and Petitjean, Fran{\c c}ois and Buntine, Wray and Hynes, Mike and De Sterck, Hans},
  year = {2017},
  month = oct,
  journal = {Machine Learning},
  volume = {106},
  number = {9},
  pages = {1289--1329},
  issn = {1573-0565},
  doi = {10.1007/s10994-016-5619-z},
  urldate = {2019-10-23},
  abstract = {Recent advances have demonstrated substantial benefits from learning with both generative and discriminative parameters. On the one hand, generative approaches address the estimation of the parameters of the joint distribution\textemdash P(y,x)\textbackslash mathrm\{P\}(y,\textbackslash mathbf\{x\}), which for most network types is very computationally efficient (a notable exception to this are Markov networks) and on the other hand, discriminative approaches address the estimation of the parameters of the posterior distribution\textemdash and, are more effective for classification, since they fit P(y|x)\textbackslash mathrm\{P\}(y|\textbackslash mathbf\{x\}) directly. However, discriminative approaches are less computationally efficient as the normalization factor in the conditional log-likelihood precludes the derivation of closed-form estimation of parameters. This paper introduces a new discriminative parameter learning method for Bayesian network classifiers that combines in an elegant fashion parameters learned using both generative and discriminative methods. The proposed method is discriminative in nature, but uses estimates of generative probabilities to speed-up the optimization process. A second contribution is to propose a simple framework to characterize the parameter learning task for Bayesian network classifiers. We conduct an extensive set of experiments on 72 standard datasets and demonstrate that our proposed discriminative parameterization provides an efficient alternative to other state-of-the-art parameterizations.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Zaidi et al. - 2017 - Efficient parameter learning of Bayesian network c.pdf}
}

@article{zaidi2018,
  title = {On the {{Inter-relationships}} among {{Drift}} Rate, {{Forgetting}} Rate, {{Bias}}/Variance Profile and {{Error}}},
  author = {Zaidi, Nayyar A. and Webb, Geoffrey I. and Petitjean, Francois and Forestier, Germain},
  year = {2018},
  month = jan,
  journal = {arXiv:1801.09354 [cs]},
  eprint = {1801.09354},
  primaryclass = {cs},
  urldate = {2018-12-15},
  abstract = {We propose two general and falsifiable hypotheses about expectations on generalization error when learning in the context of concept drift. One posits that as drift rate increases, the forgetting rate that minimizes generalization error will also increase and vice versa. The other posits that as a learner's forgetting rate increases, the bias/variance profile that minimizes generalization error will have lower variance and vice versa. These hypotheses lead to the concept of the sweet path, a path through the 3-d space of alternative drift rates, forgetting rates and bias/variance profiles on which generalization error will be minimized, such that slow drift is coupled with low forgetting and low bias, while rapid drift is coupled with fast forgetting and low variance. We present experiments that support the existence of such a sweet path. We also demonstrate that simple learners that select appropriate forgetting rates and bias/variance profiles are highly competitive with the state-of-the-art in incremental learners for concept drift on real-world drift problems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Zaidi et al. - 2018 - On the Inter-relationships among Drift rate, Forge.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\S3NWC32V\\1801.html}
}

@article{zhang2014,
  title = {Nonparametric {{Estimation}} of {{K\"ullback-Leibler Divergence}}},
  author = {Zhang, Zhiyi and Grabchak, Michael},
  year = {2014},
  month = nov,
  journal = {Neural Computation},
  volume = {26},
  number = {11},
  pages = {2570--2593},
  issn = {0899-7667},
  doi = {10.1162/NECO\_a\_00646},
  urldate = {2021-04-09},
  abstract = {In this letter, we introduce an estimator of K\"ullback-Leibler divergence based on two independent samples. We show that on any finite alphabet, this estimator has an exponentially decaying bias and that it is consistent and asymptotically normal. To explain the importance of this estimator, we provide a thorough analysis of the more standard plug-in estimator. We show that it is consistent and asymptotically normal, but with an infinite bias. Moreover, if we modify the plug-in estimator to remove the rare events that cause the bias to become infinite, the bias still decays at a rate no faster than . Further, we extend our results to estimating the symmetrized K\"ullback-Leibler divergence. We conclude by providing simulation results, which show that the asymptotic properties of these estimators hold even for relatively small sample sizes.},
  file = {D\:\\work\\literature\\library\\Zhang and Grabchak - 2014 - Nonparametric Estimation of Küllback-Leibler Diver.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\XM5MN9SQ\\Nonparametric-Estimation-of-Kullback-Leibler.html}
}

@inproceedings{zhang2017,
  title = {Causal {{Discovery}} from {{Nonstationary}}/{{Heterogeneous Data}}: {{Skeleton Estimation}} and {{Orientation Determination}}},
  shorttitle = {Causal {{Discovery}} from {{Nonstationary}}/{{Heterogeneous Data}}},
  booktitle = {Proceedings of the {{Twenty-Sixth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Zhang, Kun and Huang, Biwei and Zhang, Jiji and Glymour, Clark and Sch{\"o}lkopf, Bernhard},
  year = {2017},
  month = aug,
  pages = {1347--1353},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Melbourne, Australia}},
  doi = {10.24963/ijcai.2017/187},
  urldate = {2023-06-20},
  abstract = {It is commonplace to encounter nonstationary or heterogeneous data, of which the underlying generating process changes over time or across data sets (the data sets may have different experimental conditions or data collection conditions). Such a distribution shift feature presents both challenges and opportunities for causal discovery. In this paper we develop a principled framework for causal discovery from such data, called Constraint-based causal Discovery from Nonstationary/heterogeneous Data (CD-NOD), which addresses two important questions. First, we propose an enhanced constraint-based procedure to detect variables whose local mechanisms change and recover the skeleton of the causal structure over observed variables. Second, we present a way to determine causal orientations by making use of independence changes in the data distribution implied by the underlying causal model, benefiting from information carried by changing distributions.  Experimental results on various synthetic and real-world data sets are presented to demonstrate the efficacy of our methods.},
  isbn = {978-0-9992411-0-3},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Zhang et al. - 2017 - Causal Discovery from NonstationaryHeterogeneous .pdf}
}

@article{zhang2019,
  ids = {li},
  title = {Dynamics Reconstruction and Classification via {{Koopman}} Features},
  author = {Zhang, Wei and Yu, Yao-Chi and Li, Jr-Shin},
  year = {2019},
  month = nov,
  journal = {Data Mining and Knowledge Discovery},
  volume = {33},
  number = {6},
  pages = {1710--1735},
  issn = {1573-756X},
  doi = {10.1007/s10618-019-00639-x},
  urldate = {2021-08-07},
  abstract = {Knowledge discovery and information extraction of large and complex datasets has attracted great attention in wide-ranging areas from statistics and biology to medicine. Tools from machine learning, data mining, and neurocomputing have been extensively explored and utilized to accomplish such compelling data analytics tasks. However, for time-series data presenting active dynamic characteristics, many of the state-of-the-art techniques may not perform well in capturing the inherited temporal structures in these data. In this paper, integrating the Koopman operator and linear dynamical systems theory with support vector machines, we develop a novel dynamic data mining framework to construct low-dimensional linear models that approximate the nonlinear flow of high-dimensional time-series data generated by unknown nonlinear dynamical systems. This framework then immediately enables pattern recognition, e.g., classification, of complex time-series data to distinguish their dynamic behaviors by using the trajectories generated by the reduced linear systems. Moreover, we demonstrate the applicability and efficiency of this framework through the problems of time-series classification in bioinformatics and healthcare, including cognitive classification and seizure detection with fMRI and EEG data, respectively. The developed Koopman dynamic learning framework then lays a solid foundation for effective dynamic data mining and promises a mathematically justified method for extracting the dynamics and significant temporal structures of nonlinear dynamical systems.},
  langid = {english},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Zhang et al. - 2019 - Dynamics reconstruction and classification via Koo.pdf}
}

@article{zhang2019a,
  title = {Research on Detection and Integration Classification Based on Concept Drift of Data Stream},
  author = {Zhang, Baoju and Chen, Yidi},
  year = {2019},
  month = apr,
  journal = {EURASIP Journal on Wireless Communications and Networking},
  volume = {2019},
  number = {1},
  pages = {86},
  issn = {1687-1499},
  doi = {10.1186/s13638-019-1408-2},
  urldate = {2020-02-04},
  abstract = {As a new type of data, data stream has the characteristics of massive, high-speed, orderly, and continuous and is widely distributed in sensor networks, mobile communication, financial transactions, network traffic analysis, and other fields. However, due to the inherent problem of concept drift, it poses a great challenge to data stream mining. Therefore, this paper proposes a dual detection mechanism to judge the drift of concepts, and on this basis, the integration classification of data stream is carried out. The system periodically detects data stream with the index of classification error and uses the features of the essential emerging pattern (eEP) with high discrimination to help build the integrated classifiers to solve the classification mining problems in the dynamic data stream environment. Experiments show that the proposed algorithm can obtain better classification results under the premise of effectively coping with the change of concepts.},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Zhang and Chen - 2019 - Research on detection and integration classificati.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\9WXSDXNC\\s13638-019-1408-2.html}
}

@inproceedings{zhao2022,
  title = {Measuring {{Drift Severity}} by {{Tree Structure Classifiers}}},
  booktitle = {2022 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Zhao, Di and Koh, Yun Sing and {Fournier-Viger}, Philippe},
  year = {2022},
  month = jul,
  pages = {1--8},
  issn = {2161-4407},
  doi = {10.1109/IJCNN55064.2022.9892439},
  abstract = {Streaming data has become more common as our ability to collect data in real-time increases. A primary concern in dealing with data streams is concept drift, which describes changes in the underlying distribution of streaming data. Measuring drift severity is crucial for model adaptation. Drift severity can be a proxy in choosing concept drift adaptation strategies. Current methods measure drift severity by monitoring the changes in the learner performance or measuring the difference between data distributions. However, these methods cannot measure the drift severity if the ground truth labels are unavailable. Specifically, performance-based methods cannot measure marginal drift, and distribution-based methods cannot measure conditional drift. We propose a novel framework named Tree-based Drift Measurement (TDM) that measures both marginal and conditional drift without revisiting historical data. TDM measures the difference between tree classifiers by transforming them into sets of binary vectors. An experiment shows that TDM achieves similar performance to the state-of-the-art methods and provides the best trade-off between runtime and memory usage. A case study shows that the online learner performance can be improved by adapting different drift adaptation strategies based on the drift severity.},
  keywords = {Concept Drift,Current measurement,Data privacy,Data Stream,Drift Severity,Memory management,Neural networks,Runtime,Time division multiplexing,Time measurement}
}

@article{zhou2022,
  title = {Domain {{Generalization}}: {{A Survey}}},
  shorttitle = {Domain {{Generalization}}},
  author = {Zhou, Kaiyang and Liu, Ziwei and Qiao, Yu and Xiang, Tao and Loy, Chen Change},
  year = {2022},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages = {1--20},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2022.3195549},
  abstract = {Generalization to out-of-distribution (OOD) data is a capability natural to humans yet challenging for machines to reproduce. This is because most learning algorithms strongly rely on the i.i.d. assumption on source/target data, which is often violated in practice due to domain shift. Domain generalization (DG) aims to achieve OOD generalization by using only source data for model learning. Over the last ten years, research in DG has made great progress, leading to a broad spectrum of methodologies, e.g., those based on domain alignment, meta-learning, data augmentation, or ensemble learning, to name a few; DG has also been studied in various application areas including computer vision, speech recognition, natural language processing, medical imaging, and reinforcement learning. In this paper, for the first time a comprehensive literature review in DG is provided to summarize the developments over the past decade. Specifically, we first cover the background by formally defining DG and relating it to other relevant fields like domain adaptation and transfer learning. Then, we conduct a thorough review into existing methods and theories. Finally, we conclude this survey with insights and discussions on future research directions.},
  keywords = {Adaptation models,Biomedical imaging,Data models,Domain shift,Face recognition,Handwriting recognition,machine learning,model robustness,out-of-distribution generalization,Soft sensors,Speech recognition},
  file = {D\:\\work\\literature\\library\\Zhou et al. - 2022 - Domain Generalization A Survey.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\Y9BVUYG9\\9847099.html}
}

@article{zimek2012,
  title = {A Survey on Unsupervised Outlier Detection in High-Dimensional Numerical Data},
  author = {Zimek, Arthur and Schubert, Erich and Kriegel, Hans-Peter},
  year = {2012},
  journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  volume = {5},
  number = {5},
  pages = {363--387},
  issn = {1932-1872},
  doi = {10.1002/sam.11161},
  urldate = {2020-02-05},
  abstract = {High-dimensional data in Euclidean space pose special challenges to data mining algorithms. These challenges are often indiscriminately subsumed under the term `curse of dimensionality', more concrete aspects being the so-called `distance concentration effect', the presence of irrelevant attributes concealing relevant information, or simply efficiency issues. In about just the last few years, the task of unsupervised outlier detection has found new specialized solutions for tackling high-dimensional data in Euclidean space. These approaches fall under mainly two categories, namely considering or not considering subspaces (subsets of attributes) for the definition of outliers. The former are specifically addressing the presence of irrelevant attributes, the latter do consider the presence of irrelevant attributes implicitly at best but are more concerned with general issues of efficiency and effectiveness. Nevertheless, both types of specialized outlier detection algorithms tackle challenges specific to high-dimensional data. In this survey article, we discuss some important aspects of the `curse of dimensionality' in detail and survey specialized algorithms for outlier detection from both categories. \textcopyright{} 2012 Wiley Periodicals, Inc. Statistical Analysis and Data Mining, 2012},
  copyright = {Copyright \textcopyright{} 2012 Wiley Periodicals, Inc., A Wiley Company},
  langid = {english},
  keywords = {anomalies in high-dimensional data,approximate outlier detection,correlation outlier detection,curse of dimensionality,outlier detection in high-dimensional data,subspace outlier detection},
  file = {D\:\\iCloudDrive\\Documents\\Work\\literature\\library\\Zimek et al. - 2012 - A survey on unsupervised outlier detection in high.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\U3ZT593T\\sam.html}
}

@inproceedings{zliobaite2009,
  ids = {i.liobaite2009},
  title = {Determining the {{Training Window}} for {{Small Sample Size Classification}} with {{Concept Drift}}},
  booktitle = {2009 {{IEEE International Conference}} on {{Data Mining Workshops}}},
  author = {{\v Z}liobaite, Indre and Kuncheva, Ludmila I.},
  year = {2009},
  month = dec,
  pages = {447--452},
  publisher = {{IEEE}},
  address = {{Miami, FL, USA}},
  doi = {10.1109/ICDMW.2009.20},
  urldate = {2019-03-12},
  abstract = {We consider classification of sequential data in the presence of frequent and abrupt concept changes. The current practice is to use the data after the change to train a new classifier. However, if the window with the new data is too small, the classifier will be undertrained and hence less accurate that the `old' classifier. Here we propose a method (called WR*) for resizing the training window after detecting a concept change. Experiments with synthetic and real data demonstrate the advantages of WR* over other window resizing methods.},
  isbn = {978-1-4244-5384-9},
  langid = {english},
  keywords = {Change detection algorithms,Communication system traffic control,Computer science,concept change,concept drift,Conferences,Data mining,Data security,Electronic mail,Informatics,Mathematics,Monitoring,old classifier,pattern classification,sequential data classification,small sample size classification,training window,WR*},
  file = {D\:\\work\\literature\\library\\Žliobaite and Kuncheva - 2009 - Determining the Training Window for Small Sample S.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\9X99AC7T\\5360446.html}
}

@inproceedings{zliobaite2010,
  title = {Change with {{Delayed Labeling}}: {{When}} Is It {{Detectable}}?},
  shorttitle = {Change with {{Delayed Labeling}}},
  booktitle = {2010 {{IEEE International Conference}} on {{Data Mining Workshops}}},
  author = {{\v Z}liobaite, Indre},
  year = {2010},
  month = dec,
  pages = {843--850},
  doi = {10.1109/ICDMW.2010.49},
  abstract = {Handling changes over time in supervised learning (concept drift) lately has received a great deal of attention, a number of adaptive learning strategies have been developed. Most of them make an optimistic assumption that the new labels become available immediately. In real sequential classification tasks it is often unrealistic due to task specific delayed labeling or associated labeling costs. We address the problem of change detectability, given, that the new labels are not available. In this analytical study we look at the space of changes from probabilistic perspective to analyze, what changes are detectable without seeing the labels and what are not. We conduct a range of experiments with real life data with simulated and natural changes to explore this detectability issue. We propose a computationally friendly detection technique, which monitors a stream of classifier outputs. We demonstrate analytically and experimentally, what types of changes are possible to detect when the labels for the new data are not available.},
  keywords = {Accuracy,change detection,classifier output,concept drift,data analysis,data mining,Data models,data modification,delayed labeling,Labeling,learning (artificial intelligence),pattern classification,Sensitivity,sequential classification,supervised learning,Testing,Training,unlabeled data,Unsolicited electronic mail},
  file = {D\:\\work\\literature\\library\\Žliobaite - 2010 - Change with Delayed Labeling When is it Detectabl.pdf;C\:\\Users\\Loong Kuan\\Zotero\\storage\\BQ2LQCYD\\5693384.html}
}

@article{zotero-78,
  title = {Unsupervised {{Domain Adaptation}} via {{Discriminative Manifold Propagation}}},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  doi = {10.1109/tpami.2020.3014218},
  abstract = {Unsupervised domain adaptation is effective in leveraging rich information from a labeled source domain to an unlabeled target domain. Though deep learning and adversarial strategy made a significant breakthrough in the adaptability of features, there are two issues to be further studied. First, hard-assigned pseudo labels on the target domain are arbitrary and error-prone, and direct application of them may destroy the intrinsic data structure. Second, batch-wise training of deep learning limits the characterization of the global structure. In this paper, a Riemannian manifold learning framework is proposed to achieve transferability and discriminability simultaneously. For the first issue, this framework establishes a probabilistic discriminant criterion on the target domain via soft labels. Based on pre-built prototypes, this criterion is extended to a global approximation scheme for the second issue. Manifold metric alignment is adopted to be compatible with the embedding space. The theoretical error bounds of different alignment metrics are derived for constructive guidance. The proposed method can be used to tackle a series of variants of domain adaptation problems, including both vanilla and partial settings. Extensive experiments have been conducted to investigate the method and a comparative study shows the superiority of the discriminative manifold learning framework.},
  keywords = {Researcher App}
}
